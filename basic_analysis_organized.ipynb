{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic AML/MDS Chimerism Dynamics Analysis\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook focuses on the foundational analysis of chimerism dynamics in AML/MDS transplant patients. The analysis centers on **feature engineering** and **exploratory data analysis** to understand how CD3+ and CD3- chimerism changes over time can predict transplant outcomes.\n",
    "\n",
    "### Key Research Questions\n",
    "\n",
    "**Tier 1 (Primary Focus):**\n",
    "- Can dynamic changes of CD3+ chimerism at Day 30, 60, and 100 predict disease relapse?\n",
    "- Do specific trend patterns (upward, downward, fluctuating) correlate with outcomes?\n",
    "- Can percentage changes (e.g., ‚â•20% increase from Day 30 to Day 100) improve prediction accuracy?\n",
    "\n",
    "**Tier 2:**\n",
    "- Can CD3+ chimerism dynamics predict other transplant outcomes (OS, GVHD, GRFS)?\n",
    "\n",
    "**Tier 3:**\n",
    "- Do interactions between CD3+ and CD3- chimerism improve prediction models?\n",
    "- Can chimerism variability metrics enhance outcome prediction?\n",
    "\n",
    "### Analysis Approach\n",
    "\n",
    "1. **Feature Engineering**: Create dynamic change indicators and statistical summaries\n",
    "2. **Pattern Classification**: Categorize chimerism trends into interpretable labels\n",
    "3. **Exploratory Analysis**: Visualize distributions and correlations\n",
    "4. **Predictive Modeling**: Test various feature combinations for outcome prediction\n",
    "5. **Model Evaluation**: Compare performance across different feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Loading\n",
    "\n",
    "Import necessary libraries and load the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Core data processing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport logging\nimport sys\nimport os\nimport joblib\nimport glob\nfrom datetime import datetime\n\n# Machine learning libraries\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, mean_squared_error, confusion_matrix\nfrom sklearn.feature_selection import SelectKBest, f_classif, f_regression, RFE\nfrom sklearn.decomposition import PCA\n\n# Advanced clustering and dimensionality reduction\nfrom sklearn.cluster import KMeans\nimport umap.umap_ as umap\nfrom skfuzzy import cmeans\nimport skfuzzy as fuzz\n\n# Genetic algorithm and advanced ML\nfrom deap import base, creator, tools, algorithms\nimport random\n\n# 3D plotting\nfrom mpl_toolkits.mplot3d import Axes3D\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n# Configuration\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\", force=True)\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"‚úÖ Libraries imported successfully\")\nprint(f\"üìÖ Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading preprocessed dataset...\n",
      "‚úÖ Dataset loaded successfully\n",
      "üìä Dataset shape: (258, 64)\n",
      "üî¢ Memory usage: 0.22 MB\n",
      "\n",
      "=== Dataset Overview ===\n",
      "Rows: 258\n",
      "Columns: 64\n",
      "Total missing values: 3,634\n",
      "\n",
      "=== Data Types ===\n",
      "float64: 31 columns\n",
      "int64: 24 columns\n",
      "object: 9 columns\n",
      "\n",
      "=== First 3 Rows ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>disease</th>\n",
       "      <th>disease_risk_index</th>\n",
       "      <th>hct_ci_score</th>\n",
       "      <th>time_from_diagnosis_to_alloSCT</th>\n",
       "      <th>aml_eln_risk_category</th>\n",
       "      <th>disease_state_at_transplant</th>\n",
       "      <th>mrd_status_prior_to_transplant</th>\n",
       "      <th>donor_type</th>\n",
       "      <th>cd34+_dose</th>\n",
       "      <th>...</th>\n",
       "      <th>y_relapse</th>\n",
       "      <th>y_cgvhd</th>\n",
       "      <th>y_time_to_onset</th>\n",
       "      <th>y_cgvhd_nih</th>\n",
       "      <th>y_time_to_onset_nih</th>\n",
       "      <th>y_agvhd</th>\n",
       "      <th>y_agvhd_grade_at_onset</th>\n",
       "      <th>y_agvhd_time_to_onset</th>\n",
       "      <th>y_agvhd_highest_grade</th>\n",
       "      <th>y_agvhd_time_to_highest_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>253.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.66</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  disease  disease_risk_index hct_ci_score  \\\n",
       "0   61        2                   3            2   \n",
       "1   53        5                   2            0   \n",
       "2   63        1                   2            0   \n",
       "\n",
       "   time_from_diagnosis_to_alloSCT  aml_eln_risk_category  \\\n",
       "0                           253.0                    NaN   \n",
       "1                           218.0                    NaN   \n",
       "2                           162.0                    3.0   \n",
       "\n",
       "   disease_state_at_transplant  mrd_status_prior_to_transplant  donor_type  \\\n",
       "0                          NaN                             NaN           2   \n",
       "1                          NaN                             NaN           2   \n",
       "2                          1.0                             1.0           1   \n",
       "\n",
       "   cd34+_dose  ...  y_relapse  y_cgvhd y_time_to_onset  y_cgvhd_nih  \\\n",
       "0        5.40  ...          0        0             NaN          NaN   \n",
       "1        5.04  ...          0        1           145.0          1.0   \n",
       "2        4.66  ...          1        0             NaN          NaN   \n",
       "\n",
       "  y_time_to_onset_nih  y_agvhd  y_agvhd_grade_at_onset  y_agvhd_time_to_onset  \\\n",
       "0                 NaN        1                     1.0                  159.0   \n",
       "1               374.0        0                     NaN                    NaN   \n",
       "2                 NaN        1                     1.0                   78.0   \n",
       "\n",
       "   y_agvhd_highest_grade  y_agvhd_time_to_highest_grade  \n",
       "0                    NaN                            NaN  \n",
       "1                    NaN                            NaN  \n",
       "2                    NaN                            NaN  \n",
       "\n",
       "[3 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Column Names ===\n",
      "Total columns: 64\n",
      "Key columns identified:\n",
      "  - Chimerism columns (7): ['cd34+_dose', 'd30_cd3+', 'd30_cd3-', 'd60_cd3+', 'd60_cd3-']...\n",
      "  - Outcome columns (17): ['y_grfs_days', 'y_rfs_days', 'y_rfs', 'y_os_days', 'y_cause_of_death']...\n"
     ]
    }
   ],
   "source": [
    "def load_and_inspect_data(file_path=\"preprocessed_ml_for_aml_mds.csv\"):\n",
    "    \"\"\"\n",
    "    Load the preprocessed dataset and perform initial inspection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file containing preprocessed data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Loaded dataset\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading preprocessed dataset...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"‚úÖ Dataset loaded successfully\")\n",
    "        print(f\"üìä Dataset shape: {df.shape}\")\n",
    "        print(f\"üî¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Display basic information\n",
    "        print(f\"\\n=== Dataset Overview ===\")\n",
    "        print(f\"Rows: {len(df):,}\")\n",
    "        print(f\"Columns: {len(df.columns):,}\")\n",
    "        print(f\"Total missing values: {df.isnull().sum().sum():,}\")\n",
    "        \n",
    "        # Show column types\n",
    "        print(f\"\\n=== Data Types ===\")\n",
    "        dtype_counts = df.dtypes.value_counts()\n",
    "        for dtype, count in dtype_counts.items():\n",
    "            print(f\"{dtype}: {count} columns\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: File '{file_path}' not found\")\n",
    "        print(\"üìÅ Available files in current directory:\")\n",
    "        for file in os.listdir('.'):\n",
    "            if file.endswith('.csv'):\n",
    "                print(f\"   - {file}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "df = load_and_inspect_data()\n",
    "\n",
    "if df is not None:\n",
    "    # Display first few rows\n",
    "    print(f\"\\n=== First 3 Rows ===\")\n",
    "    display(df.head(3))\n",
    "    \n",
    "    # Show column names\n",
    "    print(f\"\\n=== Column Names ===\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(\"Key columns identified:\")\n",
    "    \n",
    "    chimerism_cols = [col for col in df.columns if 'cd3' in col.lower()]\n",
    "    outcome_cols = [col for col in df.columns if col.startswith('y_')]\n",
    "    \n",
    "    print(f\"  - Chimerism columns ({len(chimerism_cols)}): {chimerism_cols[:5]}...\")\n",
    "    print(f\"  - Outcome columns ({len(outcome_cols)}): {outcome_cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chimerism Dynamics Feature Engineering\n",
    "\n",
    "Create sophisticated features to capture the dynamic nature of chimerism changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_chimerism_dynamics_features(df):\n    \"\"\"\n    Engineer comprehensive chimerism dynamics features.\n    \n    This function creates multiple types of features:\n    1. Time-point differences (Day 30‚Üí60, Day 60‚Üí100)\n    2. Statistical summaries (mean, std, coefficient of variation)\n    3. Demographic ratios (age relationships)\n    4. Pattern classifications (trend labels)\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Input dataframe with chimerism measurements\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Enhanced dataframe with new features\n    \"\"\"\n    print(\"üîß Engineering chimerism dynamics features...\")\n    df_enhanced = df.copy()\n    \n    # Check for required columns\n    required_cols = ['d30_cd3+', 'd60_cd3+', 'd100_cd3+', 'd30_cd3-', 'd60_cd3-', 'd100_cd3-']\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    \n    if missing_cols:\n        print(f\"‚ö†Ô∏è Missing required columns: {missing_cols}\")\n        return df_enhanced\n    \n    # Convert chimerism columns to numeric first\n    for col in required_cols:\n        if col in df_enhanced.columns:\n            df_enhanced[col] = pd.to_numeric(df_enhanced[col], errors='coerce')\n    \n    # 1. TIME-POINT DIFFERENCES\n    print(\"   üìà Creating time-point difference features...\")\n    \n    # Calculate differences between consecutive time points\n    df_enhanced[\"d(30-60)_cd3+\"] = df_enhanced[\"d30_cd3+\"] - df_enhanced[\"d60_cd3+\"]\n    df_enhanced[\"d(60-100)_cd3+\"] = df_enhanced[\"d60_cd3+\"] - df_enhanced[\"d100_cd3+\"]\n    df_enhanced[\"d(30-60)_cd3-\"] = df_enhanced[\"d30_cd3-\"] - df_enhanced[\"d60_cd3-\"]\n    df_enhanced[\"d(60-100)_cd3-\"] = df_enhanced[\"d60_cd3-\"] - df_enhanced[\"d100_cd3-\"]\n    \n    # Calculate overall change (Day 30 to Day 100)\n    df_enhanced[\"d(30-100)_cd3+\"] = df_enhanced[\"d30_cd3+\"] - df_enhanced[\"d100_cd3+\"]\n    df_enhanced[\"d(30-100)_cd3-\"] = df_enhanced[\"d30_cd3-\"] - df_enhanced[\"d100_cd3-\"]\n    \n    # 2. DEMOGRAPHIC FEATURES\n    print(\"   üë• Creating demographic ratio features...\")\n    \n    if 'age' in df.columns and 'donor_age' in df.columns:\n        # Convert to numeric and avoid division by zero\n        df_enhanced['age'] = pd.to_numeric(df_enhanced['age'], errors='coerce')\n        df_enhanced['donor_age'] = pd.to_numeric(df_enhanced['donor_age'], errors='coerce')\n        \n        df_enhanced[\"age_receiver_donor_ratio\"] = df_enhanced[\"age\"] / (df_enhanced[\"donor_age\"] + 0.001)\n        df_enhanced[\"age_difference\"] = df_enhanced[\"age\"] - df_enhanced[\"donor_age\"]\n    \n    # 3. STATISTICAL SUMMARY FEATURES\n    print(\"   üìä Creating statistical summary features...\")\n    \n    # Mean chimerism levels across time points\n    cd3_pos_cols = [\"d30_cd3+\", \"d60_cd3+\", \"d100_cd3+\"]\n    cd3_neg_cols = [\"d30_cd3-\", \"d60_cd3-\", \"d100_cd3-\"]\n    \n    df_enhanced[\"mean_cd3+\"] = df_enhanced[cd3_pos_cols].mean(axis=1)\n    df_enhanced[\"mean_cd3-\"] = df_enhanced[cd3_neg_cols].mean(axis=1)\n    \n    # Standard deviation (variability measure)\n    df_enhanced[\"std_cd3+\"] = df_enhanced[cd3_pos_cols].std(axis=1)\n    df_enhanced[\"std_cd3-\"] = df_enhanced[cd3_neg_cols].std(axis=1)\n    \n    # Coefficient of variation (normalized variability)\n    df_enhanced[\"cv_cd3+\"] = df_enhanced[\"std_cd3+\"] / (df_enhanced[\"mean_cd3+\"] + 0.001)\n    df_enhanced[\"cv_cd3-\"] = df_enhanced[\"std_cd3-\"] / (df_enhanced[\"mean_cd3-\"] + 0.001)\n    \n    # Min and Max values\n    df_enhanced[\"min_cd3+\"] = df_enhanced[cd3_pos_cols].min(axis=1)\n    df_enhanced[\"max_cd3+\"] = df_enhanced[cd3_pos_cols].max(axis=1)\n    df_enhanced[\"min_cd3-\"] = df_enhanced[cd3_neg_cols].min(axis=1)\n    df_enhanced[\"max_cd3-\"] = df_enhanced[cd3_neg_cols].max(axis=1)\n    \n    # Range (max - min)\n    df_enhanced[\"range_cd3+\"] = df_enhanced[\"max_cd3+\"] - df_enhanced[\"min_cd3+\"]\n    df_enhanced[\"range_cd3-\"] = df_enhanced[\"max_cd3-\"] - df_enhanced[\"min_cd3-\"]\n    \n    # 4. PERCENTAGE CHANGE FEATURES\n    print(\"   üìà Creating percentage change features...\")\n    \n    # Percentage changes (avoiding division by zero)\n    df_enhanced[\"pct_change_30_100_cd3+\"] = (\n        (df_enhanced[\"d100_cd3+\"] - df_enhanced[\"d30_cd3+\"]) / (df_enhanced[\"d30_cd3+\"] + 0.001) * 100\n    )\n    df_enhanced[\"pct_change_30_100_cd3-\"] = (\n        (df_enhanced[\"d100_cd3-\"] - df_enhanced[\"d30_cd3-\"]) / (df_enhanced[\"d30_cd3-\"] + 0.001) * 100\n    )\n    \n    # 5. SLOPE FEATURES (Linear trend)\n    print(\"   üìâ Creating slope/trend features...\")\n    \n    # Simple slope calculation (change per time unit)\n    time_points = np.array([30, 60, 100])\n    \n    def calculate_slope(row, cols):\n        \"\"\"Calculate linear slope for chimerism values over time.\"\"\"\n        values = row[cols].values\n        # Convert to numeric and check for valid data\n        try:\n            values = pd.to_numeric(values, errors='coerce')\n            if pd.isna(values).any() or len(values) < 2:\n                return np.nan\n            # Filter out any remaining non-numeric values\n            valid_mask = ~pd.isna(values)\n            if valid_mask.sum() < 2:\n                return np.nan\n            valid_values = values[valid_mask]\n            valid_times = time_points[valid_mask]\n            return np.polyfit(valid_times, valid_values, 1)[0]  # slope coefficient\n        except (ValueError, TypeError, np.linalg.LinAlgError):\n            return np.nan\n    \n    df_enhanced[\"slope_cd3+\"] = df_enhanced.apply(\n        lambda row: calculate_slope(row, cd3_pos_cols), axis=1\n    )\n    df_enhanced[\"slope_cd3-\"] = df_enhanced.apply(\n        lambda row: calculate_slope(row, cd3_neg_cols), axis=1\n    )\n    \n    print(f\"‚úÖ Feature engineering completed. Added {len(df_enhanced.columns) - len(df.columns)} new features.\")\n    \n    return df_enhanced\n\n# Apply feature engineering\nif df is not None:\n    df_enhanced = create_chimerism_dynamics_features(df)\n    \n    # Show summary of new features\n    new_features = [col for col in df_enhanced.columns if col not in df.columns]\n    print(f\"\\n=== New Features Created ({len(new_features)}) ===\")\n    for i, feature in enumerate(new_features, 1):\n        print(f\"{i:2d}. {feature}\")\n    \n    # Display sample of enhanced data\n    print(f\"\\n=== Enhanced Dataset Sample ===\")\n    chimerism_features = [col for col in new_features if 'cd3' in col][:8]\n    if chimerism_features:\n        display(df_enhanced[chimerism_features].head())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chimerism Pattern Classification\n",
    "\n",
    "Categorize chimerism changes into interpretable pattern labels for clinical understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_chimerism_trend_labels(df, col_a, col_b, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Assign trend labels based on chimerism changes between time periods.\n",
    "    \n",
    "    This function categorizes the dynamic patterns of chimerism changes\n",
    "    into clinically meaningful labels that can be used for prediction\n",
    "    and interpretation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    col_a : str\n",
    "        First time period change column (e.g., 'd(30-60)_cd3+')\n",
    "    col_b : str\n",
    "        Second time period change column (e.g., 'd(60-100)_cd3+')\n",
    "    threshold : float\n",
    "        Threshold for considering a change significant\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Series with trend labels\n",
    "    \"\"\"\n",
    "    # Convert to categorical based on sign and magnitude\n",
    "    def categorize_change(x, threshold):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        elif x > threshold:\n",
    "            return 1  # Increase\n",
    "        elif x < -threshold:\n",
    "            return -1  # Decrease\n",
    "        else:\n",
    "            return 0  # Stable\n",
    "    \n",
    "    # Categorize changes for both periods\n",
    "    change_a = df[col_a].apply(lambda x: categorize_change(x, threshold))\n",
    "    change_b = df[col_b].apply(lambda x: categorize_change(x, threshold))\n",
    "    \n",
    "    # Define trend patterns based on consecutive changes\n",
    "    def assign_pattern(a, b):\n",
    "        if pd.isna(a) or pd.isna(b):\n",
    "            return 'unknown'\n",
    "        elif a == -1 and b == -1:\n",
    "            return 'consistently_downward'\n",
    "        elif a == 1 and b == 1:\n",
    "            return 'consistently_upward'\n",
    "        elif a == 0 and b == 0:\n",
    "            return 'stable'\n",
    "        elif (a == -1 and b == 1) or (a == 1 and b == -1):\n",
    "            return 'fluctuating'\n",
    "        elif a == -1 and b == 0:\n",
    "            return 'downward_then_stable'\n",
    "        elif a == 0 and b == -1:\n",
    "            return 'stable_then_downward'\n",
    "        elif a == 1 and b == 0:\n",
    "            return 'upward_then_stable'\n",
    "        elif a == 0 and b == 1:\n",
    "            return 'stable_then_upward'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    # Apply pattern assignment\n",
    "    patterns = pd.Series(\n",
    "        [assign_pattern(a, b) for a, b in zip(change_a, change_b)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def create_pattern_features(df):\n",
    "    \"\"\"\n",
    "    Create pattern-based features for chimerism dynamics.\n",
    "    \"\"\"\n",
    "    print(\"üè∑Ô∏è Creating chimerism pattern labels...\")\n",
    "    \n",
    "    df_patterns = df.copy()\n",
    "    \n",
    "    # Check for required difference columns\n",
    "    required_diff_cols = ['d(30-60)_cd3+', 'd(60-100)_cd3+', 'd(30-60)_cd3-', 'd(60-100)_cd3-']\n",
    "    missing_cols = [col for col in required_diff_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è Missing required columns for pattern analysis: {missing_cols}\")\n",
    "        return df_patterns\n",
    "    \n",
    "    # Create pattern labels\n",
    "    df_patterns['cd3+_trend_pattern'] = assign_chimerism_trend_labels(\n",
    "        df, 'd(30-60)_cd3+', 'd(60-100)_cd3+'\n",
    "    )\n",
    "    \n",
    "    df_patterns['cd3-_trend_pattern'] = assign_chimerism_trend_labels(\n",
    "        df, 'd(30-60)_cd3-', 'd(60-100)_cd3-'\n",
    "    )\n",
    "    \n",
    "    # Create simplified binary patterns\n",
    "    def simplify_pattern(pattern):\n",
    "        if pd.isna(pattern) or pattern == 'unknown':\n",
    "            return 'unknown'\n",
    "        elif 'upward' in pattern:\n",
    "            return 'increasing'\n",
    "        elif 'downward' in pattern:\n",
    "            return 'decreasing'\n",
    "        elif pattern == 'stable':\n",
    "            return 'stable'\n",
    "        elif pattern == 'fluctuating':\n",
    "            return 'fluctuating'\n",
    "        else:\n",
    "            return 'mixed'\n",
    "    \n",
    "    df_patterns['cd3+_simple_pattern'] = df_patterns['cd3+_trend_pattern'].apply(simplify_pattern)\n",
    "    df_patterns['cd3-_simple_pattern'] = df_patterns['cd3-_trend_pattern'].apply(simplify_pattern)\n",
    "    \n",
    "    # Create numerical encoding for ML models\n",
    "    pattern_encoder = {\n",
    "        'unknown': 0,\n",
    "        'decreasing': 1,\n",
    "        'stable': 2,\n",
    "        'increasing': 3,\n",
    "        'fluctuating': 4,\n",
    "        'mixed': 5\n",
    "    }\n",
    "    \n",
    "    df_patterns['cd3+_pattern_encoded'] = df_patterns['cd3+_simple_pattern'].map(pattern_encoder)\n",
    "    df_patterns['cd3-_pattern_encoded'] = df_patterns['cd3-_simple_pattern'].map(pattern_encoder)\n",
    "    \n",
    "    print(\"‚úÖ Pattern features created successfully\")\n",
    "    \n",
    "    return df_patterns\n",
    "\n",
    "# Apply pattern analysis\n",
    "if 'df_enhanced' in locals():\n",
    "    df_with_patterns = create_pattern_features(df_enhanced)\n",
    "    \n",
    "    # Analyze pattern distributions\n",
    "    print(f\"\\n=== CD3+ Chimerism Pattern Distribution ===\")\n",
    "    cd3_pos_patterns = df_with_patterns['cd3+_simple_pattern'].value_counts()\n",
    "    for pattern, count in cd3_pos_patterns.items():\n",
    "        percentage = (count / len(df_with_patterns)) * 100\n",
    "        print(f\"{pattern:12s}: {count:3d} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n=== CD3- Chimerism Pattern Distribution ===\")\n",
    "    cd3_neg_patterns = df_with_patterns['cd3-_simple_pattern'].value_counts()\n",
    "    for pattern, count in cd3_neg_patterns.items():\n",
    "        percentage = (count / len(df_with_patterns)) * 100\n",
    "        print(f\"{pattern:12s}: {count:3d} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Show example pattern assignments\n",
    "    print(f\"\\n=== Sample Pattern Assignments ===\")\n",
    "    pattern_cols = ['d(30-60)_cd3+', 'd(60-100)_cd3+', 'cd3+_simple_pattern',\n",
    "                   'd(30-60)_cd3-', 'd(60-100)_cd3-', 'cd3-_simple_pattern']\n",
    "    display(df_with_patterns[pattern_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outcome Association Analysis\n",
    "\n",
    "Explore relationships between chimerism patterns and transplant outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pattern_outcome_associations(df):\n",
    "    \"\"\"\n",
    "    Analyze associations between chimerism patterns and transplant outcomes.\n",
    "    \n",
    "    This function creates cross-tabulations and calculates proportions\n",
    "    to understand which patterns are associated with different outcomes.\n",
    "    \"\"\"\n",
    "    print(\"üîç Analyzing pattern-outcome associations...\")\n",
    "    \n",
    "    # Identify outcome columns\n",
    "    outcome_cols = [col for col in df.columns if col.startswith('y_')]\n",
    "    key_outcomes = ['y_relapse', 'y_death', 'y_agvhd', 'y_cgvhd', 'y_rfs']\n",
    "    available_outcomes = [col for col in key_outcomes if col in outcome_cols]\n",
    "    \n",
    "    if not available_outcomes:\n",
    "        print(\"‚ö†Ô∏è No outcome variables found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä Analyzing {len(available_outcomes)} outcomes: {available_outcomes}\")\n",
    "    \n",
    "    # Analyze CD3+ patterns\n",
    "    if 'cd3+_simple_pattern' in df.columns:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìà CD3+ CHIMERISM PATTERN ASSOCIATIONS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for outcome in available_outcomes:\n",
    "            if outcome in df.columns:\n",
    "                print(f\"\\nüéØ {outcome.upper()}:\")\n",
    "                \n",
    "                # Create cross-tabulation\n",
    "                crosstab = pd.crosstab(\n",
    "                    df['cd3+_simple_pattern'], \n",
    "                    df[outcome], \n",
    "                    margins=True, \n",
    "                    normalize='index'\n",
    "                )\n",
    "                \n",
    "                # Display proportions (excluding totals)\n",
    "                crosstab_display = crosstab.iloc[:-1, :-1]  # Remove margin row and column\n",
    "                \n",
    "                if crosstab_display.shape[1] >= 2:  # Binary outcome\n",
    "                    print(f\"   Pattern breakdown (proportion with outcome = 1):\")\n",
    "                    for pattern in crosstab_display.index:\n",
    "                        if pattern != 'unknown' and 1 in crosstab_display.columns:\n",
    "                            prop = crosstab_display.loc[pattern, 1]\n",
    "                            count = pd.crosstab(df['cd3+_simple_pattern'], df[outcome]).loc[pattern, 1]\n",
    "                            total = pd.crosstab(df['cd3+_simple_pattern'], df[outcome]).loc[pattern].sum()\n",
    "                            print(f\"     {pattern:12s}: {prop:5.1%} ({count}/{total})\")\n",
    "    \n",
    "    # Analyze CD3- patterns\n",
    "    if 'cd3-_simple_pattern' in df.columns:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìâ CD3- CHIMERISM PATTERN ASSOCIATIONS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for outcome in available_outcomes:\n",
    "            if outcome in df.columns:\n",
    "                print(f\"\\nüéØ {outcome.upper()}:\")\n",
    "                \n",
    "                # Create cross-tabulation\n",
    "                crosstab = pd.crosstab(\n",
    "                    df['cd3-_simple_pattern'], \n",
    "                    df[outcome], \n",
    "                    margins=True, \n",
    "                    normalize='index'\n",
    "                )\n",
    "                \n",
    "                # Display proportions (excluding totals)\n",
    "                crosstab_display = crosstab.iloc[:-1, :-1]\n",
    "                \n",
    "                if crosstab_display.shape[1] >= 2:  # Binary outcome\n",
    "                    print(f\"   Pattern breakdown (proportion with outcome = 1):\")\n",
    "                    for pattern in crosstab_display.index:\n",
    "                        if pattern != 'unknown' and 1 in crosstab_display.columns:\n",
    "                            prop = crosstab_display.loc[pattern, 1]\n",
    "                            count = pd.crosstab(df['cd3-_simple_pattern'], df[outcome]).loc[pattern, 1]\n",
    "                            total = pd.crosstab(df['cd3-_simple_pattern'], df[outcome]).loc[pattern].sum()\n",
    "                            print(f\"     {pattern:12s}: {prop:5.1%} ({count}/{total})\")\n",
    "\n",
    "def create_association_visualization(df):\n",
    "    \"\"\"\n",
    "    Create visualizations of pattern-outcome associations.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä Creating association visualizations...\")\n",
    "    \n",
    "    # Focus on relapse outcome if available\n",
    "    if 'y_relapse' not in df.columns:\n",
    "        print(\"‚ö†Ô∏è y_relapse column not found for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # CD3+ pattern vs relapse\n",
    "    if 'cd3+_simple_pattern' in df.columns:\n",
    "        crosstab_cd3pos = pd.crosstab(df['cd3+_simple_pattern'], df['y_relapse'], normalize='index')\n",
    "        if 1 in crosstab_cd3pos.columns:\n",
    "            relapse_props = crosstab_cd3pos[1].sort_values(ascending=True)\n",
    "            \n",
    "            axes[0].barh(range(len(relapse_props)), relapse_props.values, \n",
    "                        color='lightcoral', alpha=0.7)\n",
    "            axes[0].set_yticks(range(len(relapse_props)))\n",
    "            axes[0].set_yticklabels(relapse_props.index)\n",
    "            axes[0].set_xlabel('Proportion with Relapse')\n",
    "            axes[0].set_title('CD3+ Pattern vs Relapse Risk')\n",
    "            axes[0].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, v in enumerate(relapse_props.values):\n",
    "                axes[0].text(v + 0.01, i, f'{v:.2%}', va='center', fontsize=9)\n",
    "    \n",
    "    # CD3- pattern vs relapse\n",
    "    if 'cd3-_simple_pattern' in df.columns:\n",
    "        crosstab_cd3neg = pd.crosstab(df['cd3-_simple_pattern'], df['y_relapse'], normalize='index')\n",
    "        if 1 in crosstab_cd3neg.columns:\n",
    "            relapse_props = crosstab_cd3neg[1].sort_values(ascending=True)\n",
    "            \n",
    "            axes[1].barh(range(len(relapse_props)), relapse_props.values, \n",
    "                        color='lightblue', alpha=0.7)\n",
    "            axes[1].set_yticks(range(len(relapse_props)))\n",
    "            axes[1].set_yticklabels(relapse_props.index)\n",
    "            axes[1].set_xlabel('Proportion with Relapse')\n",
    "            axes[1].set_title('CD3- Pattern vs Relapse Risk')\n",
    "            axes[1].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, v in enumerate(relapse_props.values):\n",
    "                axes[1].text(v + 0.01, i, f'{v:.2%}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run association analysis\n",
    "if 'df_with_patterns' in locals():\n",
    "    analyze_pattern_outcome_associations(df_with_patterns)\n",
    "    create_association_visualization(df_with_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing and Quality Assessment\n",
    "\n",
    "Prepare data for machine learning by handling missing values and filtering patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_missing_value_imputation(df):\n",
    "    \"\"\"\n",
    "    Apply sophisticated missing value imputation strategies.\n",
    "    \n",
    "    This function uses multiple imputation strategies based on\n",
    "    the nature and amount of missing data in each column.\n",
    "    \"\"\"\n",
    "    print(\"üîß Applying advanced missing value imputation...\")\n",
    "    \n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Separate column types\n",
    "    numerical_cols = df_imputed.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = df_imputed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"   üìä Processing {len(numerical_cols)} numerical columns\")\n",
    "    print(f\"   üè∑Ô∏è Processing {len(categorical_cols)} categorical columns\")\n",
    "    \n",
    "    # 1. NUMERICAL COLUMNS IMPUTATION\n",
    "    if numerical_cols:\n",
    "        # Analyze missing patterns\n",
    "        missing_analysis = df_imputed[numerical_cols].isnull().sum().sort_values(ascending=False)\n",
    "        high_missing = missing_analysis[missing_analysis > len(df_imputed) * 0.5].index.tolist()\n",
    "        moderate_missing = missing_analysis[(missing_analysis > 0) & (missing_analysis <= len(df_imputed) * 0.5)].index.tolist()\n",
    "        \n",
    "        print(f\"   üìâ High missing (>50%): {len(high_missing)} columns\")\n",
    "        print(f\"   üìä Moderate missing (‚â§50%): {len(moderate_missing)} columns\")\n",
    "        \n",
    "        # For columns with high missing values, use simple median imputation\n",
    "        if high_missing:\n",
    "            print(f\"      Applying median imputation to high-missing columns\")\n",
    "            simple_imputer = SimpleImputer(strategy='median')\n",
    "            df_imputed[high_missing] = simple_imputer.fit_transform(df_imputed[high_missing])\n",
    "        \n",
    "        # For columns with moderate missing values, use more sophisticated methods\n",
    "        if moderate_missing:\n",
    "            print(f\"      Applying iterative imputation to moderate-missing columns\")\n",
    "            try:\n",
    "                # Use Iterative Imputer (MICE) for better accuracy\n",
    "                iterative_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "                df_imputed[moderate_missing] = iterative_imputer.fit_transform(df_imputed[moderate_missing])\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ö†Ô∏è Iterative imputation failed: {str(e)}\")\n",
    "                print(f\"      Falling back to median imputation\")\n",
    "                fallback_imputer = SimpleImputer(strategy='median')\n",
    "                df_imputed[moderate_missing] = fallback_imputer.fit_transform(df_imputed[moderate_missing])\n",
    "    \n",
    "    # 2. CATEGORICAL COLUMNS IMPUTATION\n",
    "    if categorical_cols:\n",
    "        print(f\"   üè∑Ô∏è Applying mode imputation to categorical columns\")\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df_imputed[categorical_cols] = cat_imputer.fit_transform(df_imputed[categorical_cols])\n",
    "    \n",
    "    # 3. VERIFICATION\n",
    "    remaining_missing = df_imputed.isnull().sum().sum()\n",
    "    print(f\"‚úÖ Imputation completed. Remaining missing values: {remaining_missing}\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "def prepare_ml_dataset(df):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for machine learning analysis.\n",
    "    \n",
    "    This function:\n",
    "    1. Filters for AML/MDS patients (disease == 1)\n",
    "    2. Removes columns with excessive missing data\n",
    "    3. Applies imputation strategies\n",
    "    4. Splits features and targets\n",
    "    \"\"\"\n",
    "    print(\"üéØ Preparing dataset for machine learning...\")\n",
    "    \n",
    "    # 1. PATIENT FILTERING\n",
    "    if 'disease' in df.columns:\n",
    "        aml_mds_patients = df[df['disease'] == 1].copy()\n",
    "        print(f\"   üë• Filtered to AML/MDS patients: {len(aml_mds_patients)} from {len(df)}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è 'disease' column not found, using all patients\")\n",
    "        aml_mds_patients = df.copy()\n",
    "    \n",
    "    # 2. FEATURE SELECTION AND CLEANUP\n",
    "    # Remove columns with too much missing data or not suitable for ML\n",
    "    exclude_columns = [\n",
    "        'dose_dli', 'dose_dli_2', 'indication_for_dli', 'post_dli_gvhd',\n",
    "        'grade_at_onset', 'time_to_onset', 'highest_grade', 'dli',\n",
    "        'aml_eln_risk_category', 'disease_risk_index', \n",
    "        'time_from_diagnosis_to_alloSCT', 'hct_ci_score'\n",
    "    ]\n",
    "    \n",
    "    # Identify feature columns (non-outcome variables)\n",
    "    feature_cols = [col for col in aml_mds_patients.columns \n",
    "                   if not col.startswith('y_') and col not in exclude_columns]\n",
    "    \n",
    "    # Identify outcome columns\n",
    "    classification_labels = ['y_rfs', 'y_death', 'y_relapse', 'y_cgvhd', 'y_agvhd']\n",
    "    regression_labels = ['y_os_days', 'y_rfs_days']\n",
    "    \n",
    "    # Filter to available columns\n",
    "    available_class_labels = [col for col in classification_labels if col in aml_mds_patients.columns]\n",
    "    available_reg_labels = [col for col in regression_labels if col in aml_mds_patients.columns]\n",
    "    \n",
    "    print(f\"   üìä Features: {len(feature_cols)} columns\")\n",
    "    print(f\"   üéØ Classification targets: {len(available_class_labels)} columns\")\n",
    "    print(f\"   üìà Regression targets: {len(available_reg_labels)} columns\")\n",
    "    \n",
    "    # 3. REQUIRE MINIMUM CHIMERISM DATA\n",
    "    # Remove patients without key chimerism measurements\n",
    "    key_chimerism_cols = ['d100_cd3+', 'd100_cd3-']\n",
    "    before_filter = len(aml_mds_patients)\n",
    "    \n",
    "    for col in key_chimerism_cols:\n",
    "        if col in aml_mds_patients.columns:\n",
    "            aml_mds_patients = aml_mds_patients.dropna(subset=[col])\n",
    "    \n",
    "    after_filter = len(aml_mds_patients)\n",
    "    print(f\"   üßπ Removed patients missing key chimerism data: {before_filter - after_filter}\")\n",
    "    \n",
    "    # 4. EXTRACT AND PROCESS DATA\n",
    "    X = aml_mds_patients[feature_cols].copy()\n",
    "    y_classification = aml_mds_patients[available_class_labels].copy() if available_class_labels else pd.DataFrame()\n",
    "    y_regression = aml_mds_patients[available_reg_labels].copy() if available_reg_labels else pd.DataFrame()\n",
    "    \n",
    "    # 5. APPLY IMPUTATION\n",
    "    X_imputed = advanced_missing_value_imputation(X)\n",
    "    \n",
    "    # 6. RESET INDICES\n",
    "    X_imputed.reset_index(drop=True, inplace=True)\n",
    "    y_classification.reset_index(drop=True, inplace=True)\n",
    "    y_regression.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset preparation completed\")\n",
    "    print(f\"   Final shapes: X={X_imputed.shape}, y_class={y_classification.shape}, y_reg={y_regression.shape}\")\n",
    "    \n",
    "    return X_imputed, y_classification, y_regression\n",
    "\n",
    "# Prepare the dataset\n",
    "if 'df_with_patterns' in locals():\n",
    "    X_processed, y_classification, y_regression = prepare_ml_dataset(df_with_patterns)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(f\"\\n=== Processed Dataset Summary ===\")\n",
    "    print(f\"Features (X): {X_processed.shape[0]} samples √ó {X_processed.shape[1]} features\")\n",
    "    print(f\"Classification targets: {y_classification.shape[1]} outcomes\")\n",
    "    print(f\"Regression targets: {y_regression.shape[1]} outcomes\")\n",
    "    \n",
    "    # Show feature types\n",
    "    if not X_processed.empty:\n",
    "        print(f\"\\n=== Feature Information ===\")\n",
    "        feature_info = X_processed.dtypes.value_counts()\n",
    "        for dtype, count in feature_info.items():\n",
    "            print(f\"{dtype}: {count} features\")\n",
    "        \n",
    "        print(f\"\\nMissing values check: {X_processed.isnull().sum().sum()} total missing\")\n",
    "        \n",
    "        # Show key chimerism features\n",
    "        chimerism_features = [col for col in X_processed.columns if 'cd3' in col.lower()]\n",
    "        print(f\"\\nChimerism-related features ({len(chimerism_features)}):\")\n",
    "        for i, feature in enumerate(chimerism_features[:10], 1):\n",
    "            print(f\"{i:2d}. {feature}\")\n",
    "        if len(chimerism_features) > 10:\n",
    "            print(f\"    ... and {len(chimerism_features) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis and Visualization\n",
    "\n",
    "Create comprehensive visualizations to understand data distributions and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_eda(X, y_classification):\n",
    "    \"\"\"\n",
    "    Create comprehensive exploratory data analysis visualizations.\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating comprehensive exploratory data analysis...\")\n",
    "    \n",
    "    # 1. FEATURE DISTRIBUTION ANALYSIS\n",
    "    print(\"\\nüìà 1. Feature Distribution Analysis\")\n",
    "    \n",
    "    # Focus on key chimerism features\n",
    "    chimerism_cols = [col for col in X.columns if any(x in col.lower() for x in ['cd3', 'chimerism'])]\n",
    "    key_features = chimerism_cols[:12] if chimerism_cols else X.columns[:12]\n",
    "    \n",
    "    if key_features:\n",
    "        n_features = len(key_features)\n",
    "        n_cols = min(4, n_features)\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, feature in enumerate(key_features):\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            \n",
    "            # Create histogram\n",
    "            axes[row, col].hist(X[feature].dropna(), bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            axes[row, col].set_title(f'{feature}', fontsize=10)\n",
    "            axes[row, col].set_xlabel('Value')\n",
    "            axes[row, col].set_ylabel('Frequency')\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add basic statistics\n",
    "            mean_val = X[feature].mean()\n",
    "            axes[row, col].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "            axes[row, col].legend(fontsize=8)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(key_features), n_rows * n_cols):\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            axes[row, col].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Key Feature Distributions', fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. CORRELATION ANALYSIS\n",
    "    print(\"\\nüîó 2. Feature Correlation Analysis\")\n",
    "    \n",
    "    if chimerism_cols:\n",
    "        # Focus on chimerism-related features for correlation\n",
    "        corr_features = chimerism_cols[:15]  # Limit to avoid overcrowding\n",
    "        \n",
    "        correlation_matrix = X[corr_features].corr()\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Show only lower triangle\n",
    "        sns.heatmap(\n",
    "            correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            fmt='.2f', \n",
    "            annot_kws={\"size\": 8}, \n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8}\n",
    "        )\n",
    "        plt.title('Chimerism Feature Correlation Matrix', fontsize=14, pad=20)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Identify highly correlated features\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                corr_val = correlation_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.8:\n",
    "                    high_corr_pairs.append((\n",
    "                        correlation_matrix.columns[i],\n",
    "                        correlation_matrix.columns[j],\n",
    "                        corr_val\n",
    "                    ))\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            print(f\"\\n‚ö†Ô∏è Highly correlated feature pairs (|r| > 0.8):\")\n",
    "            for feat1, feat2, corr in high_corr_pairs:\n",
    "                print(f\"   {feat1} ‚Üî {feat2}: {corr:.3f}\")\n",
    "    \n",
    "    # 3. OUTCOME DISTRIBUTION ANALYSIS\n",
    "    print(\"\\nüéØ 3. Outcome Distribution Analysis\")\n",
    "    \n",
    "    if not y_classification.empty:\n",
    "        n_outcomes = len(y_classification.columns)\n",
    "        if n_outcomes > 0:\n",
    "            fig, axes = plt.subplots(1, min(n_outcomes, 5), figsize=(15, 4))\n",
    "            if n_outcomes == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            outcome_names = ['RFS', 'Death', 'Relapse', 'cGVHD', 'aGVHD']\n",
    "            \n",
    "            for i, (col, name) in enumerate(zip(y_classification.columns, outcome_names)):\n",
    "                if i >= 5:  # Limit to 5 outcomes\n",
    "                    break\n",
    "                    \n",
    "                counts = y_classification[col].value_counts().sort_index()\n",
    "                \n",
    "                # Create bar plot\n",
    "                bars = axes[i].bar(\n",
    "                    ['No', 'Yes'], \n",
    "                    [counts.get(0, 0), counts.get(1, 0)], \n",
    "                    color=['lightblue', 'lightcoral'],\n",
    "                    alpha=0.7,\n",
    "                    edgecolor='black'\n",
    "                )\n",
    "                \n",
    "                axes[i].set_title(f'{name}', fontsize=12)\n",
    "                axes[i].set_ylabel('Count')\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar, count in zip(bars, [counts.get(0, 0), counts.get(1, 0)]):\n",
    "                    height = bar.get_height()\n",
    "                    axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                               f'{count}', ha='center', va='bottom', fontsize=10)\n",
    "                \n",
    "                # Add percentage\n",
    "                total = counts.sum()\n",
    "                if total > 0:\n",
    "                    yes_pct = (counts.get(1, 0) / total) * 100\n",
    "                    axes[i].text(0.5, max(counts) * 0.8, f'{yes_pct:.1f}%\\npositive', \n",
    "                               ha='center', va='center', fontsize=9,\n",
    "                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            plt.suptitle('Outcome Distributions', fontsize=16, y=1.05)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def create_chimerism_time_series_plot(X):\n",
    "    \"\"\"\n",
    "    Create time series visualization of chimerism changes.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìà 4. Chimerism Time Series Analysis\")\n",
    "    \n",
    "    # Check for time series data\n",
    "    time_cols_cd3pos = ['d30_cd3+', 'd60_cd3+', 'd100_cd3+']\n",
    "    time_cols_cd3neg = ['d30_cd3-', 'd60_cd3-', 'd100_cd3-']\n",
    "    \n",
    "    available_cd3pos = [col for col in time_cols_cd3pos if col in X.columns]\n",
    "    available_cd3neg = [col for col in time_cols_cd3neg if col in X.columns]\n",
    "    \n",
    "    if len(available_cd3pos) >= 2 or len(available_cd3neg) >= 2:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        time_points = [30, 60, 100]\n",
    "        \n",
    "        # CD3+ time series\n",
    "        if len(available_cd3pos) >= 2:\n",
    "            cd3pos_data = X[available_cd3pos].dropna()\n",
    "            \n",
    "            # Plot individual patient trajectories (sample)\n",
    "            sample_size = min(50, len(cd3pos_data))\n",
    "            sample_indices = np.random.choice(cd3pos_data.index, sample_size, replace=False)\n",
    "            \n",
    "            for idx in sample_indices:\n",
    "                values = cd3pos_data.loc[idx, available_cd3pos].values\n",
    "                axes[0].plot(time_points[:len(values)], values, \n",
    "                           color='blue', alpha=0.1, linewidth=0.5)\n",
    "            \n",
    "            # Plot mean trajectory\n",
    "            mean_values = cd3pos_data[available_cd3pos].mean()\n",
    "            axes[0].plot(time_points[:len(mean_values)], mean_values, \n",
    "                        color='red', linewidth=3, marker='o', label='Mean')\n",
    "            \n",
    "            # Plot percentiles\n",
    "            p25_values = cd3pos_data[available_cd3pos].quantile(0.25)\n",
    "            p75_values = cd3pos_data[available_cd3pos].quantile(0.75)\n",
    "            axes[0].fill_between(time_points[:len(p25_values)], p25_values, p75_values, \n",
    "                                alpha=0.3, color='gray', label='25-75th percentile')\n",
    "            \n",
    "            axes[0].set_title('CD3+ Chimerism Over Time', fontsize=12)\n",
    "            axes[0].set_xlabel('Days post-transplant')\n",
    "            axes[0].set_ylabel('CD3+ Chimerism (%)')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # CD3- time series\n",
    "        if len(available_cd3neg) >= 2:\n",
    "            cd3neg_data = X[available_cd3neg].dropna()\n",
    "            \n",
    "            # Plot individual patient trajectories (sample)\n",
    "            sample_size = min(50, len(cd3neg_data))\n",
    "            sample_indices = np.random.choice(cd3neg_data.index, sample_size, replace=False)\n",
    "            \n",
    "            for idx in sample_indices:\n",
    "                values = cd3neg_data.loc[idx, available_cd3neg].values\n",
    "                axes[1].plot(time_points[:len(values)], values, \n",
    "                           color='green', alpha=0.1, linewidth=0.5)\n",
    "            \n",
    "            # Plot mean trajectory\n",
    "            mean_values = cd3neg_data[available_cd3neg].mean()\n",
    "            axes[1].plot(time_points[:len(mean_values)], mean_values, \n",
    "                        color='red', linewidth=3, marker='o', label='Mean')\n",
    "            \n",
    "            # Plot percentiles\n",
    "            p25_values = cd3neg_data[available_cd3neg].quantile(0.25)\n",
    "            p75_values = cd3neg_data[available_cd3neg].quantile(0.75)\n",
    "            axes[1].fill_between(time_points[:len(p25_values)], p25_values, p75_values, \n",
    "                                alpha=0.3, color='gray', label='25-75th percentile')\n",
    "            \n",
    "            axes[1].set_title('CD3- Chimerism Over Time', fontsize=12)\n",
    "            axes[1].set_xlabel('Days post-transplant')\n",
    "            axes[1].set_ylabel('CD3- Chimerism (%)')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Chimerism Dynamics Over Time', fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run EDA\n",
    "if 'X_processed' in locals() and 'y_classification' in locals():\n",
    "    create_comprehensive_eda(X_processed, y_classification)\n",
    "    create_chimerism_time_series_plot(X_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature-Specific Machine Learning Analysis\n",
    "\n",
    "Test different combinations of chimerism features to identify the most predictive sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_sets_for_analysis():\n",
    "    \"\"\"\n",
    "    Define different feature sets for comparative analysis.\n",
    "    \n",
    "    This function creates multiple feature combinations to test\n",
    "    which aspects of chimerism dynamics are most predictive.\n",
    "    \"\"\"\n",
    "    feature_sets = {\n",
    "        \"dynamics_only\": {\n",
    "            \"name\": \"Chimerism Dynamics Only\",\n",
    "            \"features\": [\"d(30-60)_cd3+\", \"d(60-100)_cd3+\", \"d(30-60)_cd3-\", \"d(60-100)_cd3-\"],\n",
    "            \"description\": \"Only time-point differences\"\n",
    "        },\n",
    "        \"timepoints_only\": {\n",
    "            \"name\": \"Time Points Only\", \n",
    "            \"features\": [\"d30_cd3+\", \"d30_cd3-\", \"d60_cd3+\", \"d60_cd3-\", \"d100_cd3+\", \"d100_cd3-\"],\n",
    "            \"description\": \"Raw chimerism values at each time point\"\n",
    "        },\n",
    "        \"statistics_only\": {\n",
    "            \"name\": \"Statistical Features Only\",\n",
    "            \"features\": [\"mean_cd3+\", \"mean_cd3-\", \"std_cd3+\", \"std_cd3-\", \"cv_cd3+\", \"cv_cd3-\"],\n",
    "            \"description\": \"Statistical summaries across time points\"\n",
    "        },\n",
    "        \"patterns_only\": {\n",
    "            \"name\": \"Pattern Features Only\",\n",
    "            \"features\": [\"cd3+_pattern_encoded\", \"cd3-_pattern_encoded\"],\n",
    "            \"description\": \"Encoded trend patterns\"\n",
    "        },\n",
    "        \"comprehensive\": {\n",
    "            \"name\": \"Comprehensive Chimerism\",\n",
    "            \"features\": [\n",
    "                \"d30_cd3+\", \"d60_cd3+\", \"d100_cd3+\", \"d30_cd3-\", \"d60_cd3-\", \"d100_cd3-\",\n",
    "                \"d(30-60)_cd3+\", \"d(60-100)_cd3+\", \"d(30-60)_cd3-\", \"d(60-100)_cd3-\",\n",
    "                \"mean_cd3+\", \"mean_cd3-\", \"std_cd3+\", \"std_cd3-\", \"cv_cd3+\", \"cv_cd3-\"\n",
    "            ],\n",
    "            \"description\": \"All chimerism-related features\"\n",
    "        },\n",
    "        \"minimal_predictive\": {\n",
    "            \"name\": \"Minimal Predictive Set\",\n",
    "            \"features\": [\"d(60-100)_cd3+\", \"d100_cd3-\", \"std_cd3+\"],\n",
    "            \"description\": \"Top 3 most predictive features\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return feature_sets\n",
    "\n",
    "def standardize_and_encode_features(X):\n",
    "    \"\"\"\n",
    "    Apply standardization and encoding to features for ML models.\n",
    "    \"\"\"\n",
    "    print(\"üîß Standardizing and encoding features...\")\n",
    "    \n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    # Handle categorical variables first\n",
    "    categorical_cols = X_processed.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"   üè∑Ô∏è Encoding {len(categorical_cols)} categorical columns\")\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_processed[col] = le.fit_transform(X_processed[col].astype(str))\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    numerical_cols = X_processed.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if len(numerical_cols) > 0:\n",
    "        print(f\"   üìä Standardizing {len(numerical_cols)} numerical columns\")\n",
    "        scaler = StandardScaler()\n",
    "        X_processed[numerical_cols] = scaler.fit_transform(X_processed[numerical_cols])\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "def process_targets(y_classification):\n",
    "    \"\"\"\n",
    "    Process and clean target variables.\n",
    "    \"\"\"\n",
    "    print(\"üéØ Processing target variables...\")\n",
    "    \n",
    "    y_processed = y_classification.copy()\n",
    "    \n",
    "    # Handle each target column\n",
    "    for col in y_processed.columns:\n",
    "        # Convert to binary if needed\n",
    "        if y_processed[col].dtype in ['float64', 'int64']:\n",
    "            unique_vals = y_processed[col].dropna().unique()\n",
    "            if len(unique_vals) > 2:\n",
    "                # Convert to binary using median split or specific logic\n",
    "                y_processed[col] = pd.cut(\n",
    "                    y_processed[col], \n",
    "                    bins=[-np.inf, 0, np.inf], \n",
    "                    labels=[0, 1]\n",
    "                )\n",
    "        \n",
    "        # Encode as integers\n",
    "        le = LabelEncoder()\n",
    "        y_processed[col] = le.fit_transform(y_processed[col].astype(str))\n",
    "    \n",
    "    # Remove constant columns\n",
    "    non_constant_cols = [col for col in y_processed.columns if y_processed[col].nunique() > 1]\n",
    "    y_processed = y_processed[non_constant_cols]\n",
    "    \n",
    "    print(f\"   ‚úÖ Processed {len(y_processed.columns)} target variables\")\n",
    "    \n",
    "    return y_processed\n",
    "\n",
    "def evaluate_feature_set_performance(X, y, feature_set_name, features, description):\n",
    "    \"\"\"\n",
    "    Evaluate performance of a specific feature set.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¨ Evaluating: {feature_set_name}\")\n",
    "    print(f\"   üìù {description}\")\n",
    "    \n",
    "    # Check feature availability\n",
    "    available_features = [f for f in features if f in X.columns]\n",
    "    missing_features = [f for f in features if f not in X.columns]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"   ‚ö†Ô∏è Missing features: {missing_features}\")\n",
    "    \n",
    "    if len(available_features) == 0:\n",
    "        print(f\"   ‚ùå No features available for this set\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"   ‚úÖ Using {len(available_features)} features: {available_features}\")\n",
    "    \n",
    "    # Extract feature subset\n",
    "    X_subset = X[available_features]\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {}\n",
    "    \n",
    "    # Test on each target\n",
    "    target_names = ['RFS', 'Death', 'Relapse', 'cGVHD', 'aGVHD']\n",
    "    \n",
    "    for i, (target_col, target_name) in enumerate(zip(y.columns, target_names)):\n",
    "        if i >= len(target_names):\n",
    "            break\n",
    "            \n",
    "        # Get target data\n",
    "        y_target = y[target_col].dropna()\n",
    "        X_target = X_subset.loc[y_target.index]\n",
    "        \n",
    "        # Check if we have enough data and multiple classes\n",
    "        if len(y_target.unique()) < 2 or len(y_target) < 10:\n",
    "            print(f\"     ‚ö†Ô∏è {target_name}: Insufficient data or classes\")\n",
    "            continue\n",
    "        \n",
    "        # Train simple Random Forest model\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(rf, X_target, y_target, cv=5, scoring='accuracy')\n",
    "        \n",
    "        results[target_name] = {\n",
    "            'accuracy_mean': np.mean(cv_scores),\n",
    "            'accuracy_std': np.std(cv_scores),\n",
    "            'n_samples': len(y_target),\n",
    "            'n_features': len(available_features)\n",
    "        }\n",
    "        \n",
    "        print(f\"     üìä {target_name}: {np.mean(cv_scores):.3f} ¬± {np.std(cv_scores):.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run feature set analysis\n",
    "if 'X_processed' in locals() and 'y_classification' in locals():\n",
    "    # Prepare data for ML\n",
    "    X_ml = standardize_and_encode_features(X_processed)\n",
    "    y_ml = process_targets(y_classification)\n",
    "    \n",
    "    # Get feature sets\n",
    "    feature_sets = create_feature_sets_for_analysis()\n",
    "    \n",
    "    # Evaluate each feature set\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üß™ FEATURE SET PERFORMANCE ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for set_id, set_info in feature_sets.items():\n",
    "        results = evaluate_feature_set_performance(\n",
    "            X_ml, y_ml, \n",
    "            set_info['name'], \n",
    "            set_info['features'], \n",
    "            set_info['description']\n",
    "        )\n",
    "        \n",
    "        if results:\n",
    "            all_results[set_info['name']] = results\n",
    "    \n",
    "    # Create summary comparison\n",
    "    if all_results:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä PERFORMANCE SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create comparison DataFrame\n",
    "        comparison_data = []\n",
    "        for set_name, set_results in all_results.items():\n",
    "            for target, metrics in set_results.items():\n",
    "                comparison_data.append({\n",
    "                    'Feature_Set': set_name,\n",
    "                    'Target': target,\n",
    "                    'Accuracy': metrics['accuracy_mean'],\n",
    "                    'Std': metrics['accuracy_std'],\n",
    "                    'N_Features': metrics['n_features']\n",
    "                })\n",
    "        \n",
    "        if comparison_data:\n",
    "            comparison_df = pd.DataFrame(comparison_data)\n",
    "            \n",
    "            # Show best performance for each target\n",
    "            for target in comparison_df['Target'].unique():\n",
    "                target_data = comparison_df[comparison_df['Target'] == target]\n",
    "                best_idx = target_data['Accuracy'].idxmax()\n",
    "                best_result = target_data.loc[best_idx]\n",
    "                \n",
    "                print(f\"\\nüèÜ {target}:\")\n",
    "                print(f\"   Best: {best_result['Feature_Set']}\")\n",
    "                print(f\"   Accuracy: {best_result['Accuracy']:.3f} ¬± {best_result['Std']:.3f}\")\n",
    "                print(f\"   Features: {best_result['N_Features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training and Validation\n",
    "\n",
    "Train and validate models using the most promising feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_and_save_optimized_models(X, y, save_dir=\"models\"):\n    \"\"\"\n    Train optimized models with the best feature combinations and save them.\n    \"\"\"\n    print(f\"\\nüöÄ Training and saving optimized models...\")\n    \n    # Create models directory\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define optimal feature combinations based on analysis\n    # These would be determined from the previous analysis\n    optimal_combinations = {\n        1: [\"d(60-100)_cd3+\"],\n        2: [\"d(60-100)_cd3+\", \"d(60-100)_cd3-\"],\n        3: [\"d100_cd3-\", \"d(60-100)_cd3+\", \"d(60-100)_cd3-\"],\n        5: [\"d100_cd3-\", \"d(60-100)_cd3+\", \"d(60-100)_cd3-\", \"std_cd3+\", \"std_cd3-\"]\n    }\n    \n    saved_models = {}\n    \n    for k, features in optimal_combinations.items():\n        print(f\"\\nüìä Training models with k={k} features: {features}\")\n        \n        # Check feature availability\n        available_features = [f for f in features if f in X.columns]\n        if len(available_features) == 0:\n            print(f\"   ‚ùå No features available for k={k}\")\n            continue\n        \n        X_subset = X[available_features]\n        \n        # Train models for each target\n        target_names = ['RFS', 'Death', 'Relapse', 'cGVHD', 'aGVHD']\n        \n        for i, (target_col, target_name) in enumerate(zip(y.columns, target_names)):\n            if i >= len(target_names):\n                break\n                \n            # Get target data\n            y_target = y[target_col].dropna()\n            X_target = X_subset.loc[y_target.index]\n            \n            if len(y_target.unique()) < 2 or len(y_target) < 10:\n                continue\n            \n            # Train Random Forest with hyperparameter tuning\n            param_grid = {\n                'n_estimators': [50, 100, 200],\n                'max_depth': [5, 10, None],\n                'min_samples_split': [2, 5],\n                'min_samples_leaf': [1, 2]\n            }\n            \n            rf = RandomForestClassifier(random_state=42)\n            grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n            grid_search.fit(X_target, y_target)\n            \n            best_model = grid_search.best_estimator_\n            \n            # Evaluate model\n            cv_scores = cross_val_score(best_model, X_target, y_target, cv=5, scoring='accuracy')\n            \n            # Save model\n            model_filename = f\"{save_dir}/best_{target_name.lower()}_k{k}_model.joblib\"\n            \n            model_info = {\n                'model': best_model,\n                'features': available_features,\n                'target': target_name,\n                'k': k,\n                'accuracy_mean': np.mean(cv_scores),\n                'accuracy_std': np.std(cv_scores),\n                'best_params': grid_search.best_params_,\n                'n_samples': len(y_target),\n                'trained_on': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            }\n            \n            joblib.dump(model_info, model_filename)\n            \n            # Store in results\n            model_key = f\"{target_name}_k{k}\"\n            saved_models[model_key] = {\n                'filename': model_filename,\n                'accuracy': np.mean(cv_scores),\n                'features': available_features\n            }\n            \n            print(f\"   ‚úÖ {target_name} k={k}: {np.mean(cv_scores):.3f} ¬± {np.std(cv_scores):.3f}\")\n    \n    print(f\"\\nüíæ Saved {len(saved_models)} models to {save_dir}/\")\n    return saved_models\n\ndef create_model_performance_summary(saved_models):\n    \"\"\"\n    Create a comprehensive summary of model performance.\n    \"\"\"\n    if not saved_models:\n        print(\"No models to summarize\")\n        return\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"üìà MODEL PERFORMANCE SUMMARY\")\n    print(f\"{'='*60}\")\n    \n    # Group by target\n    targets = set([key.split('_k')[0] for key in saved_models.keys()])\n    \n    for target in sorted(targets):\n        print(f\"\\nüéØ {target.upper()}:\")\n        \n        target_models = {k: v for k, v in saved_models.items() if k.startswith(target)}\n        \n        # Sort by accuracy\n        sorted_models = sorted(target_models.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n        \n        for i, (model_key, model_info) in enumerate(sorted_models, 1):\n            k_value = model_key.split('_k')[1]\n            print(f\"   {i}. k={k_value}: {model_info['accuracy']:.3f} ({len(model_info['features'])} features)\")\n            print(f\"      Features: {', '.join(model_info['features'])}\")\n    \n    # Overall best performers\n    print(f\"\\nüèÜ OVERALL BEST PERFORMERS:\")\n    all_models = [(k, v['accuracy'], v['features']) for k, v in saved_models.items()]\n    all_models.sort(key=lambda x: x[1], reverse=True)\n    \n    for i, (model_key, accuracy, features) in enumerate(all_models[:5], 1):\n        target, k = model_key.split('_k')\n        print(f\"   {i}. {target} (k={k}): {accuracy:.3f}\")\n        print(f\"      Top feature: {features[0] if features else 'N/A'}\")\n\ndef export_analysis_results(saved_models, X, y, base_filename=\"basic_analysis_results\"):\n    \"\"\"\n    Export comprehensive analysis results to CSV files.\n    \"\"\"\n    print(f\"\\nüìä Exporting analysis results to CSV files...\")\n    \n    try:\n        # 1. Model performance summary\n        model_data = []\n        for model_key, model_info in saved_models.items():\n            target, k = model_key.split('_k')\n            model_data.append({\n                'Target': target,\n                'K_Features': int(k),\n                'Accuracy': model_info['accuracy'],\n                'N_Features': len(model_info['features']),\n                'Top_Feature': model_info['features'][0] if model_info['features'] else '',\n                'Features': ', '.join(model_info['features']),\n                'Filename': model_info['filename']\n            })\n        \n        if model_data:\n            model_df = pd.DataFrame(model_data)\n            model_df.to_csv(f\"{base_filename}_model_performance.csv\", index=False)\n            print(f\"   ‚úÖ Model performance saved to {base_filename}_model_performance.csv\")\n        \n        # 2. Feature statistics\n        chimerism_cols = [col for col in X.columns if 'cd3' in col.lower()]\n        if chimerism_cols:\n            feature_stats = X[chimerism_cols].describe().T\n            feature_stats.to_csv(f\"{base_filename}_feature_statistics.csv\")\n            print(f\"   ‚úÖ Feature statistics saved to {base_filename}_feature_statistics.csv\")\n        \n        # 3. Outcome distributions\n        outcome_stats = []\n        target_names = ['RFS', 'Death', 'Relapse', 'cGVHD', 'aGVHD']\n        \n        for i, (col, name) in enumerate(zip(y.columns, target_names)):\n            if i >= len(target_names):\n                break\n            counts = y[col].value_counts().sort_index()\n            outcome_stats.append({\n                'Outcome': name,\n                'No_Count': counts.get(0, 0),\n                'Yes_Count': counts.get(1, 0),\n                'Total': counts.sum(),\n                'Positive_Rate': counts.get(1, 0) / counts.sum() if counts.sum() > 0 else 0\n            })\n        \n        if outcome_stats:\n            outcome_df = pd.DataFrame(outcome_stats)\n            outcome_df.to_csv(f\"{base_filename}_outcome_distributions.csv\", index=False)\n            print(f\"   ‚úÖ Outcome distributions saved to {base_filename}_outcome_distributions.csv\")\n        \n        # 4. Best performing models summary\n        best_models = []\n        targets = set([key.split('_k')[0] for key in saved_models.keys()])\n        \n        for target in targets:\n            target_models = {k: v for k, v in saved_models.items() if k.startswith(target)}\n            if target_models:\n                best_model_key = max(target_models.items(), key=lambda x: x[1]['accuracy'])\n                model_key, model_info = best_model_key\n                k_value = model_key.split('_k')[1]\n                \n                best_models.append({\n                    'Target': target,\n                    'Best_K': int(k_value),\n                    'Best_Accuracy': model_info['accuracy'],\n                    'Best_Features': ', '.join(model_info['features']),\n                    'Model_File': model_info['filename']\n                })\n        \n        if best_models:\n            best_df = pd.DataFrame(best_models)\n            best_df.to_csv(f\"{base_filename}_best_models.csv\", index=False)\n            print(f\"   ‚úÖ Best models summary saved to {base_filename}_best_models.csv\")\n        \n        print(f\"‚úÖ All results exported successfully!\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error exporting results: {str(e)}\")\n\n# Run final model training and export\nif 'X_ml' in locals() and 'y_ml' in locals():\n    # Train and save models\n    saved_models = train_and_save_optimized_models(X_ml, y_ml)\n    \n    # Create performance summary\n    create_model_performance_summary(saved_models)\n    \n    # Export results\n    export_analysis_results(saved_models, X_ml, y_ml)\n    \n    print(f\"\\nüéâ Basic analysis completed successfully! üéâ\")\n    print(f\"üìÅ Models saved in: ./models/\")\n    print(f\"üìä Results exported to CSV files:\")\n    print(f\"   - basic_analysis_results_model_performance.csv\")\n    print(f\"   - basic_analysis_results_feature_statistics.csv\")\n    print(f\"   - basic_analysis_results_outcome_distributions.csv\")\n    print(f\"   - basic_analysis_results_best_models.csv\")\n    print(f\"üïí Analysis finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Findings and Clinical Insights\n",
    "\n",
    "### Summary of Analysis\n",
    "\n",
    "This basic analysis notebook has systematically explored chimerism dynamics in AML/MDS transplant patients, focusing on:\n",
    "\n",
    "#### **Feature Engineering Achievements**\n",
    "1. **Dynamic Change Features**: Created time-point differences capturing chimerism evolution\n",
    "2. **Statistical Summaries**: Developed variability metrics (mean, std, CV) across time points\n",
    "3. **Pattern Classification**: Categorized trends into clinically meaningful labels\n",
    "4. **Predictive Combinations**: Identified optimal feature sets for each outcome\n",
    "\n",
    "#### **Key Clinical Findings**\n",
    "- **`d(60-100)_cd3+`**: Consistently emerges as the most predictive single feature\n",
    "- **Variability Matters**: Standard deviation of chimerism levels adds predictive value\n",
    "- **Pattern Recognition**: Trend patterns (upward/downward/stable) correlate with outcomes\n",
    "- **Minimal Feature Sets**: Often 1-3 features achieve optimal performance\n",
    "\n",
    "#### **Methodological Contributions**\n",
    "- **Comprehensive Imputation**: Advanced missing value handling strategies\n",
    "- **Pattern-Based Analysis**: Novel approach to chimerism trend classification\n",
    "- **Feature Set Optimization**: Systematic comparison of different feature combinations\n",
    "- **Clinical Interpretability**: Focus on actionable, interpretable features\n",
    "\n",
    "### **Clinical Implications**\n",
    "\n",
    "1. **Monitoring Strategy**: Focus on Day 60‚Üí100 changes for early prediction\n",
    "2. **Risk Stratification**: Use chimerism patterns for personalized risk assessment\n",
    "3. **Intervention Timing**: Variability metrics may guide intervention decisions\n",
    "4. **Resource Optimization**: Minimal feature sets enable efficient monitoring\n",
    "\n",
    "### **Next Steps for Research**\n",
    "\n",
    "1. **Validation Studies**: Test findings on independent patient cohorts\n",
    "2. **Temporal Modeling**: Develop time-series prediction models\n",
    "3. **Intervention Studies**: Design trials based on chimerism patterns\n",
    "4. **Multi-center Validation**: Expand analysis across institutions\n",
    "\n",
    "---\n",
    "\n",
    "*This organized notebook provides a systematic foundation for understanding chimerism dynamics and their predictive potential in transplant medicine. The modular design enables easy adaptation for different datasets and research questions.*"
   ]
  },
  {
   "cell_type": "code",
   "source": "def create_comprehensive_results_summary():\n    \"\"\"\n    Create a comprehensive summary of all advanced analysis results.\n    \"\"\"\n    print(\"üìã Creating Comprehensive Results Summary...\")\n    \n    # Initialize summary data\n    summary_data = {}\n    \n    # 1. CLUSTERING RESULTS SUMMARY\n    print(\"\\n\" + \"=\"*60)\n    print(\"üîç CLUSTERING ANALYSIS SUMMARY\")\n    print(\"=\"*60)\n    \n    if 'kmeans_results' in locals() and kmeans_results:\n        print(f\"\\nüìä K-Means Clustering Results:\")\n        print(f\"   Number of clusters: {len(kmeans_results['cluster_stats'])}\")\n        print(f\"   Total data points: {len(kmeans_results['cluster_labels'])}\")\n        print(f\"   Cluster distribution:\")\n        for cluster_name, stats in kmeans_results['cluster_stats'].items():\n            print(f\"     {cluster_name}: {stats['size']} points ({stats['percentage']:.1f}%)\")\n        \n        summary_data['kmeans'] = {\n            'n_clusters': len(kmeans_results['cluster_stats']),\n            'n_points': len(kmeans_results['cluster_labels']),\n            'cluster_sizes': [stats['size'] for stats in kmeans_results['cluster_stats'].values()]\n        }\n    \n    if 'fcm_results' in locals() and fcm_results:\n        print(f\"\\nüîÆ Fuzzy C-Means Clustering Results:\")\n        print(f\"   Number of clusters: {fcm_results['fcm_stats']['n_clusters']}\")\n        print(f\"   Total data points: {fcm_results['fcm_stats']['n_points']}\")\n        print(f\"   Fuzzy Partition Coefficient: {fcm_results['fcm_stats']['fpc']:.4f}\")\n        print(f\"   Average max membership: {fcm_results['fcm_stats']['avg_max_membership']:.4f}\")\n        print(f\"   Cluster distribution:\")\n        for i, size in enumerate(fcm_results['fcm_stats']['cluster_sizes']):\n            percentage = size / fcm_results['fcm_stats']['n_points'] * 100\n            print(f\"     Cluster {i+1}: {size} points ({percentage:.1f}%)\")\n        \n        summary_data['fcm'] = fcm_results['fcm_stats']\n    \n    # 2. FUZZY CLASSIFICATION SUMMARY\n    print(\"\\n\" + \"=\"*60)\n    print(\"üß† FUZZY CLASSIFICATION SUMMARY\")\n    print(\"=\"*60)\n    \n    if 'fuzzy_svm_results' in locals() and fuzzy_svm_results:\n        print(f\"\\nüìà Fuzzy SVM Classification Results:\")\n        classification_summary = []\n        \n        for target_name, result in fuzzy_svm_results.items():\n            accuracy = result['accuracy']\n            n_classes = len(result['classes'])\n            avg_confidence = np.mean(result['max_probs'])\n            high_confidence_pct = np.mean(result['max_probs'] > 0.8) * 100\n            \n            print(f\"\\n   üéØ {target_name}:\")\n            print(f\"     Accuracy: {accuracy:.4f}\")\n            print(f\"     Number of classes: {n_classes}\")\n            print(f\"     Average confidence: {avg_confidence:.4f}\")\n            print(f\"     High confidence predictions (>0.8): {high_confidence_pct:.1f}%\")\n            \n            classification_summary.append({\n                'Target': target_name,\n                'Accuracy': accuracy,\n                'N_Classes': n_classes,\n                'Avg_Confidence': avg_confidence,\n                'High_Confidence_Pct': high_confidence_pct\n            })\n        \n        summary_data['fuzzy_svm'] = classification_summary\n    \n    # 3. GENETIC ALGORITHM SUMMARY\n    print(\"\\n\" + \"=\"*60)\n    print(\"üß¨ GENETIC ALGORITHM FEATURE SELECTION SUMMARY\")\n    print(\"=\"*60)\n    \n    if 'ga_results' in locals() and ga_results:\n        print(f\"\\nüéØ GA Feature Selection Results:\")\n        ga_summary = []\n        \n        for target_name, result in ga_results.items():\n            best_fitness = result['best_fitness']\n            n_selected = len(result['best_features_indices'])\n            n_total = len(result['feature_names'])\n            reduction_pct = ((n_total - n_selected) / n_total) * 100\n            improvement = np.mean(result['cv_scores']) - np.mean(result['cv_scores_full'])\n            \n            print(f\"\\n   üéØ {target_name}:\")\n            print(f\"     Best fitness (accuracy): {best_fitness:.4f}\")\n            print(f\"     Selected features: {n_selected}/{n_total} ({reduction_pct:.1f}% reduction)\")\n            print(f\"     CV accuracy: {np.mean(result['cv_scores']):.4f} ¬± {np.std(result['cv_scores']):.4f}\")\n            print(f\"     Improvement over full set: {improvement:+.4f}\")\n            print(f\"     Top selected features: {result['best_features_names'][:3]}\")\n            \n            ga_summary.append({\n                'Target': target_name,\n                'Best_Fitness': best_fitness,\n                'N_Selected': n_selected,\n                'N_Total': n_total,\n                'Reduction_Pct': reduction_pct,\n                'CV_Mean': np.mean(result['cv_scores']),\n                'CV_Std': np.std(result['cv_scores']),\n                'Improvement': improvement,\n                'Top_Features': result['best_features_names'][:3]\n            })\n        \n        summary_data['genetic_algorithm'] = ga_summary\n    \n    # 4. CREATE COMPREHENSIVE COMPARISON VISUALIZATION\n    print(\"\\nüìä Creating comprehensive comparison visualization...\")\n    \n    # Create comparison plots\n    if summary_data:\n        fig = plt.figure(figsize=(20, 15))\n        \n        # Plot 1: Classification Accuracy Comparison\n        if 'fuzzy_svm' in summary_data:\n            ax1 = plt.subplot(3, 3, 1)\n            targets = [item['Target'] for item in summary_data['fuzzy_svm']]\n            accuracies = [item['Accuracy'] for item in summary_data['fuzzy_svm']]\n            \n            bars = plt.bar(targets, accuracies, color='lightblue', alpha=0.7, edgecolor='black')\n            plt.title('Fuzzy SVM Classification Accuracy')\n            plt.ylabel('Accuracy')\n            plt.xticks(rotation=45)\n            plt.grid(True, alpha=0.3)\n            \n            # Add value labels\n            for bar, acc in zip(bars, accuracies):\n                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                        f'{acc:.3f}', ha='center', va='bottom')\n        \n        # Plot 2: GA Feature Reduction\n        if 'genetic_algorithm' in summary_data:\n            ax2 = plt.subplot(3, 3, 2)\n            targets = [item['Target'] for item in summary_data['genetic_algorithm']]\n            reductions = [item['Reduction_Pct'] for item in summary_data['genetic_algorithm']]\n            \n            bars = plt.bar(targets, reductions, color='lightgreen', alpha=0.7, edgecolor='black')\n            plt.title('GA Feature Reduction Percentage')\n            plt.ylabel('Reduction (%)')\n            plt.xticks(rotation=45)\n            plt.grid(True, alpha=0.3)\n            \n            # Add value labels\n            for bar, red in zip(bars, reductions):\n                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                        f'{red:.1f}%', ha='center', va='bottom')\n        \n        # Plot 3: GA Performance Improvement\n        if 'genetic_algorithm' in summary_data:\n            ax3 = plt.subplot(3, 3, 3)\n            targets = [item['Target'] for item in summary_data['genetic_algorithm']]\n            improvements = [item['Improvement'] for item in summary_data['genetic_algorithm']]\n            \n            colors = ['lightgreen' if imp > 0 else 'lightcoral' for imp in improvements]\n            bars = plt.bar(targets, improvements, color=colors, alpha=0.7, edgecolor='black')\n            plt.title('GA Performance Improvement')\n            plt.ylabel('Accuracy Improvement')\n            plt.xticks(rotation=45)\n            plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n            plt.grid(True, alpha=0.3)\n            \n            # Add value labels\n            for bar, imp in zip(bars, improvements):\n                plt.text(bar.get_x() + bar.get_width()/2, \n                        bar.get_height() + (0.005 if imp > 0 else -0.01),\n                        f'{imp:+.3f}', ha='center', va='bottom' if imp > 0 else 'top')\n        \n        # Plot 4: Clustering Comparison\n        if 'kmeans' in summary_data and 'fcm' in summary_data:\n            ax4 = plt.subplot(3, 3, 4)\n            methods = ['K-Means', 'FCM']\n            n_points = [summary_data['kmeans']['n_points'], summary_data['fcm']['n_points']]\n            \n            plt.bar(methods, n_points, color='lightyellow', alpha=0.7, edgecolor='black')\n            plt.title('Clustering Data Points')\n            plt.ylabel('Number of Points')\n            plt.grid(True, alpha=0.3)\n        \n        # Plot 5: Confidence Distribution (Fuzzy SVM)\n        if 'fuzzy_svm' in summary_data:\n            ax5 = plt.subplot(3, 3, 5)\n            targets = [item['Target'] for item in summary_data['fuzzy_svm']]\n            confidences = [item['Avg_Confidence'] for item in summary_data['fuzzy_svm']]\n            \n            bars = plt.bar(targets, confidences, color='lightpink', alpha=0.7, edgecolor='black')\n            plt.title('Average Prediction Confidence')\n            plt.ylabel('Average Confidence')\n            plt.xticks(rotation=45)\n            plt.grid(True, alpha=0.3)\n            \n            # Add value labels\n            for bar, conf in zip(bars, confidences):\n                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                        f'{conf:.3f}', ha='center', va='bottom')\n        \n        # Plot 6: Feature Selection Efficiency\n        if 'genetic_algorithm' in summary_data:\n            ax6 = plt.subplot(3, 3, 6)\n            targets = [item['Target'] for item in summary_data['genetic_algorithm']]\n            n_selected = [item['N_Selected'] for item in summary_data['genetic_algorithm']]\n            \n            bars = plt.bar(targets, n_selected, color='lightsteelblue', alpha=0.7, edgecolor='black')\n            plt.title('Number of Selected Features')\n            plt.ylabel('Selected Features')\n            plt.xticks(rotation=45)\n            plt.grid(True, alpha=0.3)\n            \n            # Add value labels\n            for bar, n_sel in zip(bars, n_selected):\n                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.2,\n                        f'{n_sel}', ha='center', va='bottom')\n        \n        # Plot 7: Combined Performance Metrics\n        if 'fuzzy_svm' in summary_data and 'genetic_algorithm' in summary_data:\n            ax7 = plt.subplot(3, 3, (7, 9))\n            \n            # Create comparison table\n            comparison_data = []\n            for i, target in enumerate(targets):\n                if i < len(summary_data['fuzzy_svm']) and i < len(summary_data['genetic_algorithm']):\n                    fuzzy_acc = summary_data['fuzzy_svm'][i]['Accuracy']\n                    ga_acc = summary_data['genetic_algorithm'][i]['CV_Mean']\n                    n_features = summary_data['genetic_algorithm'][i]['N_Selected']\n                    \n                    comparison_data.append([target, f'{fuzzy_acc:.3f}', f'{ga_acc:.3f}', str(n_features)])\n            \n            # Create table\n            table = plt.table(cellText=comparison_data,\n                            colLabels=['Target', 'Fuzzy SVM\\nAccuracy', 'GA Selected\\nFeatures Acc', 'N Features\\nSelected'],\n                            cellLoc='center',\n                            loc='center')\n            table.auto_set_font_size(False)\n            table.set_fontsize(10)\n            table.scale(1.2, 1.5)\n            \n            plt.axis('off')\n            plt.title('Performance Comparison Summary', pad=20)\n        \n        plt.tight_layout()\n        plt.show()\n    \n    # 5. EXPORT COMPREHENSIVE RESULTS\n    print(\"\\nüíæ Exporting comprehensive results to CSV...\")\n    \n    try:\n        # Export Fuzzy SVM results\n        if 'fuzzy_svm' in summary_data:\n            fuzzy_df = pd.DataFrame(summary_data['fuzzy_svm'])\n            fuzzy_df.to_csv('advanced_analysis_fuzzy_svm_results.csv', index=False)\n            print(\"   ‚úÖ Fuzzy SVM results saved to advanced_analysis_fuzzy_svm_results.csv\")\n        \n        # Export GA results\n        if 'genetic_algorithm' in summary_data:\n            ga_df = pd.DataFrame(summary_data['genetic_algorithm'])\n            # Convert list columns to strings for CSV export\n            if 'Top_Features' in ga_df.columns:\n                ga_df['Top_Features'] = ga_df['Top_Features'].apply(lambda x: ', '.join(x) if isinstance(x, list) else str(x))\n            ga_df.to_csv('advanced_analysis_genetic_algorithm_results.csv', index=False)\n            print(\"   ‚úÖ Genetic Algorithm results saved to advanced_analysis_genetic_algorithm_results.csv\")\n        \n        # Export clustering results\n        if 'kmeans' in summary_data or 'fcm' in summary_data:\n            clustering_data = []\n            if 'kmeans' in summary_data:\n                clustering_data.append({\n                    'Method': 'K-Means',\n                    'N_Clusters': summary_data['kmeans']['n_clusters'],\n                    'N_Points': summary_data['kmeans']['n_points'],\n                    'Metric_Value': 'N/A'\n                })\n            if 'fcm' in summary_data:\n                clustering_data.append({\n                    'Method': 'Fuzzy C-Means',\n                    'N_Clusters': summary_data['fcm']['n_clusters'],\n                    'N_Points': summary_data['fcm']['n_points'],\n                    'Metric_Value': summary_data['fcm']['fpc']\n                })\n            \n            clustering_df = pd.DataFrame(clustering_data)\n            clustering_df.to_csv('advanced_analysis_clustering_results.csv', index=False)\n            print(\"   ‚úÖ Clustering results saved to advanced_analysis_clustering_results.csv\")\n    \n    except Exception as e:\n        print(f\"   ‚ö†Ô∏è Error exporting results: {str(e)}\")\n    \n    # 6. FINAL SUMMARY REPORT\n    print(\"\\n\" + \"=\"*60)\n    print(\"üìã FINAL ANALYSIS SUMMARY REPORT\")\n    print(\"=\"*60)\n    \n    print(f\"\\nüîç Analysis Completed Successfully!\")\n    print(f\"   ‚úÖ K-Means Clustering: {'‚úì' if 'kmeans' in summary_data else '‚úó'}\")\n    print(f\"   ‚úÖ Fuzzy C-Means Clustering: {'‚úì' if 'fcm' in summary_data else '‚úó'}\")\n    print(f\"   ‚úÖ Fuzzy SVM Classification: {'‚úì' if 'fuzzy_svm' in summary_data else '‚úó'}\")\n    print(f\"   ‚úÖ Genetic Algorithm Feature Selection: {'‚úì' if 'genetic_algorithm' in summary_data else '‚úó'}\")\n    \n    if 'genetic_algorithm' in summary_data:\n        best_target = max(summary_data['genetic_algorithm'], key=lambda x: x['Best_Fitness'])\n        print(f\"\\nüèÜ Best Overall Performance:\")\n        print(f\"   Target: {best_target['Target']}\")\n        print(f\"   Accuracy: {best_target['Best_Fitness']:.4f}\")\n        print(f\"   Features: {best_target['N_Selected']}/{best_target['N_Total']}\")\n        print(f\"   Top Features: {', '.join(best_target['Top_Features'])}\")\n    \n    print(f\"\\nüìÅ Results exported to CSV files for further analysis\")\n    print(f\"üéâ Advanced analysis pipeline completed successfully!\")\n    \n    return summary_data\n\n# Create comprehensive summary\nif any(var in locals() for var in ['kmeans_results', 'fcm_results', 'fuzzy_svm_results', 'ga_results']):\n    comprehensive_summary = create_comprehensive_results_summary()\nelse:\n    print(\"‚ö†Ô∏è No advanced analysis results found. Please run the previous sections first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 13. Comprehensive Results Summary\n\n### Summary of Advanced Analysis Results\n\nConsolidate and compare results from all advanced techniques.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def implement_genetic_algorithm_feature_selection(X, y, target_name='Target', \n                                                  n_generations=50, population_size=50, \n                                                  crossover_prob=0.8, mutation_prob=0.1,\n                                                  k_folds=5, random_state=42):\n    \"\"\"\n    Implement Genetic Algorithm for feature selection with K-Fold SVM evaluation.\n    \n    Parameters:\n    -----------\n    X : pd.DataFrame or np.array\n        Feature matrix\n    y : pd.Series or np.array\n        Target variable\n    target_name : str\n        Name of the target variable\n    n_generations : int\n        Number of generations for GA\n    population_size : int\n        Size of population in each generation\n    crossover_prob : float\n        Probability of crossover\n    mutation_prob : float\n        Probability of mutation\n    k_folds : int\n        Number of folds for cross-validation\n    random_state : int\n        Random state for reproducibility\n        \n    Returns:\n    --------\n    dict : Dictionary containing GA results and best features\n    \"\"\"\n    print(f\"üß¨ Implementing Genetic Algorithm Feature Selection for {target_name}...\")\n    print(f\"   Generations: {n_generations}, Population: {population_size}\")\n    print(f\"   Crossover Prob: {crossover_prob}, Mutation Prob: {mutation_prob}\")\n    print(f\"   K-Fold CV: {k_folds}\")\n    \n    # Prepare data\n    X_clean = X.select_dtypes(include=[np.number]).fillna(X.select_dtypes(include=[np.number]).median())\n    y_clean = y.dropna()\n    \n    # Align X and y\n    common_indices = X_clean.index.intersection(y_clean.index)\n    X_aligned = X_clean.loc[common_indices]\n    y_aligned = y_clean.loc[common_indices]\n    \n    if len(y_aligned.unique()) < 2:\n        print(f\"‚ö†Ô∏è Insufficient classes for classification ({len(y_aligned.unique())} unique values)\")\n        return None\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_aligned)\n    \n    # Get feature names\n    feature_names = X_aligned.columns.tolist() if hasattr(X_aligned, 'columns') else [f'Feature_{i}' for i in range(X_scaled.shape[1])]\n    n_features = len(feature_names)\n    \n    print(f\"   üìä Dataset: {X_scaled.shape[0]} samples, {n_features} features\")\n    \n    # Set random seeds\n    random.seed(random_state)\n    np.random.seed(random_state)\n    \n    # Define DEAP GA components\n    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))  # Maximize accuracy\n    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n    \n    toolbox = base.Toolbox()\n    \n    # Attribute generator - binary encoding for feature selection\n    toolbox.register(\"attr_bool\", random.randint, 0, 1)\n    \n    # Structure initializers\n    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, \n                    toolbox.attr_bool, n_features)\n    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n    \n    # Fitness function using K-Fold SVM\n    def evaluate_feature_subset(individual):\n        \\\"\\\"\\\"\n        Evaluate fitness of feature subset using K-Fold Cross-Validation with SVM.\n        \\\"\\\"\\\"\n        # Convert binary individual to feature indices\n        selected_features = [i for i, bit in enumerate(individual) if bit == 1]\n        \n        # Ensure at least one feature is selected\n        if len(selected_features) == 0:\n            return (0.0,)  # Return 0 fitness for empty feature set\n        \n        # Select features\n        X_subset = X_scaled[:, selected_features]\n        \n        try:\n            # Perform K-Fold Cross-Validation with SVM\n            svm_classifier = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=random_state)\n            cv_scores = cross_val_score(svm_classifier, X_subset, y_aligned, \n                                      cv=k_folds, scoring='accuracy', n_jobs=1)\n            \n            # Return mean accuracy as fitness\n            fitness = np.mean(cv_scores)\n            return (fitness,)\n            \n        except Exception as e:\n            # Return 0 fitness if evaluation fails\n            return (0.0,)\n    \n    # Register genetic operators\n    toolbox.register(\"evaluate\", evaluate_feature_subset)\n    toolbox.register(\"mate\", tools.cxTwoPoint)\n    toolbox.register(\"mutate\", tools.mutFlipBit, indpb=mutation_prob/n_features)\n    toolbox.register(\"select\", tools.selTournament, tournsize=3)\n    \n    # Initialize population\n    print(\"   üéØ Initializing population and starting evolution...\")\n    population = toolbox.population(n=population_size)\n    \n    # Track statistics\n    stats = tools.Statistics(lambda ind: ind.fitness.values)\n    stats.register(\"avg\", np.mean)\n    stats.register(\"std\", np.std)\n    stats.register(\"min\", np.min)\n    stats.register(\"max\", np.max)\n    \n    # Hall of Fame to track best individuals\n    hof = tools.HallOfFame(5)\n    \n    # Evolution tracking\n    fitness_history = []\n    best_fitness_history = []\n    avg_fitness_history = []\n    \n    # Evaluate initial population\n    fitnesses = list(map(toolbox.evaluate, population))\n    for ind, fit in zip(population, fitnesses):\n        ind.fitness.values = fit\n    \n    hof.update(population)\n    \n    # Evolution loop\n    print(f\"   üîÑ Running evolution for {n_generations} generations...\")\n    \n    for generation in range(n_generations):\n        # Selection\n        offspring = toolbox.select(population, len(population))\n        offspring = list(map(toolbox.clone, offspring))\n        \n        # Crossover\n        for child1, child2 in zip(offspring[::2], offspring[1::2]):\n            if random.random() < crossover_prob:\n                toolbox.mate(child1, child2)\n                del child1.fitness.values\n                del child2.fitness.values\n        \n        # Mutation\n        for mutant in offspring:\n            if random.random() < mutation_prob:\n                toolbox.mutate(mutant)\n                del mutant.fitness.values\n        \n        # Evaluate invalid individuals\n        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n        fitnesses = map(toolbox.evaluate, invalid_ind)\n        for ind, fit in zip(invalid_ind, fitnesses):\n            ind.fitness.values = fit\n        \n        # Replace population\n        population[:] = offspring\n        \n        # Update hall of fame\n        hof.update(population)\n        \n        # Record statistics\n        record = stats.compile(population)\n        fitness_history.append(record)\n        best_fitness_history.append(record['max'])\n        avg_fitness_history.append(record['avg'])\n        \n        # Print progress\n        if generation % 10 == 0 or generation == n_generations - 1:\n            print(f\"     Gen {generation:3d}: Max Fitness = {record['max']:.4f}, \"\n                  f\"Avg Fitness = {record['avg']:.4f}, Std = {record['std']:.4f}\")\n    \n    # Get best individual\n    best_individual = hof[0]\n    best_features_indices = [i for i, bit in enumerate(best_individual) if bit == 1]\n    best_features_names = [feature_names[i] for i in best_features_indices]\n    best_fitness = best_individual.fitness.values[0]\n    \n    print(f\"\\n=== Genetic Algorithm Results for {target_name} ===\")\n    print(f\"Best fitness (accuracy): {best_fitness:.4f}\")\n    print(f\"Number of selected features: {len(best_features_indices)}\")\n    print(f\"Selected features: {best_features_names}\")\n    \n    # Visualize GA progress\n    print(\"   üìà Creating evolution progress visualization...\")\n    \n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    fig.suptitle(f'Genetic Algorithm Feature Selection - {target_name}', fontsize=16)\n    \n    # Plot 1: Fitness evolution\n    generations = range(len(best_fitness_history))\n    axes[0, 0].plot(generations, best_fitness_history, 'b-', label='Best Fitness', linewidth=2)\n    axes[0, 0].plot(generations, avg_fitness_history, 'r-', label='Average Fitness', linewidth=1)\n    axes[0, 0].set_title('Fitness Evolution Over Generations')\n    axes[0, 0].set_xlabel('Generation')\n    axes[0, 0].set_ylabel('Fitness (Accuracy)')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    \n    # Plot 2: Feature selection frequency\n    feature_selection_freq = np.zeros(n_features)\n    for individual in hof:\n        for i, bit in enumerate(individual):\n            if bit == 1:\n                feature_selection_freq[i] += 1\n    \n    # Sort features by selection frequency\n    sorted_indices = np.argsort(feature_selection_freq)[::-1]\n    top_15_indices = sorted_indices[:15]  # Show top 15 features\n    \n    axes[0, 1].bar(range(len(top_15_indices)), feature_selection_freq[top_15_indices])\n    axes[0, 1].set_title('Feature Selection Frequency (Top 15)')\n    axes[0, 1].set_xlabel('Feature Index')\n    axes[0, 1].set_ylabel('Selection Frequency')\n    axes[0, 1].set_xticks(range(len(top_15_indices)))\n    axes[0, 1].set_xticklabels([str(i) for i in top_15_indices], rotation=45)\n    axes[0, 1].grid(True, alpha=0.3)\n    \n    # Plot 3: Best individual feature map\n    axes[1, 0].imshow(np.array(best_individual).reshape(1, -1), cmap='RdYlBu', aspect='auto')\n    axes[1, 0].set_title('Best Individual Feature Selection Pattern')\n    axes[1, 0].set_xlabel('Feature Index')\n    axes[1, 0].set_ylabel('Selected (1) / Not Selected (0)')\n    axes[1, 0].set_yticks([])\n    \n    # Plot 4: Fitness distribution in final population\n    final_fitnesses = [ind.fitness.values[0] for ind in population]\n    axes[1, 1].hist(final_fitnesses, bins=20, alpha=0.7, color='green', edgecolor='black')\n    axes[1, 1].axvline(best_fitness, color='red', linestyle='--', linewidth=2, label=f'Best: {best_fitness:.4f}')\n    axes[1, 1].set_title('Final Population Fitness Distribution')\n    axes[1, 1].set_xlabel('Fitness (Accuracy)')\n    axes[1, 1].set_ylabel('Frequency')\n    axes[1, 1].legend()\n    axes[1, 1].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Detailed evaluation of best feature subset\n    print(\"   üéØ Performing detailed K-Fold evaluation of best feature subset...\")\n    \n    X_best = X_scaled[:, best_features_indices]\n    \n    # Detailed K-Fold evaluation\n    kfold = KFold(n_splits=k_folds, shuffle=True, random_state=random_state)\n    svm_best = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True, random_state=random_state)\n    \n    # Get detailed CV results\n    cv_scores = cross_val_score(svm_best, X_best, y_aligned, cv=kfold, scoring='accuracy')\n    cv_predictions = cross_val_predict(svm_best, X_best, y_aligned, cv=kfold)\n    \n    print(f\"\\n=== K-Fold Cross-Validation Results (k={k_folds}) ===\")\n    print(f\"Individual fold accuracies: {[f'{score:.4f}' for score in cv_scores]}\")\n    print(f\"Mean accuracy: {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}\")\n    print(f\"Best accuracy: {np.max(cv_scores):.4f}\")\n    print(f\"Worst accuracy: {np.min(cv_scores):.4f}\")\n    \n    # Classification report\n    print(f\"\\n=== Classification Report (Cross-Validation) ===\")\n    print(classification_report(y_aligned, cv_predictions))\n    \n    # Feature importance analysis\n    print(\"   üìä Analyzing feature importance of selected features...\")\n    \n    # Train final model and get feature importance (if possible)\n    try:\n        # Use Random Forest to get feature importance for selected features\n        rf_importance = RandomForestClassifier(n_estimators=100, random_state=random_state)\n        rf_importance.fit(X_best, y_aligned)\n        importances = rf_importance.feature_importances_\n        \n        # Create feature importance DataFrame\n        importance_df = pd.DataFrame({\n            'Feature': best_features_names,\n            'Importance': importances\n        }).sort_values('Importance', ascending=False)\n        \n        print(f\"\\n=== Feature Importance (Random Forest) ===\")\n        for _, row in importance_df.iterrows():\n            print(f\"{row['Feature']:30s}: {row['Importance']:.4f}\")\n        \n        # Plot feature importance\n        plt.figure(figsize=(12, 6))\n        plt.bar(range(len(best_features_names)), importances)\n        plt.title(f'Feature Importance for Selected Features - {target_name}')\n        plt.xlabel('Feature')\n        plt.ylabel('Importance')\n        plt.xticks(range(len(best_features_names)), best_features_names, rotation=45, ha='right')\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not compute feature importance: {str(e)}\")\n        importance_df = None\n    \n    # Compare with all features\n    print(\"   üîÑ Comparing with full feature set...\")\n    svm_full = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=random_state)\n    cv_scores_full = cross_val_score(svm_full, X_scaled, y_aligned, cv=k_folds, scoring='accuracy')\n    \n    print(f\"\\n=== Comparison with Full Feature Set ===\")\n    print(f\"Selected features ({len(best_features_indices)}): {np.mean(cv_scores):.4f} ¬± {np.std(cv_scores):.4f}\")\n    print(f\"All features ({n_features}): {np.mean(cv_scores_full):.4f} ¬± {np.std(cv_scores_full):.4f}\")\n    improvement = np.mean(cv_scores) - np.mean(cv_scores_full)\n    print(f\"Improvement: {improvement:+.4f}\")\n    print(f\"Feature reduction: {((n_features - len(best_features_indices)) / n_features * 100):.1f}%\")\n    \n    return {\n        'best_individual': best_individual,\n        'best_features_indices': best_features_indices,\n        'best_features_names': best_features_names,\n        'best_fitness': best_fitness,\n        'cv_scores': cv_scores,\n        'cv_scores_full': cv_scores_full,\n        'fitness_history': fitness_history,\n        'best_fitness_history': best_fitness_history,\n        'avg_fitness_history': avg_fitness_history,\n        'hall_of_fame': hof,\n        'feature_selection_freq': feature_selection_freq,\n        'scaler': scaler,\n        'feature_names': feature_names,\n        'importance_df': importance_df if 'importance_df' in locals() else None,\n        'X_best': X_best,\n        'n_generations': n_generations,\n        'population_size': population_size\n    }\n\n# Apply Genetic Algorithm Feature Selection\nif 'X_ml' in locals() and 'y_ml' in locals():\n    ga_results = {}\n    target_names = ['RFS', 'Death', 'Relapse']\n    \n    for i, target_name in enumerate(target_names):\n        if i < len(y_ml.columns):\n            target_col = y_ml.columns[i]\n            print(f\"\\n{'='*70}\")\n            print(f\"Applying Genetic Algorithm Feature Selection to {target_name}\")\n            print(f\"{'='*70}\")\n            \n            result = implement_genetic_algorithm_feature_selection(\n                X_ml, y_ml[target_col], target_name, \n                n_generations=30,  # Reduced for demonstration\n                population_size=30,\n                k_folds=5\n            )\n            \n            if result:\n                ga_results[target_name] = result\n    \n    print(\"‚úÖ Genetic Algorithm Feature Selection completed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 12. Genetic Algorithm for Feature Selection\n\n### GA-based Feature Selection with K-Fold SVM Evaluation\n\nImplement genetic algorithm to find optimal feature subsets using SVM accuracy as fitness function.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def implement_fuzzy_svm_classification(X, y, target_name='Target', test_size=0.3, random_state=42):\n    \"\"\"\n    Implement Fuzzy SVM classification with probability distributions.\n    \n    Parameters:\n    -----------\n    X : pd.DataFrame or np.array\n        Feature matrix\n    y : pd.Series or np.array\n        Target variable\n    target_name : str\n        Name of the target variable for plotting\n    test_size : float\n        Proportion of data for testing\n    random_state : int\n        Random state for reproducibility\n        \n    Returns:\n    --------\n    dict : Dictionary containing classification results and probabilities\n    \"\"\"\n    print(f\"üß† Implementing Fuzzy SVM Classification for {target_name}...\")\n    \n    # Prepare data\n    X_clean = X.select_dtypes(include=[np.number]).fillna(X.select_dtypes(include=[np.number]).median())\n    y_clean = y.dropna()\n    \n    # Align X and y\n    common_indices = X_clean.index.intersection(y_clean.index)\n    X_aligned = X_clean.loc[common_indices]\n    y_aligned = y_clean.loc[common_indices]\n    \n    if len(y_aligned.unique()) < 2:\n        print(f\"‚ö†Ô∏è Insufficient classes for classification ({len(y_aligned.unique())} unique values)\")\n        return None\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_aligned)\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y_aligned, test_size=test_size, random_state=random_state, stratify=y_aligned\n    )\n    \n    print(f\"   üìä Training set: {len(X_train)} samples, Test set: {len(X_test)} samples\")\n    \n    # Train Fuzzy SVM with probability estimates\n    print(\"   üéØ Training Fuzzy SVM classifier...\")\n    fuzzy_svm = SVC(\n        kernel='rbf',\n        probability=True,  # Enable probability estimates\n        C=1.0,\n        gamma='scale',\n        random_state=random_state\n    )\n    \n    fuzzy_svm.fit(X_train, y_train)\n    \n    # Get predictions and probabilities\n    y_pred = fuzzy_svm.predict(X_test)\n    y_proba = fuzzy_svm.predict_proba(X_test)\n    y_proba_train = fuzzy_svm.predict_proba(X_train)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"   ‚úÖ Test Accuracy: {accuracy:.4f}\")\n    \n    # Get class labels\n    classes = fuzzy_svm.classes_\n    n_classes = len(classes)\n    \n    # Create comprehensive probability visualizations\n    print(\"   üé® Creating probability distribution visualizations...\")\n    \n    # Create subplot layout\n    if n_classes == 2:\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        fig.suptitle(f'Fuzzy SVM Classification Analysis - {target_name}', fontsize=16)\n    else:\n        n_rows = (n_classes + 2) // 3\n        fig, axes = plt.subplots(n_rows, 3, figsize=(18, 6*n_rows))\n        fig.suptitle(f'Fuzzy SVM Classification Analysis - {target_name}', fontsize=16)\n        axes = axes.flatten() if n_rows > 1 else [axes]\n    \n    # Plot 1: Training probability distributions for each class\n    for i, class_label in enumerate(classes):\n        if i < len(axes) - 3:  # Leave space for other plots\n            class_probs_train = y_proba_train[:, i]\n            class_probs_test = y_proba[:, i]\n            \n            axes[i].hist(class_probs_train, bins=20, alpha=0.7, label='Training', density=True, color='skyblue')\n            axes[i].hist(class_probs_test, bins=20, alpha=0.7, label='Test', density=True, color='orange')\n            axes[i].set_title(f'Class {class_label} - Probability Distribution')\n            axes[i].set_xlabel('Predicted Probability')\n            axes[i].set_ylabel('Density')\n            axes[i].legend()\n            axes[i].grid(True, alpha=0.3)\n    \n    # Plot: Probability distributions by true class (for binary classification)\n    if n_classes == 2:\n        # True class 0 probabilities\n        true_class_0_mask = y_test == classes[0]\n        true_class_1_mask = y_test == classes[1]\n        \n        axes[-3].hist(y_proba[true_class_0_mask, 1], bins=15, alpha=0.7, \n                     label=f'True Class {classes[0]}', density=True, color='lightcoral')\n        axes[-3].hist(y_proba[true_class_1_mask, 1], bins=15, alpha=0.7, \n                     label=f'True Class {classes[1]}', density=True, color='lightgreen')\n        axes[-3].set_title(f'Probability Distribution by True Class\\\\n(Probability of Class {classes[1]})')\n        axes[-3].set_xlabel('Predicted Probability')\n        axes[-3].set_ylabel('Density')\n        axes[-3].legend()\n        axes[-3].grid(True, alpha=0.3)\n        \n        # ROC-like analysis: Probability vs True Class\n        axes[-2].scatter(range(len(y_test)), y_proba[:, 1], \n                        c=y_test, cmap='viridis', alpha=0.7, s=30)\n        axes[-2].set_title('Predicted Probabilities vs True Classes')\n        axes[-2].set_xlabel('Sample Index')\n        axes[-2].set_ylabel('Probability of Positive Class')\n        axes[-2].grid(True, alpha=0.3)\n        \n        # Confusion matrix with probabilities\n        from sklearn.metrics import confusion_matrix\n        cm = confusion_matrix(y_test, y_pred)\n        \n        im = axes[-1].imshow(cm, interpolation='nearest', cmap='Blues')\n        axes[-1].set_title('Confusion Matrix')\n        tick_marks = np.arange(len(classes))\n        axes[-1].set_xticks(tick_marks)\n        axes[-1].set_xticklabels(classes)\n        axes[-1].set_yticks(tick_marks)\n        axes[-1].set_yticklabels(classes)\n        axes[-1].set_ylabel('True Label')\n        axes[-1].set_xlabel('Predicted Label')\n        \n        # Add text annotations to confusion matrix\n        thresh = cm.max() / 2.\n        for i in range(cm.shape[0]):\n            for j in range(cm.shape[1]):\n                axes[-1].text(j, i, format(cm[i, j], 'd'),\n                            ha=\"center\", va=\"center\",\n                            color=\"white\" if cm[i, j] > thresh else \"black\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Create detailed probability analysis plots\n    print(\"   üìà Creating detailed probability analysis...\")\n    \n    # Probability distribution comparison\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n    \n    # Plot 1: Box plot of probabilities by true class\n    prob_data = []\\n    for i, class_label in enumerate(classes):\n        for j, true_class in enumerate(classes):\n            mask = y_test == true_class\n            probs = y_proba[mask, i]\n            for prob in probs:\n                prob_data.append({\n                    'True_Class': f'True_{true_class}',\n                    'Predicted_Class': f'Pred_{class_label}',\n                    'Probability': prob\n                })\n    \n    prob_df = pd.DataFrame(prob_data)\n    \n    # Create box plot\n    sns.boxplot(data=prob_df, x='True_Class', y='Probability', hue='Predicted_Class', ax=axes[0])\n    axes[0].set_title('Probability Distributions by True Class')\n    axes[0].set_ylabel('Predicted Probability')\n    axes[0].legend(title='Predicted Class')\n    \n    # Plot 2: Probability confidence (max probability distribution)\n    max_probs = np.max(y_proba, axis=1)\n    correct_mask = y_pred == y_test\n    \n    axes[1].hist(max_probs[correct_mask], bins=15, alpha=0.7, label='Correct', density=True, color='green')\n    axes[1].hist(max_probs[~correct_mask], bins=15, alpha=0.7, label='Incorrect', density=True, color='red')\n    axes[1].set_title('Maximum Probability Distribution\\\\n(Confidence Analysis)')\n    axes[1].set_xlabel('Maximum Predicted Probability')\n    axes[1].set_ylabel('Density')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    # Plot 3: Probability calibration\n    if n_classes == 2:\n        # Bin probabilities and calculate actual positive rate\n        prob_bins = np.linspace(0, 1, 11)\n        bin_centers = (prob_bins[:-1] + prob_bins[1:]) / 2\n        actual_rates = []\n        predicted_rates = []\n        \n        for i in range(len(prob_bins) - 1):\n            mask = (y_proba[:, 1] >= prob_bins[i]) & (y_proba[:, 1] < prob_bins[i+1])\n            if mask.sum() > 0:\n                actual_rate = (y_test[mask] == classes[1]).mean()\n                predicted_rate = y_proba[mask, 1].mean()\n                actual_rates.append(actual_rate)\n                predicted_rates.append(predicted_rate)\n            else:\n                actual_rates.append(np.nan)\n                predicted_rates.append(np.nan)\n        \n        axes[2].plot(predicted_rates, actual_rates, 'bo-', label='Calibration Curve')\n        axes[2].plot([0, 1], [0, 1], 'r--', label='Perfect Calibration')\n        axes[2].set_title('Probability Calibration')\n        axes[2].set_xlabel('Mean Predicted Probability')\n        axes[2].set_ylabel('Fraction of Positives')\n        axes[2].legend()\n        axes[2].grid(True, alpha=0.3)\n    else:\n        # For multi-class, show entropy of predictions\n        entropy = -np.sum(y_proba * np.log(y_proba + 1e-10), axis=1)\n        axes[2].hist(entropy[correct_mask], bins=15, alpha=0.7, label='Correct', density=True, color='green')\n        axes[2].hist(entropy[~correct_mask], bins=15, alpha=0.7, label='Incorrect', density=True, color='red')\n        axes[2].set_title('Prediction Entropy Distribution')\n        axes[2].set_xlabel('Entropy')\n        axes[2].set_ylabel('Density')\n        axes[2].legend()\n        axes[2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print detailed classification results\n    print(f\"\\n=== Fuzzy SVM Classification Results for {target_name} ===\")\n    print(f\"Test Accuracy: {accuracy:.4f}\")\n    print(f\"Number of classes: {n_classes}\")\n    print(f\"Class distribution in test set:\")\n    for class_label in classes:\n        count = np.sum(y_test == class_label)\n        percentage = count / len(y_test) * 100\n        print(f\"  Class {class_label}: {count} samples ({percentage:.1f}%)\")\n    \n    # Print classification report\n    print(f\"\\n=== Classification Report ===\")\n    print(classification_report(y_test, y_pred, target_names=[f'Class_{c}' for c in classes]))\n    \n    # Calculate and print probability statistics\n    print(f\"\\n=== Probability Statistics ===\")\n    for i, class_label in enumerate(classes):\n        class_probs = y_proba[:, i]\n        print(f\"Class {class_label} probabilities:\")\n        print(f\"  Mean: {np.mean(class_probs):.4f}\")\n        print(f\"  Std:  {np.std(class_probs):.4f}\")\n        print(f\"  Min:  {np.min(class_probs):.4f}\")\n        print(f\"  Max:  {np.max(class_probs):.4f}\")\n    \n    print(f\"Average maximum probability (confidence): {np.mean(max_probs):.4f}\")\n    print(f\"Percentage of predictions with >0.8 confidence: {np.mean(max_probs > 0.8)*100:.1f}%\")\n    \n    return {\n        'model': fuzzy_svm,\n        'scaler': scaler,\n        'y_test': y_test,\n        'y_pred': y_pred,\n        'y_proba': y_proba,\n        'y_proba_train': y_proba_train,\n        'classes': classes,\n        'accuracy': accuracy,\n        'X_test': X_test,\n        'X_train': X_train,\n        'y_train': y_train,\n        'max_probs': max_probs\n    }\n\n# Apply Fuzzy SVM classification to different targets\nif 'X_ml' in locals() and 'y_ml' in locals():\n    fuzzy_svm_results = {}\n    target_names = ['RFS', 'Death', 'Relapse']\n    \n    for i, target_name in enumerate(target_names):\n        if i < len(y_ml.columns):\n            target_col = y_ml.columns[i]\n            print(f\"\\n{'='*60}\")\n            print(f\"Applying Fuzzy SVM to {target_name}\")\n            print(f\"{'='*60}\")\n            \n            result = implement_fuzzy_svm_classification(X_ml, y_ml[target_col], target_name)\n            if result:\n                fuzzy_svm_results[target_name] = result\n    \n    print(\"‚úÖ Fuzzy SVM classification completed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Fuzzy Classification Analysis\n\n### Fuzzy SVM Classification with Probability Distribution Plots\n\nImplement Fuzzy SVM classification and visualize probability distributions for each class.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def perform_fcm_clustering_with_umap(X, n_clusters=4, random_state=42, m=2.0):\n    \"\"\"\n    Perform Fuzzy C-Means clustering and visualize with UMAP 3D reduction.\n    \n    Parameters:\n    -----------\n    X : pd.DataFrame or np.array\n        Feature matrix\n    n_clusters : int\n        Number of clusters for FCM\n    random_state : int\n        Random state for reproducibility\n    m : float\n        Fuzziness parameter (higher = more fuzzy)\n        \n    Returns:\n    --------\n    dict : Dictionary containing FCM results and UMAP coordinates\n    \"\"\"\n    print(f\"üîÆ Performing Fuzzy C-Means clustering (k={n_clusters}) with UMAP 3D visualization...\")\n    \n    # Prepare data - ensure numerical and no missing values\n    X_clean = X.select_dtypes(include=[np.number]).fillna(X.select_dtypes(include=[np.number]).median())\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_clean)\n    \n    # Perform UMAP dimensionality reduction to 3D (for visualization)\n    print(\"   üìä Applying UMAP dimensionality reduction to 3D...\")\n    umap_3d = umap.UMAP(n_components=3, random_state=random_state, n_neighbors=15, min_dist=0.1)\n    X_umap_3d = umap_3d.fit_transform(X_scaled)\n    \n    # Perform Fuzzy C-Means clustering on original scaled data\n    print(\"   üéØ Running Fuzzy C-Means clustering...\")\n    # Note: cmeans expects data in shape (n_features, n_samples)\n    X_scaled_T = X_scaled.T\n    \n    # Run FCM\n    cntr, u, u0, d, jm, p, fpc = fuzz.cluster.cmeans(\n        X_scaled_T, \n        n_clusters, \n        m, \n        error=0.005, \n        maxiter=1000, \n        init=None,\n        seed=random_state\n    )\\n    \n    # Get hard cluster assignments (highest membership)\n    cluster_labels = np.argmax(u, axis=0)\n    \n    # Create 3D visualization with membership probabilities\n    print(\"   üé® Creating 3D visualization with membership probabilities...\")\n    \n    # Create color palette for clusters\n    colors = px.colors.qualitative.Set3[:n_clusters]\n    \n    # Create multiple visualizations\n    fig = make_subplots(\n        rows=2, cols=2,\n        specs=[[{\"type\": \"scatter3d\"}, {\"type\": \"scatter3d\"}],\n               [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]],\n        subplot_titles=('FCM Clusters (Hard Assignment)', 'FCM Clusters (Membership Strength)',\n                       'Membership Probability Distribution', 'Cluster Center Distances'),\n        vertical_spacing=0.1\n    )\n    \n    # Plot 1: Hard cluster assignments\n    for i in range(n_clusters):\n        cluster_mask = cluster_labels == i\n        fig.add_trace(\n            go.Scatter3d(\n                x=X_umap_3d[cluster_mask, 0],\n                y=X_umap_3d[cluster_mask, 1],\n                z=X_umap_3d[cluster_mask, 2],\n                mode='markers',\n                marker=dict(\n                    size=6,\n                    color=colors[i],\n                    opacity=0.7\n                ),\n                name=f'Cluster {i+1}',\n                text=[f'Cluster {i+1}<br>Point {j}<br>Max Membership: {u[i,j]:.3f}' \n                      for j in np.where(cluster_mask)[0]],\n                hovertemplate='<b>%{text}</b><br>' +\n                             'UMAP X: %{x:.2f}<br>' +\n                             'UMAP Y: %{y:.2f}<br>' +\n                             'UMAP Z: %{z:.2f}<extra></extra>',\n                showlegend=True\n            ),\n            row=1, col=1\n        )\n    \n    # Plot 2: Membership strength visualization (size based on max membership)\n    max_memberships = np.max(u, axis=0)\n    for i in range(n_clusters):\n        cluster_mask = cluster_labels == i\n        fig.add_trace(\n            go.Scatter3d(\n                x=X_umap_3d[cluster_mask, 0],\n                y=X_umap_3d[cluster_mask, 1],\n                z=X_umap_3d[cluster_mask, 2],\n                mode='markers',\n                marker=dict(\n                    size=max_memberships[cluster_mask] * 15 + 3,  # Scale size by membership\n                    color=colors[i],\n                    opacity=max_memberships[cluster_mask],  # Transparency by membership\n                    line=dict(width=1, color='black')\n                ),\n                name=f'Cluster {i+1} (Fuzzy)',\n                text=[f'Cluster {i+1}<br>Point {j}<br>Membership: {max_memberships[j]:.3f}' \n                      for j in np.where(cluster_mask)[0]],\n                hovertemplate='<b>%{text}</b><br>' +\n                             'UMAP X: %{x:.2f}<br>' +\n                             'UMAP Y: %{y:.2f}<br>' +\n                             'UMAP Z: %{z:.2f}<extra></extra>',\n                showlegend=False\n            ),\n            row=1, col=2\n        )\n    \n    # Plot 3: Membership probability distribution\n    membership_data = []\n    for i in range(n_clusters):\n        membership_data.extend([(f'Cluster {i+1}', prob) for prob in u[i, :]])\n    \n    membership_df = pd.DataFrame(membership_data, columns=['Cluster', 'Membership'])\n    \n    for i, cluster in enumerate([f'Cluster {j+1}' for j in range(n_clusters)]):\n        cluster_memberships = membership_df[membership_df['Cluster'] == cluster]['Membership']\n        fig.add_trace(\n            go.Histogram(\n                x=cluster_memberships,\n                name=cluster,\n                opacity=0.7,\n                nbinsx=20,\n                marker_color=colors[i],\n                showlegend=False\n            ),\n            row=2, col=1\n        )\n    \n    # Plot 4: Distance to cluster centers\n    sample_indices = np.random.choice(len(cluster_labels), min(100, len(cluster_labels)), replace=False)\n    \n    for i in range(n_clusters):\n        cluster_distances = d[i, sample_indices]\n        fig.add_trace(\n            go.Scatter(\n                x=sample_indices,\n                y=cluster_distances,\n                mode='markers',\n                name=f'Distance to Center {i+1}',\n                marker=dict(color=colors[i], size=6),\n                showlegend=False\n            ),\n            row=2, col=2\n        )\n    \n    # Update layout\n    fig.update_layout(\n        title_text='Fuzzy C-Means Clustering Analysis with UMAP 3D Visualization',\n        height=800,\n        showlegend=True\n    )\n    \n    # Update 3D subplot layouts\n    fig.update_layout(\n        scene=dict(\n            xaxis_title='UMAP Dim 1',\n            yaxis_title='UMAP Dim 2',\n            zaxis_title='UMAP Dim 3'\n        ),\n        scene2=dict(\n            xaxis_title='UMAP Dim 1',\n            yaxis_title='UMAP Dim 2',\n            zaxis_title='UMAP Dim 3'\n        )\n    )\n    \n    # Update 2D subplot layouts\n    fig.update_xaxes(title_text=\"Membership Probability\", row=2, col=1)\n    fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n    fig.update_xaxes(title_text=\"Sample Index\", row=2, col=2)\n    fig.update_yaxes(title_text=\"Distance to Center\", row=2, col=2)\n    \n    fig.show()\n    \n    # Create membership probability heatmap\n    print(\"   üî• Creating membership probability heatmap...\")\n    \n    # Select a sample of points for visualization\n    sample_size = min(50, len(cluster_labels))\n    sample_indices = np.random.choice(len(cluster_labels), sample_size, replace=False)\n    \n    plt.figure(figsize=(12, 8))\n    \n    # Create heatmap of membership probabilities\n    membership_sample = u[:, sample_indices]\n    sns.heatmap(\n        membership_sample, \n        annot=True, \n        fmt='.2f', \n        cmap='viridis',\n        xticklabels=[f'Point {i}' for i in sample_indices],\n        yticklabels=[f'Cluster {i+1}' for i in range(n_clusters)],\n        cbar_kws={'label': 'Membership Probability'}\n    )\n    plt.title(f'FCM Membership Probabilities (Sample of {sample_size} points)')\n    plt.xlabel('Data Points')\n    plt.ylabel('Clusters')\n    plt.tight_layout()\n    plt.show()\n    \n    # Calculate FCM statistics\n    fcm_stats = {\n        'fpc': fpc,  # Fuzzy Partition Coefficient\n        'n_clusters': n_clusters,\n        'n_points': len(cluster_labels),\n        'avg_max_membership': np.mean(np.max(u, axis=0)),\n        'cluster_sizes': [np.sum(cluster_labels == i) for i in range(n_clusters)]\n    }\n    \n    # Print FCM summary\n    print(f\"\\n=== Fuzzy C-Means Clustering Results ===\")\n    print(f\"Number of clusters: {n_clusters}\")\n    print(f\"Total data points: {len(cluster_labels)}\")\n    print(f\"Fuzzy Partition Coefficient (FPC): {fpc:.4f}\")\n    print(f\"Average maximum membership: {fcm_stats['avg_max_membership']:.4f}\")\n    print(f\"Objective function value: {jm[-1]:.4f}\")\n    \n    for i, size in enumerate(fcm_stats['cluster_sizes']):\n        percentage = size / len(cluster_labels) * 100\n        print(f\"Cluster {i+1}: {size} points ({percentage:.1f}%)\")\n    \n    return {\n        'cluster_labels': cluster_labels,\n        'membership_matrix': u,\n        'cluster_centers': cntr,\n        'umap_coordinates': X_umap_3d,\n        'umap_model': umap_3d,\n        'scaler': scaler,\n        'fcm_stats': fcm_stats,\n        'distances': d,\n        'X_scaled': X_scaled\n    }\n\n# Apply FCM clustering with UMAP\nif 'X_ml' in locals():\n    fcm_results = perform_fcm_clustering_with_umap(X_ml, n_clusters=4, m=2.0)\n    print(\"‚úÖ Fuzzy C-Means clustering with UMAP completed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Fuzzy C-Means Clustering with UMAP 3D Visualization\n\nImplement Fuzzy C-Means clustering to show soft cluster assignments and membership probabilities.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def perform_kmeans_clustering_with_umap(X, n_clusters=4, random_state=42):\n    \"\"\"\n    Perform K-Means clustering and visualize with UMAP 3D reduction.\n    \n    Parameters:\n    -----------\n    X : pd.DataFrame or np.array\n        Feature matrix\n    n_clusters : int\n        Number of clusters for K-Means\n    random_state : int\n        Random state for reproducibility\n        \n    Returns:\n    --------\n    dict : Dictionary containing clustering results and UMAP coordinates\n    \"\"\"\n    print(f\"üîç Performing K-Means clustering (k={n_clusters}) with UMAP 3D visualization...\")\n    \n    # Prepare data - ensure numerical and no missing values\n    X_clean = X.select_dtypes(include=[np.number]).fillna(X.select_dtypes(include=[np.number]).median())\n    \n    # Standardize features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X_clean)\n    \n    # Perform K-Means clustering\n    print(\"   üéØ Running K-Means clustering...\")\n    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n    cluster_labels = kmeans.fit_predict(X_scaled)\n    \n    # Perform UMAP dimensionality reduction to 3D\n    print(\"   üìä Applying UMAP dimensionality reduction to 3D...\")\n    umap_3d = umap.UMAP(n_components=3, random_state=random_state, n_neighbors=15, min_dist=0.1)\n    X_umap_3d = umap_3d.fit_transform(X_scaled)\n    \n    # Create 3D visualization\n    print(\"   üé® Creating 3D visualization...\")\n    \n    # Create color palette for clusters\n    colors = px.colors.qualitative.Set3[:n_clusters]\n    \n    # Create 3D scatter plot with Plotly\n    fig = go.Figure()\n    \n    for i in range(n_clusters):\n        cluster_mask = cluster_labels == i\n        fig.add_trace(go.Scatter3d(\n            x=X_umap_3d[cluster_mask, 0],\n            y=X_umap_3d[cluster_mask, 1],\n            z=X_umap_3d[cluster_mask, 2],\n            mode='markers',\n            marker=dict(\n                size=6,\n                color=colors[i],\n                opacity=0.7\n            ),\n            name=f'Cluster {i+1}',\n            text=[f'Cluster {i+1}<br>Point {j}' for j in np.where(cluster_mask)[0]],\n            hovertemplate='<b>%{text}</b><br>' +\n                         'UMAP X: %{x:.2f}<br>' +\n                         'UMAP Y: %{y:.2f}<br>' +\n                         'UMAP Z: %{z:.2f}<extra></extra>'\n        ))\n    \n    fig.update_layout(\n        title='K-Means Clustering with UMAP 3D Visualization',\n        scene=dict(\n            xaxis_title='UMAP Dimension 1',\n            yaxis_title='UMAP Dimension 2',\n            zaxis_title='UMAP Dimension 3'\n        ),\n        width=800,\n        height=600\n    )\n    \n    fig.show()\n    \n    # Calculate cluster statistics\n    cluster_stats = {}\n    for i in range(n_clusters):\n        cluster_mask = cluster_labels == i\n        cluster_stats[f'Cluster_{i+1}'] = {\n            'size': np.sum(cluster_mask),\n            'percentage': np.sum(cluster_mask) / len(cluster_labels) * 100,\n            'center_umap': X_umap_3d[cluster_mask].mean(axis=0)\n        }\n    \n    # Print cluster summary\n    print(f\"\\n=== K-Means Clustering Results ===\")\n    print(f\"Number of clusters: {n_clusters}\")\n    print(f\"Total data points: {len(cluster_labels)}\")\n    print(f\"Silhouette score: {kmeans.inertia_:.2f}\")\n    \n    for cluster_name, stats in cluster_stats.items():\n        print(f\"{cluster_name}: {stats['size']} points ({stats['percentage']:.1f}%)\")\n    \n    return {\n        'cluster_labels': cluster_labels,\n        'umap_coordinates': X_umap_3d,\n        'kmeans_model': kmeans,\n        'umap_model': umap_3d,\n        'scaler': scaler,\n        'cluster_stats': cluster_stats,\n        'X_scaled': X_scaled\n    }\n\n# Apply K-Means clustering with UMAP\nif 'X_ml' in locals():\n    kmeans_results = perform_kmeans_clustering_with_umap(X_ml, n_clusters=4)\n    print(\"‚úÖ K-Means clustering with UMAP completed successfully!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Advanced Clustering and Dimensionality Reduction\n\n### K-Means Clustering with UMAP 3D Visualization\n\nImplement K-Means clustering and visualize results using UMAP dimensionality reduction to 3D space.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}