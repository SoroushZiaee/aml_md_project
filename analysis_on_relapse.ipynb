{
 "cells": [
  {
   "cell_type": "code",
   "source": "# ROC Curves and Calibration Analysis\ndef plot_roc_curves_and_calibration(model_results, y_true, title_suffix=\"\"):\n    \"\"\"Create comprehensive ROC curves and calibration plots for relapse prediction.\"\"\"\n    from sklearn.metrics import roc_curve, auc, calibration_curve, brier_score_loss\n    from sklearn.calibration import calibration_curve\n    \n    # Set up the plotting layout\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    \n    # Colors for different models\n    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FED766']\n    \n    # 1. ROC Curves (Top Left)\n    ax_roc = axes[0, 0]\n    \n    for i, (model_name, results) in enumerate(model_results.items()):\n        y_pred_proba = results['y_pred_proba'][:, 1]  # Probability of positive class\n        \n        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n        roc_auc = auc(fpr, tpr)\n        \n        ax_roc.plot(fpr, tpr, color=colors[i], lw=2, \n                   label=f'{model_name} (AUC = {roc_auc:.3f})')\n    \n    # Plot random classifier line\n    ax_roc.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--', alpha=0.7, label='Random')\n    \n    ax_roc.set_xlim([0.0, 1.0])\n    ax_roc.set_ylim([0.0, 1.05])\n    ax_roc.set_xlabel('False Positive Rate', fontsize=12)\n    ax_roc.set_ylabel('True Positive Rate', fontsize=12)\n    ax_roc.set_title(f'ROC Curves - Relapse Prediction{title_suffix}', fontsize=14, pad=20)\n    ax_roc.legend(loc=\"lower right\", fontsize=10)\n    ax_roc.grid(alpha=0.3)\n    \n    # 2. Precision-Recall Curves (Top Right)\n    from sklearn.metrics import precision_recall_curve, average_precision_score\n    ax_pr = axes[0, 1]\n    \n    for i, (model_name, results) in enumerate(model_results.items()):\n        y_pred_proba = results['y_pred_proba'][:, 1]\n        \n        precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n        avg_precision = average_precision_score(y_true, y_pred_proba)\n        \n        ax_pr.plot(recall, precision, color=colors[i], lw=2,\n                  label=f'{model_name} (AP = {avg_precision:.3f})')\n    \n    # Baseline (random classifier for imbalanced data)\n    baseline = np.sum(y_true) / len(y_true)\n    ax_pr.axhline(y=baseline, color='gray', linestyle='--', alpha=0.7, \n                 label=f'Baseline (AP = {baseline:.3f})')\n    \n    ax_pr.set_xlim([0.0, 1.0])\n    ax_pr.set_ylim([0.0, 1.05])\n    ax_pr.set_xlabel('Recall', fontsize=12)\n    ax_pr.set_ylabel('Precision', fontsize=12)\n    ax_pr.set_title(f'Precision-Recall Curves{title_suffix}', fontsize=14, pad=20)\n    ax_pr.legend(loc=\"lower left\", fontsize=10)\n    ax_pr.grid(alpha=0.3)\n    \n    # 3. Calibration Plot (Bottom Left)\n    ax_cal = axes[1, 0]\n    \n    for i, (model_name, results) in enumerate(model_results.items()):\n        y_pred_proba = results['y_pred_proba'][:, 1]\n        \n        fraction_of_positives, mean_predicted_value = calibration_curve(\n            y_true, y_pred_proba, n_bins=10, normalize=False\n        )\n        \n        brier_score = brier_score_loss(y_true, y_pred_proba)\n        \n        ax_cal.plot(mean_predicted_value, fraction_of_positives, \"s-\", \n                   color=colors[i], lw=2, label=f'{model_name} (Brier = {brier_score:.3f})')\n    \n    # Perfect calibration line\n    ax_cal.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n    \n    ax_cal.set_xlim([0.0, 1.0])\n    ax_cal.set_ylim([0.0, 1.0])\n    ax_cal.set_xlabel('Mean Predicted Probability', fontsize=12)\n    ax_cal.set_ylabel('Fraction of Positives', fontsize=12)\n    ax_cal.set_title(f'Calibration Plot (Reliability Diagram){title_suffix}', fontsize=14, pad=20)\n    ax_cal.legend(loc=\"lower right\", fontsize=10)\n    ax_cal.grid(alpha=0.3)\n    \n    # 4. Performance Summary Table (Bottom Right)\n    ax_table = axes[1, 1]\n    ax_table.axis('off')\n    \n    # Create performance summary\n    summary_data = []\n    metrics = ['Accuracy', 'ROC AUC', 'F1 Score', 'Brier Score']\n    \n    for model_name, results in model_results.items():\n        y_pred_proba = results['y_pred_proba'][:, 1]\n        brier = brier_score_loss(y_true, y_pred_proba)\n        \n        row = [\n            f\"{results['accuracy']:.3f}\",\n            f\"{results['roc_auc']:.3f}\",\n            f\"{results['f1_score']:.3f}\",\n            f\"{brier:.3f}\"\n        ]\n        summary_data.append(row)\n    \n    # Create table\n    table = ax_table.table(cellText=summary_data,\n                          rowLabels=list(model_results.keys()),\n                          colLabels=metrics,\n                          cellLoc='center',\n                          loc='center',\n                          colWidths=[0.2, 0.2, 0.2, 0.2])\n    \n    table.auto_set_font_size(False)\n    table.set_fontsize(10)\n    table.scale(1, 2)\n    \n    # Style the table\n    for i in range(len(metrics)):\n        table[(0, i)].set_facecolor('#E8E8E8')\n        table[(0, i)].set_text_props(weight='bold')\n    \n    for i in range(len(model_results)):\n        table[(i+1, -1)].set_facecolor('#F5F5F5')\n    \n    ax_table.set_title(f'Performance Summary{title_suffix}', fontsize=14, pad=20)\n    \n    plt.suptitle(f'Comprehensive Model Evaluation - Relapse Prediction{title_suffix}', \n                 fontsize=16, y=0.98)\n    plt.tight_layout()\n    plt.show()\n    \n    # Print detailed analysis\n    print(f\"\\\\n{'='*80}\")\n    print(f\"üìä DETAILED PERFORMANCE ANALYSIS{title_suffix}\")\n    print(f\"{'='*80}\")\n    \n    best_model = max(model_results.items(), key=lambda x: x[1]['roc_auc'])\n    print(f\"üèÜ Best Model by ROC AUC: {best_model[0]} (AUC = {best_model[1]['roc_auc']:.3f})\")\n    \n    print(f\"\\\\nüìà Model Rankings:\")\n    sorted_models = sorted(model_results.items(), key=lambda x: x[1]['roc_auc'], reverse=True)\n    for i, (model_name, results) in enumerate(sorted_models, 1):\n        print(f\"  {i}. {model_name:15s} | ROC AUC: {results['roc_auc']:.3f} | \"\n              f\"Accuracy: {results['accuracy']:.3f} | F1: {results['f1_score']:.3f}\")\n\n# Generate ROC curves and calibration plots\nprint(\"\\\\nüéØ Creating ROC curves and calibration analysis...\")\nplot_roc_curves_and_calibration(model_results, y_relapse_processed, title_suffix=\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Machine learning pipeline for relapse prediction\ndef train_relapse_prediction_models(X_selected, y_relapse, cv=5):\n    \"\"\"Train multiple ML models specifically for relapse prediction.\"\"\"\n    from sklearn.model_selection import GridSearchCV, StratifiedKFold, cross_val_score, cross_val_predict\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.svm import SVC\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.metrics import classification_report, roc_auc_score\n    \n    # Define hyperparameter grids\n    param_grids = {\n        \"Random Forest\": {\n            'n_estimators': [50, 100, 200],\n            'max_depth': [5, 10, 20, None],\n            'min_samples_split': [2, 5, 10],\n            'min_samples_leaf': [1, 2, 4]\n        },\n        \"SVM\": {\n            'C': [0.1, 1, 10],\n            'kernel': ['rbf', 'linear'],\n            'gamma': ['scale', 'auto']\n        },\n        \"Naive Bayes\": {}\n    }\n    \n    # Initialize classifiers\n    classifiers = {\n        \"Random Forest\": RandomForestClassifier(random_state=42),\n        \"SVM\": SVC(random_state=42, probability=True),\n        \"Naive Bayes\": GaussianNB()\n    }\n    \n    results = {}\n    kf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n    \n    print(f\"üîπ Training models for relapse prediction with {cv}-fold CV üîπ\")\n    print(f\"Dataset: {X_selected.shape[0]} samples, {X_selected.shape[1]} features\")\n    print(f\"Class distribution: {np.bincount(y_relapse)}\")\n    \n    for model_name, model in classifiers.items():\n        print(f\"\\n  Training {model_name}...\")\n        \n        # Hyperparameter tuning\n        if param_grids[model_name]:\n            grid_search = GridSearchCV(\n                model, param_grids[model_name], cv=3, scoring='accuracy', n_jobs=-1\n            )\n            grid_search.fit(X_selected, y_relapse)\n            best_model = grid_search.best_estimator_\n            best_params = grid_search.best_params_\n            print(f\"    Best params: {best_params}\")\n        else:\n            best_model = model\n            best_params = {}\n            best_model.fit(X_selected, y_relapse)\n        \n        # Cross-validation evaluation\n        accuracy_scores = cross_val_score(best_model, X_selected, y_relapse, cv=kf, scoring='accuracy')\n        roc_auc_scores = cross_val_score(best_model, X_selected, y_relapse, cv=kf, scoring='roc_auc')\n        f1_scores = cross_val_score(best_model, X_selected, y_relapse, cv=kf, scoring='f1')\n        \n        # Generate predictions for detailed analysis\n        y_pred = cross_val_predict(best_model, X_selected, y_relapse, cv=kf)\n        y_pred_proba = cross_val_predict(best_model, X_selected, y_relapse, cv=kf, method='predict_proba')\n        \n        # Store results\n        results[model_name] = {\n            \"best_model\": best_model,\n            \"best_params\": best_params,\n            \"accuracy\": np.mean(accuracy_scores),\n            \"accuracy_std\": np.std(accuracy_scores),\n            \"roc_auc\": np.mean(roc_auc_scores),\n            \"roc_auc_std\": np.std(roc_auc_scores),\n            \"f1_score\": np.mean(f1_scores),\n            \"f1_std\": np.std(f1_scores),\n            \"y_pred\": y_pred,\n            \"y_pred_proba\": y_pred_proba,\n            \"classification_report\": classification_report(y_relapse, y_pred, output_dict=True)\n        }\n        \n        print(f\"    Accuracy: {np.mean(accuracy_scores):.3f} (¬±{np.std(accuracy_scores):.3f})\")\n        print(f\"    ROC AUC:  {np.mean(roc_auc_scores):.3f} (¬±{np.std(roc_auc_scores):.3f})\")\n        print(f\"    F1-Score: {np.mean(f1_scores):.3f} (¬±{np.std(f1_scores):.3f})\")\n    \n    return results\n\n# Train models\nprint(\"üöÄ Starting relapse prediction model training...\")\nmodel_results = train_relapse_prediction_models(X_selected, y_relapse_processed, cv=5)\n\n# Display summary\nprint(f\"\\n{'='*60}\")\nprint(\"üèÜ MODEL TRAINING RESULTS SUMMARY\")\nprint(f\"{'='*60}\")\nfor model_name, results in model_results.items():\n    print(f\"{model_name:15s} | Accuracy: {results['accuracy']:.3f} | ROC AUC: {results['roc_auc']:.3f} | F1: {results['f1_score']:.3f}\")\nprint(f\"{'='*60}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Feature selection optimization for relapse prediction\ndef optimize_feature_selection_for_relapse(X, y_relapse, method=\"select_k_best\"):\n    \"\"\"Perform feature selection optimization specifically for relapse prediction.\"\"\"\n    from sklearn.model_selection import cross_val_score\n    from sklearn.feature_selection import SelectKBest, f_classif, RFE\n    from sklearn.ensemble import RandomForestClassifier\n    \n    model = RandomForestClassifier(random_state=42, n_estimators=100)\n    \n    if method == \"select_k_best\":\n        k_values = [1, 2, 3, 5, 8, 13, 21, 34]\n        k_values = [k for k in k_values if k <= X.shape[1]]\n        \n        best_k = k_values[0]\n        best_score = -np.inf\n        best_selector = None\n        \n        print(f\"Testing k values: {k_values}\")\n        \n        for k in k_values:\n            selector = SelectKBest(score_func=f_classif, k=k)\n            X_selected = selector.fit_transform(X, y_relapse)\n            \n            scores = cross_val_score(model, X_selected, y_relapse, cv=5, scoring='accuracy')\n            score = np.mean(scores)\n            \n            print(f\"k={k}: accuracy={score:.3f} (¬±{np.std(scores):.3f})\")\n            \n            if score > best_score:\n                best_k, best_score = k, score\n                best_selector = selector\n        \n        selected_features = X.columns[best_selector.get_support()].tolist()\n        print(f\"\\n‚úÖ Optimal k={best_k}, score={best_score:.3f}\")\n        print(f\"Selected features: {selected_features}\")\n        \n        return best_selector.transform(X), selected_features, best_k\n    \n    elif method == \"rfe\":\n        n_features = [1, 2, 3, 5, 8, 13, 21]\n        n_features = [n for n in n_features if n <= X.shape[1]]\n        \n        best_n = n_features[0]\n        best_score = -np.inf\n        best_selector = None\n        \n        for n in n_features:\n            selector = RFE(model, n_features_to_select=n)\n            X_selected = selector.fit_transform(X, y_relapse)\n            \n            scores = cross_val_score(model, X_selected, y_relapse, cv=5, scoring='accuracy')\n            score = np.mean(scores)\n            \n            if score > best_score:\n                best_n, best_score = n, score\n                best_selector = selector\n        \n        selected_features = X.columns[best_selector.get_support()].tolist()\n        return best_selector.transform(X), selected_features, best_n\n\ndef create_feature_sets_for_relapse():\n    \"\"\"Define feature sets specifically for relapse prediction.\"\"\"\n    feature_sets = {\n        \"dynamics_only\": {\n            \"name\": \"Chimerism Dynamics Only\",\n            \"features\": [\"d(30-60)_cd3+\", \"d(60-100)_cd3+\", \"d(30-60)_cd3-\", \"d(60-100)_cd3-\"]\n        },\n        \"timepoints_only\": {\n            \"name\": \"Time Points Only\", \n            \"features\": [\"d30_cd3+\", \"d30_cd3-\", \"d60_cd3+\", \"d60_cd3-\", \"d100_cd3+\", \"d100_cd3-\"]\n        },\n        \"statistics_only\": {\n            \"name\": \"Statistical Features Only\",\n            \"features\": [\"mean_cd3+\", \"mean_cd3-\", \"std_cd3+\", \"std_cd3-\", \"cv_cd3+\", \"cv_cd3-\"]\n        },\n        \"day100_focus\": {\n            \"name\": \"Day 100 Focus\",\n            \"features\": [\"d100_cd3+\", \"d100_cd3-\", \"d(60-100)_cd3+\", \"d(60-100)_cd3-\"]\n        },\n        \"comprehensive\": {\n            \"name\": \"Comprehensive Chimerism\",\n            \"features\": [\"d30_cd3+\", \"d60_cd3+\", \"d100_cd3+\", \"d30_cd3-\", \"d60_cd3-\", \"d100_cd3-\",\n                        \"d(30-60)_cd3+\", \"d(60-100)_cd3+\", \"d(30-60)_cd3-\", \"d(60-100)_cd3-\",\n                        \"mean_cd3+\", \"std_cd3+\", \"cv_cd3+\", \"mean_cd3-\", \"std_cd3-\", \"cv_cd3-\"]\n        },\n        \"minimal_predictive\": {\n            \"name\": \"Minimal Predictive Set\",\n            \"features\": [\"d(60-100)_cd3+\", \"d100_cd3-\", \"std_cd3+\"]  # Based on unified analysis results\n        }\n    }\n    \n    return feature_sets\n\n# Perform feature selection\nprint(\"üîç Performing feature selection for relapse prediction...\")\nX_selected, selected_features, optimal_k = optimize_feature_selection_for_relapse(\n    X_preprocessed, y_relapse_processed, method=\"select_k_best\"\n)\n\nprint(f\"\\nOptimal feature selection completed:\")\nprint(f\"Original features: {X_preprocessed.shape[1]}\")\nprint(f\"Selected features: {len(selected_features)}\")\nprint(f\"Selected feature names: {selected_features}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Feature extraction and preprocessing for relapse prediction only\ndef extract_features_and_relapse_target(df):\n    \"\"\"Extract features and relapse target specifically.\"\"\"\n    # Filter for AML/MDS patients (Disease == 1)\n    if 'disease' in df.columns:\n        df_filtered = df[df['disease'] == 1].copy()\n        print(f\"üë• Filtered to AML/MDS patients: {len(df_filtered)} from {len(df)}\")\n    else:\n        print(\"‚ö†Ô∏è 'disease' column not found, using all patients\")\n        df_filtered = df.copy()\n    \n    # Identify feature columns (exclude outcome variables)\n    exclude_cols = ['dose_dli', 'dose_dli_2', 'indication_for_dli', 'post_dli_gvhd',\n                   'grade_at_onset', 'time_to_onset', 'highest_grade', 'dli']\n    \n    feature_cols = [col for col in df_filtered.columns \n                   if not col.startswith('y_') and col not in exclude_cols]\n    \n    # Extract features and relapse target\n    X = df_filtered[feature_cols].copy()\n    y_relapse = df_filtered['y_relapse'].copy() if 'y_relapse' in df_filtered.columns else None\n    \n    if y_relapse is None:\n        raise ValueError(\"y_relapse column not found in dataset!\")\n    \n    # Reset indices\n    X.reset_index(drop=True, inplace=True)\n    y_relapse.reset_index(drop=True, inplace=True)\n    \n    print(f\"Features shape: {X.shape}\")\n    print(f\"Relapse target shape: {y_relapse.shape}\")\n    print(f\"Relapse distribution: {y_relapse.value_counts().to_dict()}\")\n    \n    return X, y_relapse\n\ndef preprocess_features(X, y_relapse):\n    \"\"\"Apply preprocessing specific to relapse prediction.\"\"\"\n    print(\"üîß Applying preprocessing...\")\n    \n    X_processed = X.copy()\n    \n    # Handle categorical variables\n    categorical_cols = X_processed.select_dtypes(include=['object', 'category']).columns\n    if len(categorical_cols) > 0:\n        print(f\"üè∑Ô∏è Encoding {len(categorical_cols)} categorical columns\")\n        for col in categorical_cols:\n            le = LabelEncoder()\n            X_processed[col] = le.fit_transform(X_processed[col].astype(str))\n    \n    # Imputation for numerical columns\n    numerical_cols = X_processed.select_dtypes(include=['float64', 'int64']).columns.tolist()\n    \n    if numerical_cols:\n        print(f\"üìä Imputing {len(numerical_cols)} numerical columns\")\n        imputer = SimpleImputer(strategy='median')\n        X_processed[numerical_cols] = imputer.fit_transform(X_processed[numerical_cols])\n    \n    # Scale features\n    print(\"üìè Scaling features\")\n    scaler = StandardScaler()\n    X_scaled = pd.DataFrame(\n        scaler.fit_transform(X_processed),\n        columns=X_processed.columns,\n        index=X_processed.index\n    )\n    \n    # Ensure y_relapse is binary\n    y_processed = pd.to_numeric(y_relapse, errors='coerce')\n    y_processed = y_processed.fillna(0).astype(int)\n    \n    print(f\"‚úÖ Preprocessing completed\")\n    print(f\"Remaining missing values: {X_scaled.isnull().sum().sum()}\")\n    \n    return X_scaled, y_processed, scaler\n\n# Extract and preprocess data\nX, y_relapse = extract_features_and_relapse_target(df_enhanced)\nX_preprocessed, y_relapse_processed, scaler = preprocess_features(X, y_relapse)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Data loading and basic feature engineering (adapted from unified notebook)\ndef load_data(file_path=\"main_dataset.xlsx\", sheet_name=\"Sheet1\", skip_rows=2):\n    \"\"\"Load dataset from Excel file with proper handling of headers and empty rows.\"\"\"\n    logging.info(\"Loading dataset...\")\n    \n    try:\n        if file_path.endswith('.csv'):\n            df = pd.read_csv(file_path)\n        else:\n            df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=skip_rows)\n    except FileNotFoundError:\n        logging.info(\"Excel file not found, trying preprocessed CSV...\")\n        df = pd.read_csv(\"preprocessed_ml_for_aml_mds.csv\")\n    \n    df.dropna(axis=0, how='all', inplace=True)\n    logging.info(f\"Dataset loaded successfully. Shape: {df.shape}\")\n    return df\n\ndef create_chimerism_dynamics_features(df):\n    \"\"\"Engineer comprehensive chimerism dynamics features.\"\"\"\n    print(\"üîß Engineering chimerism dynamics features...\")\n    df_enhanced = df.copy()\n    \n    required_cols = ['d30_cd3+', 'd60_cd3+', 'd100_cd3+', 'd30_cd3-', 'd60_cd3-', 'd100_cd3-']\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    \n    if missing_cols:\n        print(f\"‚ö†Ô∏è Missing required columns: {missing_cols}\")\n        print(f\"Available columns that contain 'cd3': {[col for col in df.columns if 'cd3' in col.lower()]}\")\n        return df_enhanced\n    \n    # Convert to numeric\n    for col in required_cols:\n        df_enhanced[col] = pd.to_numeric(df_enhanced[col], errors='coerce')\n    \n    # Time-point differences\n    df_enhanced[\"d(30-60)_cd3+\"] = df_enhanced[\"d30_cd3+\"] - df_enhanced[\"d60_cd3+\"]\n    df_enhanced[\"d(60-100)_cd3+\"] = df_enhanced[\"d60_cd3+\"] - df_enhanced[\"d100_cd3+\"]\n    df_enhanced[\"d(30-60)_cd3-\"] = df_enhanced[\"d30_cd3-\"] - df_enhanced[\"d60_cd3-\"]\n    df_enhanced[\"d(60-100)_cd3-\"] = df_enhanced[\"d60_cd3-\"] - df_enhanced[\"d100_cd3-\"]\n    df_enhanced[\"d(30-100)_cd3+\"] = df_enhanced[\"d30_cd3+\"] - df_enhanced[\"d100_cd3+\"]\n    df_enhanced[\"d(30-100)_cd3-\"] = df_enhanced[\"d30_cd3-\"] - df_enhanced[\"d100_cd3-\"]\n    \n    # Statistical features\n    cd3_pos_cols = [\"d30_cd3+\", \"d60_cd3+\", \"d100_cd3+\"]\n    cd3_neg_cols = [\"d30_cd3-\", \"d60_cd3-\", \"d100_cd3-\"]\n    \n    df_enhanced[\"mean_cd3+\"] = df_enhanced[cd3_pos_cols].mean(axis=1)\n    df_enhanced[\"std_cd3+\"] = df_enhanced[cd3_pos_cols].std(axis=1)\n    df_enhanced[\"cv_cd3+\"] = df_enhanced[\"std_cd3+\"] / (df_enhanced[\"mean_cd3+\"] + 0.001)\n    \n    df_enhanced[\"mean_cd3-\"] = df_enhanced[cd3_neg_cols].mean(axis=1)\n    df_enhanced[\"std_cd3-\"] = df_enhanced[cd3_neg_cols].std(axis=1)\n    df_enhanced[\"cv_cd3-\"] = df_enhanced[\"std_cd3-\"] / (df_enhanced[\"mean_cd3-\"] + 0.001)\n    \n    print(f\"‚úÖ Feature engineering completed. Added {len(df_enhanced.columns) - len(df.columns)} new features.\")\n    return df_enhanced\n\n# Load and prepare data\ndf = load_data(file_path=\"preprocessed_ml_for_aml_mds.csv\")\ndf_enhanced = create_chimerism_dynamics_features(df)\n\nprint(f\"Dataset shape: {df_enhanced.shape}\")\nprint(f\"Relapse target distribution:\")\nif 'y_relapse' in df_enhanced.columns:\n    print(df_enhanced['y_relapse'].value_counts())\nelse:\n    print(\"y_relapse column not found!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Relapse Prediction Analysis\n\n## Comprehensive Analysis of CD3+ Chimerism Dynamics and Relapse Prediction\n\nThis notebook focuses specifically on predicting disease relapse using CD3+ chimerism dynamics and other clinical features in AML/MDS patients post-transplant.\n\n### Analysis Framework:\n1. **Data Loading and Feature Engineering**: Enhanced chimerism dynamics features\n2. **Feature Selection Optimization**: Multiple methods for optimal feature selection\n3. **Machine Learning Pipeline**: Classification models with cross-validation\n4. **ROC Curves and Calibration Analysis**: Performance evaluation and probability calibration\n5. **Clinical Translation**: Risk prediction models and feature importance analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Core data processing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport logging\nimport sys\nimport os\nimport joblib\nimport glob\nfrom datetime import datetime\n\n# Machine learning libraries  \nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, mean_squared_error, confusion_matrix\nfrom sklearn.feature_selection import SelectKBest, f_classif, f_regression, RFE\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_curve, auc, calibration_curve, brier_score_loss, precision_recall_curve, average_precision_score\n\n# Configuration\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\", force=True)\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\nprint(\"‚úÖ Libraries imported successfully for relapse prediction analysis\")\nprint(f\"üìÖ Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}