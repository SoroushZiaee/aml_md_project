{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic AML/MDS Chimerism Dynamics Analysis\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook focuses on the foundational analysis of chimerism dynamics in AML/MDS transplant patients. The analysis centers on **feature engineering** and **exploratory data analysis** to understand how CD3+ and CD3- chimerism changes over time can predict transplant outcomes.\n",
    "\n",
    "### Key Research Questions\n",
    "\n",
    "**Tier 1 (Primary Focus):**\n",
    "- Can dynamic changes of CD3+ chimerism at Day 30, 60, and 100 predict disease relapse?\n",
    "- Do specific trend patterns (upward, downward, fluctuating) correlate with outcomes?\n",
    "- Can percentage changes (e.g., ‚â•20% increase from Day 30 to Day 100) improve prediction accuracy?\n",
    "\n",
    "**Tier 2:**\n",
    "- Can CD3+ chimerism dynamics predict other transplant outcomes (OS, GVHD, GRFS)?\n",
    "\n",
    "**Tier 3:**\n",
    "- Do interactions between CD3+ and CD3- chimerism improve prediction models?\n",
    "- Can chimerism variability metrics enhance outcome prediction?\n",
    "\n",
    "### Analysis Approach\n",
    "\n",
    "1. **Feature Engineering**: Create dynamic change indicators and statistical summaries\n",
    "2. **Pattern Classification**: Categorize chimerism trends into interpretable labels\n",
    "3. **Exploratory Analysis**: Visualize distributions and correlations\n",
    "4. **Predictive Modeling**: Test various feature combinations for outcome prediction\n",
    "5. **Model Evaluation**: Compare performance across different feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Loading\n",
    "\n",
    "Import necessary libraries and load the preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n",
      "üìÖ Analysis started at: 2025-07-18 15:36:47\n"
     ]
    }
   ],
   "source": [
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, mean_squared_error, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\", force=True)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üìÖ Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading preprocessed dataset...\n",
      "‚úÖ Dataset loaded successfully\n",
      "üìä Dataset shape: (258, 64)\n",
      "üî¢ Memory usage: 0.22 MB\n",
      "\n",
      "=== Dataset Overview ===\n",
      "Rows: 258\n",
      "Columns: 64\n",
      "Total missing values: 3,634\n",
      "\n",
      "=== Data Types ===\n",
      "float64: 31 columns\n",
      "int64: 24 columns\n",
      "object: 9 columns\n",
      "\n",
      "=== First 3 Rows ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>disease</th>\n",
       "      <th>disease_risk_index</th>\n",
       "      <th>hct_ci_score</th>\n",
       "      <th>time_from_diagnosis_to_alloSCT</th>\n",
       "      <th>aml_eln_risk_category</th>\n",
       "      <th>disease_state_at_transplant</th>\n",
       "      <th>mrd_status_prior_to_transplant</th>\n",
       "      <th>donor_type</th>\n",
       "      <th>cd34+_dose</th>\n",
       "      <th>...</th>\n",
       "      <th>y_relapse</th>\n",
       "      <th>y_cgvhd</th>\n",
       "      <th>y_time_to_onset</th>\n",
       "      <th>y_cgvhd_nih</th>\n",
       "      <th>y_time_to_onset_nih</th>\n",
       "      <th>y_agvhd</th>\n",
       "      <th>y_agvhd_grade_at_onset</th>\n",
       "      <th>y_agvhd_time_to_onset</th>\n",
       "      <th>y_agvhd_highest_grade</th>\n",
       "      <th>y_agvhd_time_to_highest_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>253.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.66</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  disease  disease_risk_index hct_ci_score  \\\n",
       "0   61        2                   3            2   \n",
       "1   53        5                   2            0   \n",
       "2   63        1                   2            0   \n",
       "\n",
       "   time_from_diagnosis_to_alloSCT  aml_eln_risk_category  \\\n",
       "0                           253.0                    NaN   \n",
       "1                           218.0                    NaN   \n",
       "2                           162.0                    3.0   \n",
       "\n",
       "   disease_state_at_transplant  mrd_status_prior_to_transplant  donor_type  \\\n",
       "0                          NaN                             NaN           2   \n",
       "1                          NaN                             NaN           2   \n",
       "2                          1.0                             1.0           1   \n",
       "\n",
       "   cd34+_dose  ...  y_relapse  y_cgvhd y_time_to_onset  y_cgvhd_nih  \\\n",
       "0        5.40  ...          0        0             NaN          NaN   \n",
       "1        5.04  ...          0        1           145.0          1.0   \n",
       "2        4.66  ...          1        0             NaN          NaN   \n",
       "\n",
       "  y_time_to_onset_nih  y_agvhd  y_agvhd_grade_at_onset  y_agvhd_time_to_onset  \\\n",
       "0                 NaN        1                     1.0                  159.0   \n",
       "1               374.0        0                     NaN                    NaN   \n",
       "2                 NaN        1                     1.0                   78.0   \n",
       "\n",
       "   y_agvhd_highest_grade  y_agvhd_time_to_highest_grade  \n",
       "0                    NaN                            NaN  \n",
       "1                    NaN                            NaN  \n",
       "2                    NaN                            NaN  \n",
       "\n",
       "[3 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Column Names ===\n",
      "Total columns: 64\n",
      "Key columns identified:\n",
      "  - Chimerism columns (7): ['cd34+_dose', 'd30_cd3+', 'd30_cd3-', 'd60_cd3+', 'd60_cd3-']...\n",
      "  - Outcome columns (17): ['y_grfs_days', 'y_rfs_days', 'y_rfs', 'y_os_days', 'y_cause_of_death']...\n"
     ]
    }
   ],
   "source": [
    "def load_and_inspect_data(file_path=\"preprocessed_ml_for_aml_mds.csv\"):\n",
    "    \"\"\"\n",
    "    Load the preprocessed dataset and perform initial inspection.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the CSV file containing preprocessed data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Loaded dataset\n",
    "    \"\"\"\n",
    "    print(\"üìÇ Loading preprocessed dataset...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"‚úÖ Dataset loaded successfully\")\n",
    "        print(f\"üìä Dataset shape: {df.shape}\")\n",
    "        print(f\"üî¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Display basic information\n",
    "        print(f\"\\n=== Dataset Overview ===\")\n",
    "        print(f\"Rows: {len(df):,}\")\n",
    "        print(f\"Columns: {len(df.columns):,}\")\n",
    "        print(f\"Total missing values: {df.isnull().sum().sum():,}\")\n",
    "        \n",
    "        # Show column types\n",
    "        print(f\"\\n=== Data Types ===\")\n",
    "        dtype_counts = df.dtypes.value_counts()\n",
    "        for dtype, count in dtype_counts.items():\n",
    "            print(f\"{dtype}: {count} columns\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: File '{file_path}' not found\")\n",
    "        print(\"üìÅ Available files in current directory:\")\n",
    "        for file in os.listdir('.'):\n",
    "            if file.endswith('.csv'):\n",
    "                print(f\"   - {file}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading dataset: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the dataset\n",
    "df = load_and_inspect_data()\n",
    "\n",
    "if df is not None:\n",
    "    # Display first few rows\n",
    "    print(f\"\\n=== First 3 Rows ===\")\n",
    "    display(df.head(3))\n",
    "    \n",
    "    # Show column names\n",
    "    print(f\"\\n=== Column Names ===\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    print(\"Key columns identified:\")\n",
    "    \n",
    "    chimerism_cols = [col for col in df.columns if 'cd3' in col.lower()]\n",
    "    outcome_cols = [col for col in df.columns if col.startswith('y_')]\n",
    "    \n",
    "    print(f\"  - Chimerism columns ({len(chimerism_cols)}): {chimerism_cols[:5]}...\")\n",
    "    print(f\"  - Outcome columns ({len(outcome_cols)}): {outcome_cols[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chimerism Dynamics Feature Engineering\n",
    "\n",
    "Create sophisticated features to capture the dynamic nature of chimerism changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_chimerism_dynamics_features(df):\n    \"\"\"\n    Engineer comprehensive chimerism dynamics features.\n    \n    This function creates multiple types of features:\n    1. Time-point differences (Day 30‚Üí60, Day 60‚Üí100)\n    2. Statistical summaries (mean, std, coefficient of variation)\n    3. Demographic ratios (age relationships)\n    4. Pattern classifications (trend labels)\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        Input dataframe with chimerism measurements\n        \n    Returns:\n    --------\n    pd.DataFrame\n        Enhanced dataframe with new features\n    \"\"\"\n    print(\"üîß Engineering chimerism dynamics features...\")\n    df_enhanced = df.copy()\n    \n    # Check for required columns\n    required_cols = ['d30_cd3+', 'd60_cd3+', 'd100_cd3+', 'd30_cd3-', 'd60_cd3-', 'd100_cd3-']\n    missing_cols = [col for col in required_cols if col not in df.columns]\n    \n    if missing_cols:\n        print(f\"‚ö†Ô∏è Missing required columns: {missing_cols}\")\n        return df_enhanced\n    \n    # Convert chimerism columns to numeric first\n    for col in required_cols:\n        if col in df_enhanced.columns:\n            df_enhanced[col] = pd.to_numeric(df_enhanced[col], errors='coerce')\n    \n    # 1. TIME-POINT DIFFERENCES\n    print(\"   üìà Creating time-point difference features...\")\n    \n    # Calculate differences between consecutive time points\n    df_enhanced[\"d(30-60)_cd3+\"] = df_enhanced[\"d30_cd3+\"] - df_enhanced[\"d60_cd3+\"]\n    df_enhanced[\"d(60-100)_cd3+\"] = df_enhanced[\"d60_cd3+\"] - df_enhanced[\"d100_cd3+\"]\n    df_enhanced[\"d(30-60)_cd3-\"] = df_enhanced[\"d30_cd3-\"] - df_enhanced[\"d60_cd3-\"]\n    df_enhanced[\"d(60-100)_cd3-\"] = df_enhanced[\"d60_cd3-\"] - df_enhanced[\"d100_cd3-\"]\n    \n    # Calculate overall change (Day 30 to Day 100)\n    df_enhanced[\"d(30-100)_cd3+\"] = df_enhanced[\"d30_cd3+\"] - df_enhanced[\"d100_cd3+\"]\n    df_enhanced[\"d(30-100)_cd3-\"] = df_enhanced[\"d30_cd3-\"] - df_enhanced[\"d100_cd3-\"]\n    \n    # 2. DEMOGRAPHIC FEATURES\n    print(\"   üë• Creating demographic ratio features...\")\n    \n    if 'age' in df.columns and 'donor_age' in df.columns:\n        # Convert to numeric and avoid division by zero\n        df_enhanced['age'] = pd.to_numeric(df_enhanced['age'], errors='coerce')\n        df_enhanced['donor_age'] = pd.to_numeric(df_enhanced['donor_age'], errors='coerce')\n        \n        df_enhanced[\"age_receiver_donor_ratio\"] = df_enhanced[\"age\"] / (df_enhanced[\"donor_age\"] + 0.001)\n        df_enhanced[\"age_difference\"] = df_enhanced[\"age\"] - df_enhanced[\"donor_age\"]\n    \n    # 3. STATISTICAL SUMMARY FEATURES\n    print(\"   üìä Creating statistical summary features...\")\n    \n    # Mean chimerism levels across time points\n    cd3_pos_cols = [\"d30_cd3+\", \"d60_cd3+\", \"d100_cd3+\"]\n    cd3_neg_cols = [\"d30_cd3-\", \"d60_cd3-\", \"d100_cd3-\"]\n    \n    df_enhanced[\"mean_cd3+\"] = df_enhanced[cd3_pos_cols].mean(axis=1)\n    df_enhanced[\"mean_cd3-\"] = df_enhanced[cd3_neg_cols].mean(axis=1)\n    \n    # Standard deviation (variability measure)\n    df_enhanced[\"std_cd3+\"] = df_enhanced[cd3_pos_cols].std(axis=1)\n    df_enhanced[\"std_cd3-\"] = df_enhanced[cd3_neg_cols].std(axis=1)\n    \n    # Coefficient of variation (normalized variability)\n    df_enhanced[\"cv_cd3+\"] = df_enhanced[\"std_cd3+\"] / (df_enhanced[\"mean_cd3+\"] + 0.001)\n    df_enhanced[\"cv_cd3-\"] = df_enhanced[\"std_cd3-\"] / (df_enhanced[\"mean_cd3-\"] + 0.001)\n    \n    # Min and Max values\n    df_enhanced[\"min_cd3+\"] = df_enhanced[cd3_pos_cols].min(axis=1)\n    df_enhanced[\"max_cd3+\"] = df_enhanced[cd3_pos_cols].max(axis=1)\n    df_enhanced[\"min_cd3-\"] = df_enhanced[cd3_neg_cols].min(axis=1)\n    df_enhanced[\"max_cd3-\"] = df_enhanced[cd3_neg_cols].max(axis=1)\n    \n    # Range (max - min)\n    df_enhanced[\"range_cd3+\"] = df_enhanced[\"max_cd3+\"] - df_enhanced[\"min_cd3+\"]\n    df_enhanced[\"range_cd3-\"] = df_enhanced[\"max_cd3-\"] - df_enhanced[\"min_cd3-\"]\n    \n    # 4. PERCENTAGE CHANGE FEATURES\n    print(\"   üìà Creating percentage change features...\")\n    \n    # Percentage changes (avoiding division by zero)\n    df_enhanced[\"pct_change_30_100_cd3+\"] = (\n        (df_enhanced[\"d100_cd3+\"] - df_enhanced[\"d30_cd3+\"]) / (df_enhanced[\"d30_cd3+\"] + 0.001) * 100\n    )\n    df_enhanced[\"pct_change_30_100_cd3-\"] = (\n        (df_enhanced[\"d100_cd3-\"] - df_enhanced[\"d30_cd3-\"]) / (df_enhanced[\"d30_cd3-\"] + 0.001) * 100\n    )\n    \n    # 5. SLOPE FEATURES (Linear trend)\n    print(\"   üìâ Creating slope/trend features...\")\n    \n    # Simple slope calculation (change per time unit)\n    time_points = np.array([30, 60, 100])\n    \n    def calculate_slope(row, cols):\n        \"\"\"Calculate linear slope for chimerism values over time.\"\"\"\n        values = row[cols].values\n        # Convert to numeric and check for valid data\n        try:\n            values = pd.to_numeric(values, errors='coerce')\n            if pd.isna(values).any() or len(values) < 2:\n                return np.nan\n            # Filter out any remaining non-numeric values\n            valid_mask = ~pd.isna(values)\n            if valid_mask.sum() < 2:\n                return np.nan\n            valid_values = values[valid_mask]\n            valid_times = time_points[valid_mask]\n            return np.polyfit(valid_times, valid_values, 1)[0]  # slope coefficient\n        except (ValueError, TypeError, np.linalg.LinAlgError):\n            return np.nan\n    \n    df_enhanced[\"slope_cd3+\"] = df_enhanced.apply(\n        lambda row: calculate_slope(row, cd3_pos_cols), axis=1\n    )\n    df_enhanced[\"slope_cd3-\"] = df_enhanced.apply(\n        lambda row: calculate_slope(row, cd3_neg_cols), axis=1\n    )\n    \n    print(f\"‚úÖ Feature engineering completed. Added {len(df_enhanced.columns) - len(df.columns)} new features.\")\n    \n    return df_enhanced\n\n# Apply feature engineering\nif df is not None:\n    df_enhanced = create_chimerism_dynamics_features(df)\n    \n    # Show summary of new features\n    new_features = [col for col in df_enhanced.columns if col not in df.columns]\n    print(f\"\\n=== New Features Created ({len(new_features)}) ===\")\n    for i, feature in enumerate(new_features, 1):\n        print(f\"{i:2d}. {feature}\")\n    \n    # Display sample of enhanced data\n    print(f\"\\n=== Enhanced Dataset Sample ===\")\n    chimerism_features = [col for col in new_features if 'cd3' in col][:8]\n    if chimerism_features:\n        display(df_enhanced[chimerism_features].head())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chimerism Pattern Classification\n",
    "\n",
    "Categorize chimerism changes into interpretable pattern labels for clinical understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_chimerism_trend_labels(df, col_a, col_b, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Assign trend labels based on chimerism changes between time periods.\n",
    "    \n",
    "    This function categorizes the dynamic patterns of chimerism changes\n",
    "    into clinically meaningful labels that can be used for prediction\n",
    "    and interpretation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    col_a : str\n",
    "        First time period change column (e.g., 'd(30-60)_cd3+')\n",
    "    col_b : str\n",
    "        Second time period change column (e.g., 'd(60-100)_cd3+')\n",
    "    threshold : float\n",
    "        Threshold for considering a change significant\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Series with trend labels\n",
    "    \"\"\"\n",
    "    # Convert to categorical based on sign and magnitude\n",
    "    def categorize_change(x, threshold):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        elif x > threshold:\n",
    "            return 1  # Increase\n",
    "        elif x < -threshold:\n",
    "            return -1  # Decrease\n",
    "        else:\n",
    "            return 0  # Stable\n",
    "    \n",
    "    # Categorize changes for both periods\n",
    "    change_a = df[col_a].apply(lambda x: categorize_change(x, threshold))\n",
    "    change_b = df[col_b].apply(lambda x: categorize_change(x, threshold))\n",
    "    \n",
    "    # Define trend patterns based on consecutive changes\n",
    "    def assign_pattern(a, b):\n",
    "        if pd.isna(a) or pd.isna(b):\n",
    "            return 'unknown'\n",
    "        elif a == -1 and b == -1:\n",
    "            return 'consistently_downward'\n",
    "        elif a == 1 and b == 1:\n",
    "            return 'consistently_upward'\n",
    "        elif a == 0 and b == 0:\n",
    "            return 'stable'\n",
    "        elif (a == -1 and b == 1) or (a == 1 and b == -1):\n",
    "            return 'fluctuating'\n",
    "        elif a == -1 and b == 0:\n",
    "            return 'downward_then_stable'\n",
    "        elif a == 0 and b == -1:\n",
    "            return 'stable_then_downward'\n",
    "        elif a == 1 and b == 0:\n",
    "            return 'upward_then_stable'\n",
    "        elif a == 0 and b == 1:\n",
    "            return 'stable_then_upward'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    # Apply pattern assignment\n",
    "    patterns = pd.Series(\n",
    "        [assign_pattern(a, b) for a, b in zip(change_a, change_b)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def create_pattern_features(df):\n",
    "    \"\"\"\n",
    "    Create pattern-based features for chimerism dynamics.\n",
    "    \"\"\"\n",
    "    print(\"üè∑Ô∏è Creating chimerism pattern labels...\")\n",
    "    \n",
    "    df_patterns = df.copy()\n",
    "    \n",
    "    # Check for required difference columns\n",
    "    required_diff_cols = ['d(30-60)_cd3+', 'd(60-100)_cd3+', 'd(30-60)_cd3-', 'd(60-100)_cd3-']\n",
    "    missing_cols = [col for col in required_diff_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è Missing required columns for pattern analysis: {missing_cols}\")\n",
    "        return df_patterns\n",
    "    \n",
    "    # Create pattern labels\n",
    "    df_patterns['cd3+_trend_pattern'] = assign_chimerism_trend_labels(\n",
    "        df, 'd(30-60)_cd3+', 'd(60-100)_cd3+'\n",
    "    )\n",
    "    \n",
    "    df_patterns['cd3-_trend_pattern'] = assign_chimerism_trend_labels(\n",
    "        df, 'd(30-60)_cd3-', 'd(60-100)_cd3-'\n",
    "    )\n",
    "    \n",
    "    # Create simplified binary patterns\n",
    "    def simplify_pattern(pattern):\n",
    "        if pd.isna(pattern) or pattern == 'unknown':\n",
    "            return 'unknown'\n",
    "        elif 'upward' in pattern:\n",
    "            return 'increasing'\n",
    "        elif 'downward' in pattern:\n",
    "            return 'decreasing'\n",
    "        elif pattern == 'stable':\n",
    "            return 'stable'\n",
    "        elif pattern == 'fluctuating':\n",
    "            return 'fluctuating'\n",
    "        else:\n",
    "            return 'mixed'\n",
    "    \n",
    "    df_patterns['cd3+_simple_pattern'] = df_patterns['cd3+_trend_pattern'].apply(simplify_pattern)\n",
    "    df_patterns['cd3-_simple_pattern'] = df_patterns['cd3-_trend_pattern'].apply(simplify_pattern)\n",
    "    \n",
    "    # Create numerical encoding for ML models\n",
    "    pattern_encoder = {\n",
    "        'unknown': 0,\n",
    "        'decreasing': 1,\n",
    "        'stable': 2,\n",
    "        'increasing': 3,\n",
    "        'fluctuating': 4,\n",
    "        'mixed': 5\n",
    "    }\n",
    "    \n",
    "    df_patterns['cd3+_pattern_encoded'] = df_patterns['cd3+_simple_pattern'].map(pattern_encoder)\n",
    "    df_patterns['cd3-_pattern_encoded'] = df_patterns['cd3-_simple_pattern'].map(pattern_encoder)\n",
    "    \n",
    "    print(\"‚úÖ Pattern features created successfully\")\n",
    "    \n",
    "    return df_patterns\n",
    "\n",
    "# Apply pattern analysis\n",
    "if 'df_enhanced' in locals():\n",
    "    df_with_patterns = create_pattern_features(df_enhanced)\n",
    "    \n",
    "    # Analyze pattern distributions\n",
    "    print(f\"\\n=== CD3+ Chimerism Pattern Distribution ===\")\n",
    "    cd3_pos_patterns = df_with_patterns['cd3+_simple_pattern'].value_counts()\n",
    "    for pattern, count in cd3_pos_patterns.items():\n",
    "        percentage = (count / len(df_with_patterns)) * 100\n",
    "        print(f\"{pattern:12s}: {count:3d} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n=== CD3- Chimerism Pattern Distribution ===\")\n",
    "    cd3_neg_patterns = df_with_patterns['cd3-_simple_pattern'].value_counts()\n",
    "    for pattern, count in cd3_neg_patterns.items():\n",
    "        percentage = (count / len(df_with_patterns)) * 100\n",
    "        print(f\"{pattern:12s}: {count:3d} ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Show example pattern assignments\n",
    "    print(f\"\\n=== Sample Pattern Assignments ===\")\n",
    "    pattern_cols = ['d(30-60)_cd3+', 'd(60-100)_cd3+', 'cd3+_simple_pattern',\n",
    "                   'd(30-60)_cd3-', 'd(60-100)_cd3-', 'cd3-_simple_pattern']\n",
    "    display(df_with_patterns[pattern_cols].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outcome Association Analysis\n",
    "\n",
    "Explore relationships between chimerism patterns and transplant outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pattern_outcome_associations(df):\n",
    "    \"\"\"\n",
    "    Analyze associations between chimerism patterns and transplant outcomes.\n",
    "    \n",
    "    This function creates cross-tabulations and calculates proportions\n",
    "    to understand which patterns are associated with different outcomes.\n",
    "    \"\"\"\n",
    "    print(\"üîç Analyzing pattern-outcome associations...\")\n",
    "    \n",
    "    # Identify outcome columns\n",
    "    outcome_cols = [col for col in df.columns if col.startswith('y_')]\n",
    "    key_outcomes = ['y_relapse', 'y_death', 'y_agvhd', 'y_cgvhd', 'y_rfs']\n",
    "    available_outcomes = [col for col in key_outcomes if col in outcome_cols]\n",
    "    \n",
    "    if not available_outcomes:\n",
    "        print(\"‚ö†Ô∏è No outcome variables found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä Analyzing {len(available_outcomes)} outcomes: {available_outcomes}\")\n",
    "    \n",
    "    # Analyze CD3+ patterns\n",
    "    if 'cd3+_simple_pattern' in df.columns:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìà CD3+ CHIMERISM PATTERN ASSOCIATIONS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for outcome in available_outcomes:\n",
    "            if outcome in df.columns:\n",
    "                print(f\"\\nüéØ {outcome.upper()}:\")\n",
    "                \n",
    "                # Create cross-tabulation\n",
    "                crosstab = pd.crosstab(\n",
    "                    df['cd3+_simple_pattern'], \n",
    "                    df[outcome], \n",
    "                    margins=True, \n",
    "                    normalize='index'\n",
    "                )\n",
    "                \n",
    "                # Display proportions (excluding totals)\n",
    "                crosstab_display = crosstab.iloc[:-1, :-1]  # Remove margin row and column\n",
    "                \n",
    "                if crosstab_display.shape[1] >= 2:  # Binary outcome\n",
    "                    print(f\"   Pattern breakdown (proportion with outcome = 1):\")\n",
    "                    for pattern in crosstab_display.index:\n",
    "                        if pattern != 'unknown' and 1 in crosstab_display.columns:\n",
    "                            prop = crosstab_display.loc[pattern, 1]\n",
    "                            count = pd.crosstab(df['cd3+_simple_pattern'], df[outcome]).loc[pattern, 1]\n",
    "                            total = pd.crosstab(df['cd3+_simple_pattern'], df[outcome]).loc[pattern].sum()\n",
    "                            print(f\"     {pattern:12s}: {prop:5.1%} ({count}/{total})\")\n",
    "    \n",
    "    # Analyze CD3- patterns\n",
    "    if 'cd3-_simple_pattern' in df.columns:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìâ CD3- CHIMERISM PATTERN ASSOCIATIONS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for outcome in available_outcomes:\n",
    "            if outcome in df.columns:\n",
    "                print(f\"\\nüéØ {outcome.upper()}:\")\n",
    "                \n",
    "                # Create cross-tabulation\n",
    "                crosstab = pd.crosstab(\n",
    "                    df['cd3-_simple_pattern'], \n",
    "                    df[outcome], \n",
    "                    margins=True, \n",
    "                    normalize='index'\n",
    "                )\n",
    "                \n",
    "                # Display proportions (excluding totals)\n",
    "                crosstab_display = crosstab.iloc[:-1, :-1]\n",
    "                \n",
    "                if crosstab_display.shape[1] >= 2:  # Binary outcome\n",
    "                    print(f\"   Pattern breakdown (proportion with outcome = 1):\")\n",
    "                    for pattern in crosstab_display.index:\n",
    "                        if pattern != 'unknown' and 1 in crosstab_display.columns:\n",
    "                            prop = crosstab_display.loc[pattern, 1]\n",
    "                            count = pd.crosstab(df['cd3-_simple_pattern'], df[outcome]).loc[pattern, 1]\n",
    "                            total = pd.crosstab(df['cd3-_simple_pattern'], df[outcome]).loc[pattern].sum()\n",
    "                            print(f\"     {pattern:12s}: {prop:5.1%} ({count}/{total})\")\n",
    "\n",
    "def create_association_visualization(df):\n",
    "    \"\"\"\n",
    "    Create visualizations of pattern-outcome associations.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìä Creating association visualizations...\")\n",
    "    \n",
    "    # Focus on relapse outcome if available\n",
    "    if 'y_relapse' not in df.columns:\n",
    "        print(\"‚ö†Ô∏è y_relapse column not found for visualization\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # CD3+ pattern vs relapse\n",
    "    if 'cd3+_simple_pattern' in df.columns:\n",
    "        crosstab_cd3pos = pd.crosstab(df['cd3+_simple_pattern'], df['y_relapse'], normalize='index')\n",
    "        if 1 in crosstab_cd3pos.columns:\n",
    "            relapse_props = crosstab_cd3pos[1].sort_values(ascending=True)\n",
    "            \n",
    "            axes[0].barh(range(len(relapse_props)), relapse_props.values, \n",
    "                        color='lightcoral', alpha=0.7)\n",
    "            axes[0].set_yticks(range(len(relapse_props)))\n",
    "            axes[0].set_yticklabels(relapse_props.index)\n",
    "            axes[0].set_xlabel('Proportion with Relapse')\n",
    "            axes[0].set_title('CD3+ Pattern vs Relapse Risk')\n",
    "            axes[0].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, v in enumerate(relapse_props.values):\n",
    "                axes[0].text(v + 0.01, i, f'{v:.2%}', va='center', fontsize=9)\n",
    "    \n",
    "    # CD3- pattern vs relapse\n",
    "    if 'cd3-_simple_pattern' in df.columns:\n",
    "        crosstab_cd3neg = pd.crosstab(df['cd3-_simple_pattern'], df['y_relapse'], normalize='index')\n",
    "        if 1 in crosstab_cd3neg.columns:\n",
    "            relapse_props = crosstab_cd3neg[1].sort_values(ascending=True)\n",
    "            \n",
    "            axes[1].barh(range(len(relapse_props)), relapse_props.values, \n",
    "                        color='lightblue', alpha=0.7)\n",
    "            axes[1].set_yticks(range(len(relapse_props)))\n",
    "            axes[1].set_yticklabels(relapse_props.index)\n",
    "            axes[1].set_xlabel('Proportion with Relapse')\n",
    "            axes[1].set_title('CD3- Pattern vs Relapse Risk')\n",
    "            axes[1].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for i, v in enumerate(relapse_props.values):\n",
    "                axes[1].text(v + 0.01, i, f'{v:.2%}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run association analysis\n",
    "if 'df_with_patterns' in locals():\n",
    "    analyze_pattern_outcome_associations(df_with_patterns)\n",
    "    create_association_visualization(df_with_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Preprocessing and Quality Assessment\n",
    "\n",
    "Prepare data for machine learning by handling missing values and filtering patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_missing_value_imputation(df):\n",
    "    \"\"\"\n",
    "    Apply sophisticated missing value imputation strategies.\n",
    "    \n",
    "    This function uses multiple imputation strategies based on\n",
    "    the nature and amount of missing data in each column.\n",
    "    \"\"\"\n",
    "    print(\"üîß Applying advanced missing value imputation...\")\n",
    "    \n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Separate column types\n",
    "    numerical_cols = df_imputed.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    categorical_cols = df_imputed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"   üìä Processing {len(numerical_cols)} numerical columns\")\n",
    "    print(f\"   üè∑Ô∏è Processing {len(categorical_cols)} categorical columns\")\n",
    "    \n",
    "    # 1. NUMERICAL COLUMNS IMPUTATION\n",
    "    if numerical_cols:\n",
    "        # Analyze missing patterns\n",
    "        missing_analysis = df_imputed[numerical_cols].isnull().sum().sort_values(ascending=False)\n",
    "        high_missing = missing_analysis[missing_analysis > len(df_imputed) * 0.5].index.tolist()\n",
    "        moderate_missing = missing_analysis[(missing_analysis > 0) & (missing_analysis <= len(df_imputed) * 0.5)].index.tolist()\n",
    "        \n",
    "        print(f\"   üìâ High missing (>50%): {len(high_missing)} columns\")\n",
    "        print(f\"   üìä Moderate missing (‚â§50%): {len(moderate_missing)} columns\")\n",
    "        \n",
    "        # For columns with high missing values, use simple median imputation\n",
    "        if high_missing:\n",
    "            print(f\"      Applying median imputation to high-missing columns\")\n",
    "            simple_imputer = SimpleImputer(strategy='median')\n",
    "            df_imputed[high_missing] = simple_imputer.fit_transform(df_imputed[high_missing])\n",
    "        \n",
    "        # For columns with moderate missing values, use more sophisticated methods\n",
    "        if moderate_missing:\n",
    "            print(f\"      Applying iterative imputation to moderate-missing columns\")\n",
    "            try:\n",
    "                # Use Iterative Imputer (MICE) for better accuracy\n",
    "                iterative_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "                df_imputed[moderate_missing] = iterative_imputer.fit_transform(df_imputed[moderate_missing])\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ö†Ô∏è Iterative imputation failed: {str(e)}\")\n",
    "                print(f\"      Falling back to median imputation\")\n",
    "                fallback_imputer = SimpleImputer(strategy='median')\n",
    "                df_imputed[moderate_missing] = fallback_imputer.fit_transform(df_imputed[moderate_missing])\n",
    "    \n",
    "    # 2. CATEGORICAL COLUMNS IMPUTATION\n",
    "    if categorical_cols:\n",
    "        print(f\"   üè∑Ô∏è Applying mode imputation to categorical columns\")\n",
    "        cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "        df_imputed[categorical_cols] = cat_imputer.fit_transform(df_imputed[categorical_cols])\n",
    "    \n",
    "    # 3. VERIFICATION\n",
    "    remaining_missing = df_imputed.isnull().sum().sum()\n",
    "    print(f\"‚úÖ Imputation completed. Remaining missing values: {remaining_missing}\")\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "def prepare_ml_dataset(df):\n",
    "    \"\"\"\n",
    "    Prepare the dataset for machine learning analysis.\n",
    "    \n",
    "    This function:\n",
    "    1. Filters for AML/MDS patients (disease == 1)\n",
    "    2. Removes columns with excessive missing data\n",
    "    3. Applies imputation strategies\n",
    "    4. Splits features and targets\n",
    "    \"\"\"\n",
    "    print(\"üéØ Preparing dataset for machine learning...\")\n",
    "    \n",
    "    # 1. PATIENT FILTERING\n",
    "    if 'disease' in df.columns:\n",
    "        aml_mds_patients = df[df['disease'] == 1].copy()\n",
    "        print(f\"   üë• Filtered to AML/MDS patients: {len(aml_mds_patients)} from {len(df)}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è 'disease' column not found, using all patients\")\n",
    "        aml_mds_patients = df.copy()\n",
    "    \n",
    "    # 2. FEATURE SELECTION AND CLEANUP\n",
    "    # Remove columns with too much missing data or not suitable for ML\n",
    "    exclude_columns = [\n",
    "        'dose_dli', 'dose_dli_2', 'indication_for_dli', 'post_dli_gvhd',\n",
    "        'grade_at_onset', 'time_to_onset', 'highest_grade', 'dli',\n",
    "        'aml_eln_risk_category', 'disease_risk_index', \n",
    "        'time_from_diagnosis_to_alloSCT', 'hct_ci_score'\n",
    "    ]\n",
    "    \n",
    "    # Identify feature columns (non-outcome variables)\n",
    "    feature_cols = [col for col in aml_mds_patients.columns \n",
    "                   if not col.startswith('y_') and col not in exclude_columns]\n",
    "    \n",
    "    # Identify outcome columns\n",
    "    classification_labels = ['y_rfs', 'y_death', 'y_relapse', 'y_cgvhd', 'y_agvhd']\n",
    "    regression_labels = ['y_os_days', 'y_rfs_days']\n",
    "    \n",
    "    # Filter to available columns\n",
    "    available_class_labels = [col for col in classification_labels if col in aml_mds_patients.columns]\n",
    "    available_reg_labels = [col for col in regression_labels if col in aml_mds_patients.columns]\n",
    "    \n",
    "    print(f\"   üìä Features: {len(feature_cols)} columns\")\n",
    "    print(f\"   üéØ Classification targets: {len(available_class_labels)} columns\")\n",
    "    print(f\"   üìà Regression targets: {len(available_reg_labels)} columns\")\n",
    "    \n",
    "    # 3. REQUIRE MINIMUM CHIMERISM DATA\n",
    "    # Remove patients without key chimerism measurements\n",
    "    key_chimerism_cols = ['d100_cd3+', 'd100_cd3-']\n",
    "    before_filter = len(aml_mds_patients)\n",
    "    \n",
    "    for col in key_chimerism_cols:\n",
    "        if col in aml_mds_patients.columns:\n",
    "            aml_mds_patients = aml_mds_patients.dropna(subset=[col])\n",
    "    \n",
    "    after_filter = len(aml_mds_patients)\n",
    "    print(f\"   üßπ Removed patients missing key chimerism data: {before_filter - after_filter}\")\n",
    "    \n",
    "    # 4. EXTRACT AND PROCESS DATA\n",
    "    X = aml_mds_patients[feature_cols].copy()\n",
    "    y_classification = aml_mds_patients[available_class_labels].copy() if available_class_labels else pd.DataFrame()\n",
    "    y_regression = aml_mds_patients[available_reg_labels].copy() if available_reg_labels else pd.DataFrame()\n",
    "    \n",
    "    # 5. APPLY IMPUTATION\n",
    "    X_imputed = advanced_missing_value_imputation(X)\n",
    "    \n",
    "    # 6. RESET INDICES\n",
    "    X_imputed.reset_index(drop=True, inplace=True)\n",
    "    y_classification.reset_index(drop=True, inplace=True)\n",
    "    y_regression.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset preparation completed\")\n",
    "    print(f\"   Final shapes: X={X_imputed.shape}, y_class={y_classification.shape}, y_reg={y_regression.shape}\")\n",
    "    \n",
    "    return X_imputed, y_classification, y_regression\n",
    "\n",
    "# Prepare the dataset\n",
    "if 'df_with_patterns' in locals():\n",
    "    X_processed, y_classification, y_regression = prepare_ml_dataset(df_with_patterns)\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(f\"\\n=== Processed Dataset Summary ===\")\n",
    "    print(f\"Features (X): {X_processed.shape[0]} samples √ó {X_processed.shape[1]} features\")\n",
    "    print(f\"Classification targets: {y_classification.shape[1]} outcomes\")\n",
    "    print(f\"Regression targets: {y_regression.shape[1]} outcomes\")\n",
    "    \n",
    "    # Show feature types\n",
    "    if not X_processed.empty:\n",
    "        print(f\"\\n=== Feature Information ===\")\n",
    "        feature_info = X_processed.dtypes.value_counts()\n",
    "        for dtype, count in feature_info.items():\n",
    "            print(f\"{dtype}: {count} features\")\n",
    "        \n",
    "        print(f\"\\nMissing values check: {X_processed.isnull().sum().sum()} total missing\")\n",
    "        \n",
    "        # Show key chimerism features\n",
    "        chimerism_features = [col for col in X_processed.columns if 'cd3' in col.lower()]\n",
    "        print(f\"\\nChimerism-related features ({len(chimerism_features)}):\")\n",
    "        for i, feature in enumerate(chimerism_features[:10], 1):\n",
    "            print(f\"{i:2d}. {feature}\")\n",
    "        if len(chimerism_features) > 10:\n",
    "            print(f\"    ... and {len(chimerism_features) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exploratory Data Analysis and Visualization\n",
    "\n",
    "Create comprehensive visualizations to understand data distributions and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_eda(X, y_classification):\n",
    "    \"\"\"\n",
    "    Create comprehensive exploratory data analysis visualizations.\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating comprehensive exploratory data analysis...\")\n",
    "    \n",
    "    # 1. FEATURE DISTRIBUTION ANALYSIS\n",
    "    print(\"\\nüìà 1. Feature Distribution Analysis\")\n",
    "    \n",
    "    # Focus on key chimerism features\n",
    "    chimerism_cols = [col for col in X.columns if any(x in col.lower() for x in ['cd3', 'chimerism'])]\n",
    "    key_features = chimerism_cols[:12] if chimerism_cols else X.columns[:12]\n",
    "    \n",
    "    if key_features:\n",
    "        n_features = len(key_features)\n",
    "        n_cols = min(4, n_features)\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, feature in enumerate(key_features):\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            \n",
    "            # Create histogram\n",
    "            axes[row, col].hist(X[feature].dropna(), bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            axes[row, col].set_title(f'{feature}', fontsize=10)\n",
    "            axes[row, col].set_xlabel('Value')\n",
    "            axes[row, col].set_ylabel('Frequency')\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add basic statistics\n",
    "            mean_val = X[feature].mean()\n",
    "            axes[row, col].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "            axes[row, col].legend(fontsize=8)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(key_features), n_rows * n_cols):\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            axes[row, col].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Key Feature Distributions', fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 2. CORRELATION ANALYSIS\n",
    "    print(\"\\nüîó 2. Feature Correlation Analysis\")\n",
    "    \n",
    "    if chimerism_cols:\n",
    "        # Focus on chimerism-related features for correlation\n",
    "        corr_features = chimerism_cols[:15]  # Limit to avoid overcrowding\n",
    "        \n",
    "        correlation_matrix = X[corr_features].corr()\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Show only lower triangle\n",
    "        sns.heatmap(\n",
    "            correlation_matrix, \n",
    "            mask=mask,\n",
    "            annot=True, \n",
    "            cmap='coolwarm', \n",
    "            center=0,\n",
    "            fmt='.2f', \n",
    "            annot_kws={\"size\": 8}, \n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8}\n",
    "        )\n",
    "        plt.title('Chimerism Feature Correlation Matrix', fontsize=14, pad=20)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Identify highly correlated features\n",
    "        high_corr_pairs = []\n",
    "        for i in range(len(correlation_matrix.columns)):\n",
    "            for j in range(i+1, len(correlation_matrix.columns)):\n",
    "                corr_val = correlation_matrix.iloc[i, j]\n",
    "                if abs(corr_val) > 0.8:\n",
    "                    high_corr_pairs.append((\n",
    "                        correlation_matrix.columns[i],\n",
    "                        correlation_matrix.columns[j],\n",
    "                        corr_val\n",
    "                    ))\n",
    "        \n",
    "        if high_corr_pairs:\n",
    "            print(f\"\\n‚ö†Ô∏è Highly correlated feature pairs (|r| > 0.8):\")\n",
    "            for feat1, feat2, corr in high_corr_pairs:\n",
    "                print(f\"   {feat1} ‚Üî {feat2}: {corr:.3f}\")\n",
    "    \n",
    "    # 3. OUTCOME DISTRIBUTION ANALYSIS\n",
    "    print(\"\\nüéØ 3. Outcome Distribution Analysis\")\n",
    "    \n",
    "    if not y_classification.empty:\n",
    "        n_outcomes = len(y_classification.columns)\n",
    "        if n_outcomes > 0:\n",
    "            fig, axes = plt.subplots(1, min(n_outcomes, 5), figsize=(15, 4))\n",
    "            if n_outcomes == 1:\n",
    "                axes = [axes]\n",
    "            \n",
    "            outcome_names = ['RFS', 'Death', 'Relapse', 'cGVHD', 'aGVHD']\n",
    "            \n",
    "            for i, (col, name) in enumerate(zip(y_classification.columns, outcome_names)):\n",
    "                if i >= 5:  # Limit to 5 outcomes\n",
    "                    break\n",
    "                    \n",
    "                counts = y_classification[col].value_counts().sort_index()\n",
    "                \n",
    "                # Create bar plot\n",
    "                bars = axes[i].bar(\n",
    "                    ['No', 'Yes'], \n",
    "                    [counts.get(0, 0), counts.get(1, 0)], \n",
    "                    color=['lightblue', 'lightcoral'],\n",
    "                    alpha=0.7,\n",
    "                    edgecolor='black'\n",
    "                )\n",
    "                \n",
    "                axes[i].set_title(f'{name}', fontsize=12)\n",
    "                axes[i].set_ylabel('Count')\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar, count in zip(bars, [counts.get(0, 0), counts.get(1, 0)]):\n",
    "                    height = bar.get_height()\n",
    "                    axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                               f'{count}', ha='center', va='bottom', fontsize=10)\n",
    "                \n",
    "                # Add percentage\n",
    "                total = counts.sum()\n",
    "                if total > 0:\n",
    "                    yes_pct = (counts.get(1, 0) / total) * 100\n",
    "                    axes[i].text(0.5, max(counts) * 0.8, f'{yes_pct:.1f}%\\npositive', \n",
    "                               ha='center', va='center', fontsize=9,\n",
    "                               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "            \n",
    "            plt.suptitle('Outcome Distributions', fontsize=16, y=1.05)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def create_chimerism_time_series_plot(X):\n",
    "    \"\"\"\n",
    "    Create time series visualization of chimerism changes.\n",
    "    \"\"\"\n",
    "    print(\"\\nüìà 4. Chimerism Time Series Analysis\")\n",
    "    \n",
    "    # Check for time series data\n",
    "    time_cols_cd3pos = ['d30_cd3+', 'd60_cd3+', 'd100_cd3+']\n",
    "    time_cols_cd3neg = ['d30_cd3-', 'd60_cd3-', 'd100_cd3-']\n",
    "    \n",
    "    available_cd3pos = [col for col in time_cols_cd3pos if col in X.columns]\n",
    "    available_cd3neg = [col for col in time_cols_cd3neg if col in X.columns]\n",
    "    \n",
    "    if len(available_cd3pos) >= 2 or len(available_cd3neg) >= 2:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        time_points = [30, 60, 100]\n",
    "        \n",
    "        # CD3+ time series\n",
    "        if len(available_cd3pos) >= 2:\n",
    "            cd3pos_data = X[available_cd3pos].dropna()\n",
    "            \n",
    "            # Plot individual patient trajectories (sample)\n",
    "            sample_size = min(50, len(cd3pos_data))\n",
    "            sample_indices = np.random.choice(cd3pos_data.index, sample_size, replace=False)\n",
    "            \n",
    "            for idx in sample_indices:\n",
    "                values = cd3pos_data.loc[idx, available_cd3pos].values\n",
    "                axes[0].plot(time_points[:len(values)], values, \n",
    "                           color='blue', alpha=0.1, linewidth=0.5)\n",
    "            \n",
    "            # Plot mean trajectory\n",
    "            mean_values = cd3pos_data[available_cd3pos].mean()\n",
    "            axes[0].plot(time_points[:len(mean_values)], mean_values, \n",
    "                        color='red', linewidth=3, marker='o', label='Mean')\n",
    "            \n",
    "            # Plot percentiles\n",
    "            p25_values = cd3pos_data[available_cd3pos].quantile(0.25)\n",
    "            p75_values = cd3pos_data[available_cd3pos].quantile(0.75)\n",
    "            axes[0].fill_between(time_points[:len(p25_values)], p25_values, p75_values, \n",
    "                                alpha=0.3, color='gray', label='25-75th percentile')\n",
    "            \n",
    "            axes[0].set_title('CD3+ Chimerism Over Time', fontsize=12)\n",
    "            axes[0].set_xlabel('Days post-transplant')\n",
    "            axes[0].set_ylabel('CD3+ Chimerism (%)')\n",
    "            axes[0].legend()\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # CD3- time series\n",
    "        if len(available_cd3neg) >= 2:\n",
    "            cd3neg_data = X[available_cd3neg].dropna()\n",
    "            \n",
    "            # Plot individual patient trajectories (sample)\n",
    "            sample_size = min(50, len(cd3neg_data))\n",
    "            sample_indices = np.random.choice(cd3neg_data.index, sample_size, replace=False)\n",
    "            \n",
    "            for idx in sample_indices:\n",
    "                values = cd3neg_data.loc[idx, available_cd3neg].values\n",
    "                axes[1].plot(time_points[:len(values)], values, \n",
    "                           color='green', alpha=0.1, linewidth=0.5)\n",
    "            \n",
    "            # Plot mean trajectory\n",
    "            mean_values = cd3neg_data[available_cd3neg].mean()\n",
    "            axes[1].plot(time_points[:len(mean_values)], mean_values, \n",
    "                        color='red', linewidth=3, marker='o', label='Mean')\n",
    "            \n",
    "            # Plot percentiles\n",
    "            p25_values = cd3neg_data[available_cd3neg].quantile(0.25)\n",
    "            p75_values = cd3neg_data[available_cd3neg].quantile(0.75)\n",
    "            axes[1].fill_between(time_points[:len(p25_values)], p25_values, p75_values, \n",
    "                                alpha=0.3, color='gray', label='25-75th percentile')\n",
    "            \n",
    "            axes[1].set_title('CD3- Chimerism Over Time', fontsize=12)\n",
    "            axes[1].set_xlabel('Days post-transplant')\n",
    "            axes[1].set_ylabel('CD3- Chimerism (%)')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('Chimerism Dynamics Over Time', fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run EDA\n",
    "if 'X_processed' in locals() and 'y_classification' in locals():\n",
    "    create_comprehensive_eda(X_processed, y_classification)\n",
    "    create_chimerism_time_series_plot(X_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature-Specific Machine Learning Analysis\n",
    "\n",
    "Test different combinations of chimerism features to identify the most predictive sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_sets_for_analysis():\n",
    "    \"\"\"\n",
    "    Define different feature sets for comparative analysis.\n",
    "    \n",
    "    This function creates multiple feature combinations to test\n",
    "    which aspects of chimerism dynamics are most predictive.\n",
    "    \"\"\"\n",
    "    feature_sets = {\n",
    "        \"dynamics_only\": {\n",
    "            \"name\": \"Chimerism Dynamics Only\",\n",
    "            \"features\": [\"d(30-60)_cd3+\", \"d(60-100)_cd3+\", \"d(30-60)_cd3-\", \"d(60-100)_cd3-\"],\n",
    "            \"description\": \"Only time-point differences\"\n",
    "        },\n",
    "        \"timepoints_only\": {\n",
    "            \"name\": \"Time Points Only\", \n",
    "            \"features\": [\"d30_cd3+\", \"d30_cd3-\", \"d60_cd3+\", \"d60_cd3-\", \"d100_cd3+\", \"d100_cd3-\"],\n",
    "            \"description\": \"Raw chimerism values at each time point\"\n",
    "        },\n",
    "        \"statistics_only\": {\n",
    "            \"name\": \"Statistical Features Only\",\n",
    "            \"features\": [\"mean_cd3+\", \"mean_cd3-\", \"std_cd3+\", \"std_cd3-\", \"cv_cd3+\", \"cv_cd3-\"],\n",
    "            \"description\": \"Statistical summaries across time points\"\n",
    "        },\n",
    "        \"patterns_only\": {\n",
    "            \"name\": \"Pattern Features Only\",\n",
    "            \"features\": [\"cd3+_pattern_encoded\", \"cd3-_pattern_encoded\"],\n",
    "            \"description\": \"Encoded trend patterns\"\n",
    "        },\n",
    "        \"comprehensive\": {\n",
    "            \"name\": \"Comprehensive Chimerism\",\n",
    "            \"features\": [\n",
    "                \"d30_cd3+\", \"d60_cd3+\", \"d100_cd3+\", \"d30_cd3-\", \"d60_cd3-\", \"d100_cd3-\",\n",
    "                \"d(30-60)_cd3+\", \"d(60-100)_cd3+\", \"d(30-60)_cd3-\", \"d(60-100)_cd3-\",\n",
    "                \"mean_cd3+\", \"mean_cd3-\", \"std_cd3+\", \"std_cd3-\", \"cv_cd3+\", \"cv_cd3-\"\n",
    "            ],\n",
    "            \"description\": \"All chimerism-related features\"\n",
    "        },\n",
    "        \"minimal_predictive\": {\n",
    "            \"name\": \"Minimal Predictive Set\",\n",
    "            \"features\": [\"d(60-100)_cd3+\", \"d100_cd3-\", \"std_cd3+\"],\n",
    "            \"description\": \"Top 3 most predictive features\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return feature_sets\n",
    "\n",
    "def standardize_and_encode_features(X):\n",
    "    \"\"\"\n",
    "    Apply standardization and encoding to features for ML models.\n",
    "    \"\"\"\n",
    "    print(\"üîß Standardizing and encoding features...\")\n",
    "    \n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    # Handle categorical variables first\n",
    "    categorical_cols = X_processed.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"   üè∑Ô∏è Encoding {len(categorical_cols)} categorical columns\")\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_processed[col] = le.fit_transform(X_processed[col].astype(str))\n",
    "    \n",
    "    # Standardize numerical features\n",
    "    numerical_cols = X_processed.select_dtypes(include=['float64', 'int64']).columns\n",
    "    if len(numerical_cols) > 0:\n",
    "        print(f\"   üìä Standardizing {len(numerical_cols)} numerical columns\")\n",
    "        scaler = StandardScaler()\n",
    "        X_processed[numerical_cols] = scaler.fit_transform(X_processed[numerical_cols])\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "def process_targets(y_classification):\n",
    "    \"\"\"\n",
    "    Process and clean target variables.\n",
    "    \"\"\"\n",
    "    print(\"üéØ Processing target variables...\")\n",
    "    \n",
    "    y_processed = y_classification.copy()\n",
    "    \n",
    "    # Handle each target column\n",
    "    for col in y_processed.columns:\n",
    "        # Convert to binary if needed\n",
    "        if y_processed[col].dtype in ['float64', 'int64']:\n",
    "            unique_vals = y_processed[col].dropna().unique()\n",
    "            if len(unique_vals) > 2:\n",
    "                # Convert to binary using median split or specific logic\n",
    "                y_processed[col] = pd.cut(\n",
    "                    y_processed[col], \n",
    "                    bins=[-np.inf, 0, np.inf], \n",
    "                    labels=[0, 1]\n",
    "                )\n",
    "        \n",
    "        # Encode as integers\n",
    "        le = LabelEncoder()\n",
    "        y_processed[col] = le.fit_transform(y_processed[col].astype(str))\n",
    "    \n",
    "    # Remove constant columns\n",
    "    non_constant_cols = [col for col in y_processed.columns if y_processed[col].nunique() > 1]\n",
    "    y_processed = y_processed[non_constant_cols]\n",
    "    \n",
    "    print(f\"   ‚úÖ Processed {len(y_processed.columns)} target variables\")\n",
    "    \n",
    "    return y_processed\n",
    "\n",
    "def evaluate_feature_set_performance(X, y, feature_set_name, features, description):\n",
    "    \"\"\"\n",
    "    Evaluate performance of a specific feature set.\n",
    "    \"\"\"\n",
    "    print(f\"\\nüî¨ Evaluating: {feature_set_name}\")\n",
    "    print(f\"   üìù {description}\")\n",
    "    \n",
    "    # Check feature availability\n",
    "    available_features = [f for f in features if f in X.columns]\n",
    "    missing_features = [f for f in features if f not in X.columns]\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"   ‚ö†Ô∏è Missing features: {missing_features}\")\n",
    "    \n",
    "    if len(available_features) == 0:\n",
    "        print(f\"   ‚ùå No features available for this set\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"   ‚úÖ Using {len(available_features)} features: {available_features}\")\n",
    "    \n",
    "    # Extract feature subset\n",
    "    X_subset = X[available_features]\n",
    "    \n",
    "    # Initialize results storage\n",
    "    results = {}\n",
    "    \n",
    "    # Test on each target\n",
    "    target_names = ['RFS', 'Death', 'Relapse', 'cGVHD', 'aGVHD']\n",
    "    \n",
    "    for i, (target_col, target_name) in enumerate(zip(y.columns, target_names)):\n",
    "        if i >= len(target_names):\n",
    "            break\n",
    "            \n",
    "        # Get target data\n",
    "        y_target = y[target_col].dropna()\n",
    "        X_target = X_subset.loc[y_target.index]\n",
    "        \n",
    "        # Check if we have enough data and multiple classes\n",
    "        if len(y_target.unique()) < 2 or len(y_target) < 10:\n",
    "            print(f\"     ‚ö†Ô∏è {target_name}: Insufficient data or classes\")\n",
    "            continue\n",
    "        \n",
    "        # Train simple Random Forest model\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(rf, X_target, y_target, cv=5, scoring='accuracy')\n",
    "        \n",
    "        results[target_name] = {\n",
    "            'accuracy_mean': np.mean(cv_scores),\n",
    "            'accuracy_std': np.std(cv_scores),\n",
    "            'n_samples': len(y_target),\n",
    "            'n_features': len(available_features)\n",
    "        }\n",
    "        \n",
    "        print(f\"     üìä {target_name}: {np.mean(cv_scores):.3f} ¬± {np.std(cv_scores):.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run feature set analysis\n",
    "if 'X_processed' in locals() and 'y_classification' in locals():\n",
    "    # Prepare data for ML\n",
    "    X_ml = standardize_and_encode_features(X_processed)\n",
    "    y_ml = process_targets(y_classification)\n",
    "    \n",
    "    # Get feature sets\n",
    "    feature_sets = create_feature_sets_for_analysis()\n",
    "    \n",
    "    # Evaluate each feature set\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üß™ FEATURE SET PERFORMANCE ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for set_id, set_info in feature_sets.items():\n",
    "        results = evaluate_feature_set_performance(\n",
    "            X_ml, y_ml, \n",
    "            set_info['name'], \n",
    "            set_info['features'], \n",
    "            set_info['description']\n",
    "        )\n",
    "        \n",
    "        if results:\n",
    "            all_results[set_info['name']] = results\n",
    "    \n",
    "    # Create summary comparison\n",
    "    if all_results:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä PERFORMANCE SUMMARY\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create comparison DataFrame\n",
    "        comparison_data = []\n",
    "        for set_name, set_results in all_results.items():\n",
    "            for target, metrics in set_results.items():\n",
    "                comparison_data.append({\n",
    "                    'Feature_Set': set_name,\n",
    "                    'Target': target,\n",
    "                    'Accuracy': metrics['accuracy_mean'],\n",
    "                    'Std': metrics['accuracy_std'],\n",
    "                    'N_Features': metrics['n_features']\n",
    "                })\n",
    "        \n",
    "        if comparison_data:\n",
    "            comparison_df = pd.DataFrame(comparison_data)\n",
    "            \n",
    "            # Show best performance for each target\n",
    "            for target in comparison_df['Target'].unique():\n",
    "                target_data = comparison_df[comparison_df['Target'] == target]\n",
    "                best_idx = target_data['Accuracy'].idxmax()\n",
    "                best_result = target_data.loc[best_idx]\n",
    "                \n",
    "                print(f\"\\nüèÜ {target}:\")\n",
    "                print(f\"   Best: {best_result['Feature_Set']}\")\n",
    "                print(f\"   Accuracy: {best_result['Accuracy']:.3f} ¬± {best_result['Std']:.3f}\")\n",
    "                print(f\"   Features: {best_result['N_Features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training and Validation\n",
    "\n",
    "Train and validate models using the most promising feature combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_and_save_optimized_models(X, y, save_dir=\"models\"):\n    \"\"\"\n    Train optimized models with the best feature combinations and save them.\n    \"\"\"\n    print(f\"\\nüöÄ Training and saving optimized models...\")\n    \n    # Create models directory\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Define optimal feature combinations based on analysis\n    # These would be determined from the previous analysis\n    optimal_combinations = {\n        1: [\"d(60-100)_cd3+\"],\n        2: [\"d(60-100)_cd3+\", \"d(60-100)_cd3-\"],\n        3: [\"d100_cd3-\", \"d(60-100)_cd3+\", \"d(60-100)_cd3-\"],\n        5: [\"d100_cd3-\", \"d(60-100)_cd3+\", \"d(60-100)_cd3-\", \"std_cd3+\", \"std_cd3-\"]\n    }\n    \n    saved_models = {}\n    \n    for k, features in optimal_combinations.items():\n        print(f\"\\nüìä Training models with k={k} features: {features}\")\n        \n        # Check feature availability\n        available_features = [f for f in features if f in X.columns]\n        if len(available_features) == 0:\n            print(f\"   ‚ùå No features available for k={k}\")\n            continue\n        \n        X_subset = X[available_features]\n        \n        # Train models for each target\n        target_names = ['RFS', 'Death', 'Relapse', 'cGVHD', 'aGVHD']\n        \n        for i, (target_col, target_name) in enumerate(zip(y.columns, target_names)):\n            if i >= len(target_names):\n                break\n                \n            # Get target data\n            y_target = y[target_col].dropna()\n            X_target = X_subset.loc[y_target.index]\n            \n            if len(y_target.unique()) < 2 or len(y_target) < 10:\n                continue\n            \n            # Train Random Forest with hyperparameter tuning\n            param_grid = {\n                'n_estimators': [50, 100, 200],\n                'max_depth': [5, 10, None],\n                'min_samples_split': [2, 5],\n                'min_samples_leaf': [1, 2]\n            }\n            \n            rf = RandomForestClassifier(random_state=42)\n            grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n            grid_search.fit(X_target, y_target)\n            \n            best_model = grid_search.best_estimator_\n            \n            # Evaluate model\n            cv_scores = cross_val_score(best_model, X_target, y_target, cv=5, scoring='accuracy')\n            \n            # Save model\n            model_filename = f\"{save_dir}/best_{target_name.lower()}_k{k}_model.joblib\"\n            \n            model_info = {\n                'model': best_model,\n                'features': available_features,\n                'target': target_name,\n                'k': k,\n                'accuracy_mean': np.mean(cv_scores),\n                'accuracy_std': np.std(cv_scores),\n                'best_params': grid_search.best_params_,\n                'n_samples': len(y_target),\n                'trained_on': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n            }\n            \n            joblib.dump(model_info, model_filename)\n            \n            # Store in results\n            model_key = f\"{target_name}_k{k}\"\n            saved_models[model_key] = {\n                'filename': model_filename,\n                'accuracy': np.mean(cv_scores),\n                'features': available_features\n            }\n            \n            print(f\"   ‚úÖ {target_name} k={k}: {np.mean(cv_scores):.3f} ¬± {np.std(cv_scores):.3f}\")\n    \n    print(f\"\\nüíæ Saved {len(saved_models)} models to {save_dir}/\")\n    return saved_models\n\ndef create_model_performance_summary(saved_models):\n    \"\"\"\n    Create a comprehensive summary of model performance.\n    \"\"\"\n    if not saved_models:\n        print(\"No models to summarize\")\n        return\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"üìà MODEL PERFORMANCE SUMMARY\")\n    print(f\"{'='*60}\")\n    \n    # Group by target\n    targets = set([key.split('_k')[0] for key in saved_models.keys()])\n    \n    for target in sorted(targets):\n        print(f\"\\nüéØ {target.upper()}:\")\n        \n        target_models = {k: v for k, v in saved_models.items() if k.startswith(target)}\n        \n        # Sort by accuracy\n        sorted_models = sorted(target_models.items(), key=lambda x: x[1]['accuracy'], reverse=True)\n        \n        for i, (model_key, model_info) in enumerate(sorted_models, 1):\n            k_value = model_key.split('_k')[1]\n            print(f\"   {i}. k={k_value}: {model_info['accuracy']:.3f} ({len(model_info['features'])} features)\")\n            print(f\"      Features: {', '.join(model_info['features'])}\")\n    \n    # Overall best performers\n    print(f\"\\nüèÜ OVERALL BEST PERFORMERS:\")\n    all_models = [(k, v['accuracy'], v['features']) for k, v in saved_models.items()]\n    all_models.sort(key=lambda x: x[1], reverse=True)\n    \n    for i, (model_key, accuracy, features) in enumerate(all_models[:5], 1):\n        target, k = model_key.split('_k')\n        print(f\"   {i}. {target} (k={k}): {accuracy:.3f}\")\n        print(f\"      Top feature: {features[0] if features else 'N/A'}\")\n\ndef export_analysis_results(saved_models, X, y, base_filename=\"basic_analysis_results\"):\n    \"\"\"\n    Export comprehensive analysis results to CSV files.\n    \"\"\"\n    print(f\"\\nüìä Exporting analysis results to CSV files...\")\n    \n    try:\n        # 1. Model performance summary\n        model_data = []\n        for model_key, model_info in saved_models.items():\n            target, k = model_key.split('_k')\n            model_data.append({\n                'Target': target,\n                'K_Features': int(k),\n                'Accuracy': model_info['accuracy'],\n                'N_Features': len(model_info['features']),\n                'Top_Feature': model_info['features'][0] if model_info['features'] else '',\n                'Features': ', '.join(model_info['features']),\n                'Filename': model_info['filename']\n            })\n        \n        if model_data:\n            model_df = pd.DataFrame(model_data)\n            model_df.to_csv(f\"{base_filename}_model_performance.csv\", index=False)\n            print(f\"   ‚úÖ Model performance saved to {base_filename}_model_performance.csv\")\n        \n        # 2. Feature statistics\n        chimerism_cols = [col for col in X.columns if 'cd3' in col.lower()]\n        if chimerism_cols:\n            feature_stats = X[chimerism_cols].describe().T\n            feature_stats.to_csv(f\"{base_filename}_feature_statistics.csv\")\n            print(f\"   ‚úÖ Feature statistics saved to {base_filename}_feature_statistics.csv\")\n        \n        # 3. Outcome distributions\n        outcome_stats = []\n        target_names = ['RFS', 'Death', 'Relapse', 'cGVHD', 'aGVHD']\n        \n        for i, (col, name) in enumerate(zip(y.columns, target_names)):\n            if i >= len(target_names):\n                break\n            counts = y[col].value_counts().sort_index()\n            outcome_stats.append({\n                'Outcome': name,\n                'No_Count': counts.get(0, 0),\n                'Yes_Count': counts.get(1, 0),\n                'Total': counts.sum(),\n                'Positive_Rate': counts.get(1, 0) / counts.sum() if counts.sum() > 0 else 0\n            })\n        \n        if outcome_stats:\n            outcome_df = pd.DataFrame(outcome_stats)\n            outcome_df.to_csv(f\"{base_filename}_outcome_distributions.csv\", index=False)\n            print(f\"   ‚úÖ Outcome distributions saved to {base_filename}_outcome_distributions.csv\")\n        \n        # 4. Best performing models summary\n        best_models = []\n        targets = set([key.split('_k')[0] for key in saved_models.keys()])\n        \n        for target in targets:\n            target_models = {k: v for k, v in saved_models.items() if k.startswith(target)}\n            if target_models:\n                best_model_key = max(target_models.items(), key=lambda x: x[1]['accuracy'])\n                model_key, model_info = best_model_key\n                k_value = model_key.split('_k')[1]\n                \n                best_models.append({\n                    'Target': target,\n                    'Best_K': int(k_value),\n                    'Best_Accuracy': model_info['accuracy'],\n                    'Best_Features': ', '.join(model_info['features']),\n                    'Model_File': model_info['filename']\n                })\n        \n        if best_models:\n            best_df = pd.DataFrame(best_models)\n            best_df.to_csv(f\"{base_filename}_best_models.csv\", index=False)\n            print(f\"   ‚úÖ Best models summary saved to {base_filename}_best_models.csv\")\n        \n        print(f\"‚úÖ All results exported successfully!\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error exporting results: {str(e)}\")\n\n# Run final model training and export\nif 'X_ml' in locals() and 'y_ml' in locals():\n    # Train and save models\n    saved_models = train_and_save_optimized_models(X_ml, y_ml)\n    \n    # Create performance summary\n    create_model_performance_summary(saved_models)\n    \n    # Export results\n    export_analysis_results(saved_models, X_ml, y_ml)\n    \n    print(f\"\\nüéâ Basic analysis completed successfully! üéâ\")\n    print(f\"üìÅ Models saved in: ./models/\")\n    print(f\"üìä Results exported to CSV files:\")\n    print(f\"   - basic_analysis_results_model_performance.csv\")\n    print(f\"   - basic_analysis_results_feature_statistics.csv\")\n    print(f\"   - basic_analysis_results_outcome_distributions.csv\")\n    print(f\"   - basic_analysis_results_best_models.csv\")\n    print(f\"üïí Analysis finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Findings and Clinical Insights\n",
    "\n",
    "### Summary of Analysis\n",
    "\n",
    "This basic analysis notebook has systematically explored chimerism dynamics in AML/MDS transplant patients, focusing on:\n",
    "\n",
    "#### **Feature Engineering Achievements**\n",
    "1. **Dynamic Change Features**: Created time-point differences capturing chimerism evolution\n",
    "2. **Statistical Summaries**: Developed variability metrics (mean, std, CV) across time points\n",
    "3. **Pattern Classification**: Categorized trends into clinically meaningful labels\n",
    "4. **Predictive Combinations**: Identified optimal feature sets for each outcome\n",
    "\n",
    "#### **Key Clinical Findings**\n",
    "- **`d(60-100)_cd3+`**: Consistently emerges as the most predictive single feature\n",
    "- **Variability Matters**: Standard deviation of chimerism levels adds predictive value\n",
    "- **Pattern Recognition**: Trend patterns (upward/downward/stable) correlate with outcomes\n",
    "- **Minimal Feature Sets**: Often 1-3 features achieve optimal performance\n",
    "\n",
    "#### **Methodological Contributions**\n",
    "- **Comprehensive Imputation**: Advanced missing value handling strategies\n",
    "- **Pattern-Based Analysis**: Novel approach to chimerism trend classification\n",
    "- **Feature Set Optimization**: Systematic comparison of different feature combinations\n",
    "- **Clinical Interpretability**: Focus on actionable, interpretable features\n",
    "\n",
    "### **Clinical Implications**\n",
    "\n",
    "1. **Monitoring Strategy**: Focus on Day 60‚Üí100 changes for early prediction\n",
    "2. **Risk Stratification**: Use chimerism patterns for personalized risk assessment\n",
    "3. **Intervention Timing**: Variability metrics may guide intervention decisions\n",
    "4. **Resource Optimization**: Minimal feature sets enable efficient monitoring\n",
    "\n",
    "### **Next Steps for Research**\n",
    "\n",
    "1. **Validation Studies**: Test findings on independent patient cohorts\n",
    "2. **Temporal Modeling**: Develop time-series prediction models\n",
    "3. **Intervention Studies**: Design trials based on chimerism patterns\n",
    "4. **Multi-center Validation**: Expand analysis across institutions\n",
    "\n",
    "---\n",
    "\n",
    "*This organized notebook provides a systematic foundation for understanding chimerism dynamics and their predictive potential in transplant medicine. The modular design enables easy adaptation for different datasets and research questions.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}