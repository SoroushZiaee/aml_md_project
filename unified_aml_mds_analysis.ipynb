{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified AML/MDS Transplant Outcome Analysis\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "This unified notebook combines the best features from both analysis approaches to create a comprehensive machine learning pipeline for predicting transplant outcomes in AML/MDS patients. It integrates:\n",
    "\n",
    "- **Advanced Feature Engineering**: Dynamic chimerism patterns and statistical features\n",
    "- **Multi-Target Prediction**: Simultaneous analysis of 5 transplant outcomes\n",
    "- **Standard ML Methods**: Random Forest, SVM, Naive Bayes with optimization\n",
    "- **Advanced ML Techniques**: Clustering (K-Means, FCM), Fuzzy SVM, Genetic Algorithms\n",
    "- **Production-Ready Framework**: Model saving, comprehensive exports, and visualization\n",
    "\n",
    "### üî¨ Research Questions\n",
    "\n",
    "**Tier 1 (Primary):**\n",
    "- Can dynamic CD3+ chimerism changes at Day 30, 60, and 100 predict disease relapse?\n",
    "- Which chimerism patterns (upward, downward, fluctuating) best predict outcomes?\n",
    "\n",
    "**Tier 2:**\n",
    "- Can chimerism dynamics predict other outcomes (OS, GVHD, GRFS)?\n",
    "- What is the optimal feature set size for each outcome?\n",
    "\n",
    "**Tier 3:**\n",
    "- Do advanced ML methods improve prediction accuracy?\n",
    "- Which biomarker combinations provide the best predictive power?\n",
    "\n",
    "### üìä Target Variables\n",
    "- **aGVHD**: Acute graft-versus-host disease\n",
    "- **cGVHD**: Chronic graft-versus-host disease\n",
    "- **Relapse**: Disease relapse\n",
    "- **Death**: Overall survival\n",
    "- **RFS**: Relapse-free survival\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Imports\n",
    "\n",
    "Import all necessary libraries for data processing, machine learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Core libraries imported successfully\n",
      "‚úÖ UMAP available for dimensionality reduction\n",
      "‚úÖ Fuzzy clustering available\n",
      "‚úÖ Genetic algorithms available\n",
      "üìÖ Analysis started at: 2025-07-18 20:48:15\n"
     ]
    }
   ],
   "source": [
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "# Enable experimental features for IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Advanced clustering and dimensionality reduction\n",
    "try:\n",
    "    import umap.umap_ as umap\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è UMAP not available. Install with: pip install umap-learn\")\n",
    "    UMAP_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import skfuzzy as fuzz\n",
    "    from skfuzzy import cmeans\n",
    "    FUZZY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è scikit-fuzzy not available. Install with: pip install scikit-fuzzy\")\n",
    "    FUZZY_AVAILABLE = False\n",
    "\n",
    "# Genetic algorithm\n",
    "try:\n",
    "    from deap import base, creator, tools, algorithms\n",
    "    import random\n",
    "    GA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è DEAP not available. Install with: pip install deap\")\n",
    "    GA_AVAILABLE = False\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\", force=True)\n",
    "np.random.seed(42)\n",
    "if GA_AVAILABLE:\n",
    "    random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Core libraries imported successfully\")\n",
    "if UMAP_AVAILABLE:\n",
    "    print(\"‚úÖ UMAP available for dimensionality reduction\")\n",
    "if FUZZY_AVAILABLE:\n",
    "    print(\"‚úÖ Fuzzy clustering available\")\n",
    "if GA_AVAILABLE:\n",
    "    print(\"‚úÖ Genetic algorithms available\")\n",
    "\n",
    "print(f\"üìÖ Analysis started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Excel Column Mapping\n",
    "\n",
    "Load the dataset with proper Excel column mapping for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset loaded successfully. Shape: (258, 64)\n",
      "\n",
      "=== Dataset Overview ===\n",
      "Total rows: 258\n",
      "Total columns: 64\n",
      "Total missing values: 3,634\n",
      "Memory usage: 0.22 MB\n",
      "\n",
      "Data types:\n",
      "float64    31\n",
      "int64      24\n",
      "object      9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Key columns identified:\n",
      "  - Chimerism columns (7): ['cd34+_dose', 'd30_cd3+', 'd30_cd3-', 'd60_cd3+', 'd60_cd3-']...\n",
      "  - Outcome columns (17): ['y_grfs_days', 'y_rfs_days', 'y_rfs', 'y_os_days', 'y_cause_of_death']...\n",
      "\n",
      "=== First 3 Rows ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>disease</th>\n",
       "      <th>disease_risk_index</th>\n",
       "      <th>hct_ci_score</th>\n",
       "      <th>time_from_diagnosis_to_alloSCT</th>\n",
       "      <th>aml_eln_risk_category</th>\n",
       "      <th>disease_state_at_transplant</th>\n",
       "      <th>mrd_status_prior_to_transplant</th>\n",
       "      <th>donor_type</th>\n",
       "      <th>cd34+_dose</th>\n",
       "      <th>...</th>\n",
       "      <th>y_relapse</th>\n",
       "      <th>y_cgvhd</th>\n",
       "      <th>y_time_to_onset</th>\n",
       "      <th>y_cgvhd_nih</th>\n",
       "      <th>y_time_to_onset_nih</th>\n",
       "      <th>y_agvhd</th>\n",
       "      <th>y_agvhd_grade_at_onset</th>\n",
       "      <th>y_agvhd_time_to_onset</th>\n",
       "      <th>y_agvhd_highest_grade</th>\n",
       "      <th>y_agvhd_time_to_highest_grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>253.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>5.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>145.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.66</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  disease  disease_risk_index hct_ci_score  \\\n",
       "0   61        2                   3            2   \n",
       "1   53        5                   2            0   \n",
       "2   63        1                   2            0   \n",
       "\n",
       "   time_from_diagnosis_to_alloSCT  aml_eln_risk_category  \\\n",
       "0                           253.0                    NaN   \n",
       "1                           218.0                    NaN   \n",
       "2                           162.0                    3.0   \n",
       "\n",
       "   disease_state_at_transplant  mrd_status_prior_to_transplant  donor_type  \\\n",
       "0                          NaN                             NaN           2   \n",
       "1                          NaN                             NaN           2   \n",
       "2                          1.0                             1.0           1   \n",
       "\n",
       "   cd34+_dose  ...  y_relapse  y_cgvhd y_time_to_onset  y_cgvhd_nih  \\\n",
       "0        5.40  ...          0        0             NaN          NaN   \n",
       "1        5.04  ...          0        1           145.0          1.0   \n",
       "2        4.66  ...          1        0             NaN          NaN   \n",
       "\n",
       "  y_time_to_onset_nih  y_agvhd  y_agvhd_grade_at_onset  y_agvhd_time_to_onset  \\\n",
       "0                 NaN        1                     1.0                  159.0   \n",
       "1               374.0        0                     NaN                    NaN   \n",
       "2                 NaN        1                     1.0                   78.0   \n",
       "\n",
       "   y_agvhd_highest_grade  y_agvhd_time_to_highest_grade  \n",
       "0                    NaN                            NaN  \n",
       "1                    NaN                            NaN  \n",
       "2                    NaN                            NaN  \n",
       "\n",
       "[3 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_data(file_path=\"main_dataset.xlsx\", sheet_name=\"Sheet1\", skip_rows=2):\n",
    "    \"\"\"\n",
    "    Load dataset from Excel file with proper handling of headers and empty rows.\n",
    "    \n",
    "    For CSV files, use pd.read_csv() instead.\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading dataset...\")\n",
    "    \n",
    "    # Try loading as Excel first, then CSV if that fails\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        else:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=skip_rows)\n",
    "    except FileNotFoundError:\n",
    "        # Try loading the preprocessed CSV file\n",
    "        logging.info(\"Excel file not found, trying preprocessed CSV...\")\n",
    "        df = pd.read_csv(\"preprocessed_ml_for_aml_mds.csv\")\n",
    "    \n",
    "    # Remove completely empty rows\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    \n",
    "    logging.info(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def map_excel_columns(df):\n",
    "    \"\"\"\n",
    "    Create a mapping from Excel column letters (A, B, C...) to actual column names.\n",
    "    \"\"\"\n",
    "    column_mapping = {}\n",
    "    for idx, col_name in enumerate(df.columns):\n",
    "        col_letter = \"\"\n",
    "        col_number = idx + 1\n",
    "        while col_number > 0:\n",
    "            col_number, remainder = divmod(col_number - 1, 26)\n",
    "            col_letter = chr(65 + remainder) + col_letter\n",
    "        column_mapping[col_letter] = col_name\n",
    "    return column_mapping\n",
    "\n",
    "# Load the dataset\n",
    "df = load_data(file_path=\"preprocessed_ml_for_aml_mds.csv\", sheet_name=\"Sheet1\", skip_rows=2)\n",
    "excel_column_mapping = map_excel_columns(df)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\n=== Dataset Overview ===\")\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns):,}\")\n",
    "print(f\"Total missing values: {df.isnull().sum().sum():,}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Show column types\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Identify key columns\n",
    "chimerism_cols = [col for col in df.columns if 'cd3' in col.lower()]\n",
    "outcome_cols = [col for col in df.columns if col.startswith('y_')]\n",
    "\n",
    "print(f\"\\nKey columns identified:\")\n",
    "print(f\"  - Chimerism columns ({len(chimerism_cols)}): {chimerism_cols[:5]}...\")\n",
    "print(f\"  - Outcome columns ({len(outcome_cols)}): {outcome_cols[:5]}...\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\n=== First 3 Rows ===\")\n",
    "display(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Feature Engineering\n",
    "\n",
    "Create sophisticated chimerism dynamics features including patterns and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Engineering chimerism dynamics features...\n",
      "‚úÖ All required chimerism columns found!\n",
      "   Converting d30_cd3+: float64, missing: 23\n",
      "   Converting d60_cd3+: float64, missing: 33\n",
      "   Converting d100_cd3+: float64, missing: 38\n",
      "   Converting d30_cd3-: float64, missing: 23\n",
      "   Converting d60_cd3-: float64, missing: 33\n",
      "   Converting d100_cd3-: float64, missing: 38\n",
      "   üìà Creating time-point difference features...\n",
      "   üë• Creating demographic ratio features...\n",
      "   üìä Creating statistical summary features...\n",
      "   üìà Creating percentage change features...\n",
      "   üìâ Creating slope/trend features...\n",
      "‚úÖ Feature engineering completed. Added 24 new features.\n",
      "\n",
      "=== New Features Created (24) ===\n",
      " 1. d(30-60)_cd3+\n",
      " 2. d(60-100)_cd3+\n",
      " 3. d(30-60)_cd3-\n",
      " 4. d(60-100)_cd3-\n",
      " 5. d(30-100)_cd3+\n",
      " 6. d(30-100)_cd3-\n",
      " 7. age_receiver_donor_ratio\n",
      " 8. age_difference\n",
      " 9. mean_cd3+\n",
      "10. std_cd3+\n",
      "11. cv_cd3+\n",
      "12. min_cd3+\n",
      "13. max_cd3+\n",
      "14. range_cd3+\n",
      "15. mean_cd3-\n",
      "16. std_cd3-\n",
      "17. cv_cd3-\n",
      "18. min_cd3-\n",
      "19. max_cd3-\n",
      "20. range_cd3-\n",
      "... and 4 more\n"
     ]
    }
   ],
   "source": [
    "def create_chimerism_dynamics_features(df):\n",
    "    \"\"\"\n",
    "    Engineer comprehensive chimerism dynamics features.\n",
    "    \n",
    "    This function creates multiple types of features:\n",
    "    1. Time-point differences (Day 30‚Üí60, Day 60‚Üí100)\n",
    "    2. Statistical summaries (mean, std, coefficient of variation)\n",
    "    3. Demographic ratios (age relationships)\n",
    "    4. Slope/trend calculations\n",
    "    5. Percentage changes\n",
    "    \"\"\"\n",
    "    print(\"üîß Engineering chimerism dynamics features...\")\n",
    "    df_enhanced = df.copy()\n",
    "    \n",
    "    # Check for required columns\n",
    "    required_cols = ['d30_cd3+', 'd60_cd3+', 'd100_cd3+', 'd30_cd3-', 'd60_cd3-', 'd100_cd3-']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è Missing required columns: {missing_cols}\")\n",
    "        print(f\"Available columns that contain 'cd3': {[col for col in df.columns if 'cd3' in col.lower()]}\")\n",
    "        \n",
    "        # Try alternative column names\n",
    "        alt_mappings = {\n",
    "            'd30_cd3+': ['D30_CD3+', 'd30_cd3_pos', 'day30_cd3+', 'cd3+_d30'],\n",
    "            'd60_cd3+': ['D60_CD3+', 'd60_cd3_pos', 'day60_cd3+', 'cd3+_d60'],\n",
    "            'd100_cd3+': ['D100_CD3+', 'd100_cd3_pos', 'day100_cd3+', 'cd3+_d100'],\n",
    "            'd30_cd3-': ['D30_CD3-', 'd30_cd3_neg', 'day30_cd3-', 'cd3-_d30'],\n",
    "            'd60_cd3-': ['D60_CD3-', 'd60_cd3_neg', 'day60_cd3-', 'cd3-_d60'],\n",
    "            'd100_cd3-': ['D100_CD3-', 'd100_cd3_neg', 'day100_cd3-', 'cd3-_d100']\n",
    "        }\n",
    "        \n",
    "        mapped_any = False\n",
    "        for std_col, alt_names in alt_mappings.items():\n",
    "            if std_col not in df.columns:\n",
    "                for alt_name in alt_names:\n",
    "                    if alt_name in df.columns:\n",
    "                        df_enhanced[std_col] = df[alt_name]\n",
    "                        print(f\"   ‚úÖ Mapped {alt_name} to {std_col}\")\n",
    "                        mapped_any = True\n",
    "                        break\n",
    "        \n",
    "        if not mapped_any:\n",
    "            print(\"‚ùå Could not find chimerism columns. Available columns:\")\n",
    "            print(df.columns.tolist())\n",
    "            return df_enhanced\n",
    "    else:\n",
    "        print(\"‚úÖ All required chimerism columns found!\")\n",
    "    \n",
    "    # Convert chimerism columns to numeric, handling any string/object types\n",
    "    chimerism_cols = [col for col in required_cols if col in df_enhanced.columns]\n",
    "    for col in chimerism_cols:\n",
    "        df_enhanced[col] = pd.to_numeric(df_enhanced[col], errors='coerce')\n",
    "        print(f\"   Converting {col}: {df_enhanced[col].dtype}, missing: {df_enhanced[col].isnull().sum()}\")\n",
    "    \n",
    "    # 1. TIME-POINT DIFFERENCES\n",
    "    print(\"   üìà Creating time-point difference features...\")\n",
    "    \n",
    "    # Calculate differences between consecutive time points\n",
    "    if all(col in df_enhanced.columns for col in ['d30_cd3+', 'd60_cd3+']):\n",
    "        df_enhanced[\"d(30-60)_cd3+\"] = df_enhanced[\"d30_cd3+\"] - df_enhanced[\"d60_cd3+\"]\n",
    "    if all(col in df_enhanced.columns for col in ['d60_cd3+', 'd100_cd3+']):\n",
    "        df_enhanced[\"d(60-100)_cd3+\"] = df_enhanced[\"d60_cd3+\"] - df_enhanced[\"d100_cd3+\"]\n",
    "    if all(col in df_enhanced.columns for col in ['d30_cd3-', 'd60_cd3-']):\n",
    "        df_enhanced[\"d(30-60)_cd3-\"] = df_enhanced[\"d30_cd3-\"] - df_enhanced[\"d60_cd3-\"]\n",
    "    if all(col in df_enhanced.columns for col in ['d60_cd3-', 'd100_cd3-']):\n",
    "        df_enhanced[\"d(60-100)_cd3-\"] = df_enhanced[\"d60_cd3-\"] - df_enhanced[\"d100_cd3-\"]\n",
    "    \n",
    "    # Calculate overall change (Day 30 to Day 100)\n",
    "    if all(col in df_enhanced.columns for col in ['d30_cd3+', 'd100_cd3+']):\n",
    "        df_enhanced[\"d(30-100)_cd3+\"] = df_enhanced[\"d30_cd3+\"] - df_enhanced[\"d100_cd3+\"]\n",
    "    if all(col in df_enhanced.columns for col in ['d30_cd3-', 'd100_cd3-']):\n",
    "        df_enhanced[\"d(30-100)_cd3-\"] = df_enhanced[\"d30_cd3-\"] - df_enhanced[\"d100_cd3-\"]\n",
    "    \n",
    "    # 2. DEMOGRAPHIC FEATURES\n",
    "    print(\"   üë• Creating demographic ratio features...\")\n",
    "    \n",
    "    if 'age' in df.columns and 'donor_age' in df.columns:\n",
    "        # Convert to numeric and avoid division by zero\n",
    "        df_enhanced['age'] = pd.to_numeric(df_enhanced['age'], errors='coerce')\n",
    "        df_enhanced['donor_age'] = pd.to_numeric(df_enhanced['donor_age'], errors='coerce')\n",
    "        \n",
    "        df_enhanced[\"age_receiver_donor_ratio\"] = df_enhanced[\"age\"] / (df_enhanced[\"donor_age\"] + 0.001)\n",
    "        df_enhanced[\"age_difference\"] = df_enhanced[\"age\"] - df_enhanced[\"donor_age\"]\n",
    "    \n",
    "    # 3. STATISTICAL SUMMARY FEATURES\n",
    "    print(\"   üìä Creating statistical summary features...\")\n",
    "    \n",
    "    # Mean chimerism levels across time points\n",
    "    cd3_pos_cols = [\"d30_cd3+\", \"d60_cd3+\", \"d100_cd3+\"]\n",
    "    cd3_neg_cols = [\"d30_cd3-\", \"d60_cd3-\", \"d100_cd3-\"]\n",
    "    \n",
    "    # Check which columns actually exist\n",
    "    cd3_pos_cols = [col for col in cd3_pos_cols if col in df_enhanced.columns]\n",
    "    cd3_neg_cols = [col for col in cd3_neg_cols if col in df_enhanced.columns]\n",
    "    \n",
    "    if cd3_pos_cols:\n",
    "        df_enhanced[\"mean_cd3+\"] = df_enhanced[cd3_pos_cols].mean(axis=1)\n",
    "        df_enhanced[\"std_cd3+\"] = df_enhanced[cd3_pos_cols].std(axis=1)\n",
    "        df_enhanced[\"cv_cd3+\"] = df_enhanced[\"std_cd3+\"] / (df_enhanced[\"mean_cd3+\"] + 0.001)\n",
    "        df_enhanced[\"min_cd3+\"] = df_enhanced[cd3_pos_cols].min(axis=1)\n",
    "        df_enhanced[\"max_cd3+\"] = df_enhanced[cd3_pos_cols].max(axis=1)\n",
    "        df_enhanced[\"range_cd3+\"] = df_enhanced[\"max_cd3+\"] - df_enhanced[\"min_cd3+\"]\n",
    "    \n",
    "    if cd3_neg_cols:\n",
    "        df_enhanced[\"mean_cd3-\"] = df_enhanced[cd3_neg_cols].mean(axis=1)\n",
    "        df_enhanced[\"std_cd3-\"] = df_enhanced[cd3_neg_cols].std(axis=1)\n",
    "        df_enhanced[\"cv_cd3-\"] = df_enhanced[\"std_cd3-\"] / (df_enhanced[\"mean_cd3-\"] + 0.001)\n",
    "        df_enhanced[\"min_cd3-\"] = df_enhanced[cd3_neg_cols].min(axis=1)\n",
    "        df_enhanced[\"max_cd3-\"] = df_enhanced[cd3_neg_cols].max(axis=1)\n",
    "        df_enhanced[\"range_cd3-\"] = df_enhanced[\"max_cd3-\"] - df_enhanced[\"min_cd3-\"]\n",
    "    \n",
    "    # 4. PERCENTAGE CHANGE FEATURES\n",
    "    print(\"   üìà Creating percentage change features...\")\n",
    "    \n",
    "    if all(col in df_enhanced.columns for col in [\"d30_cd3+\", \"d100_cd3+\"]):\n",
    "        df_enhanced[\"pct_change_30_100_cd3+\"] = (\n",
    "            (df_enhanced[\"d100_cd3+\"] - df_enhanced[\"d30_cd3+\"]) / (df_enhanced[\"d30_cd3+\"] + 0.001) * 100\n",
    "        )\n",
    "    \n",
    "    if all(col in df_enhanced.columns for col in [\"d30_cd3-\", \"d100_cd3-\"]):\n",
    "        df_enhanced[\"pct_change_30_100_cd3-\"] = (\n",
    "            (df_enhanced[\"d100_cd3-\"] - df_enhanced[\"d30_cd3-\"]) / (df_enhanced[\"d30_cd3-\"] + 0.001) * 100\n",
    "        )\n",
    "    \n",
    "    # 5. SLOPE FEATURES (Linear trend)\n",
    "    print(\"   üìâ Creating slope/trend features...\")\n",
    "    \n",
    "    # Simple slope calculation (change per time unit)\n",
    "    time_points = np.array([30, 60, 100])\n",
    "    \n",
    "    def calculate_slope(row, cols):\n",
    "        \"\"\"Calculate linear slope for chimerism values over time.\"\"\"\n",
    "        values = row[cols].values\n",
    "        try:\n",
    "            values = pd.to_numeric(values, errors='coerce')\n",
    "            if pd.isna(values).any() or len(values) < 2:\n",
    "                return np.nan\n",
    "            valid_mask = ~pd.isna(values)\n",
    "            if valid_mask.sum() < 2:\n",
    "                return np.nan\n",
    "            valid_values = values[valid_mask]\n",
    "            valid_times = time_points[valid_mask]\n",
    "            return np.polyfit(valid_times, valid_values, 1)[0]\n",
    "        except (ValueError, TypeError, np.linalg.LinAlgError):\n",
    "            return np.nan\n",
    "    \n",
    "    if cd3_pos_cols:\n",
    "        df_enhanced[\"slope_cd3+\"] = df_enhanced.apply(\n",
    "            lambda row: calculate_slope(row, cd3_pos_cols), axis=1\n",
    "        )\n",
    "    \n",
    "    if cd3_neg_cols:\n",
    "        df_enhanced[\"slope_cd3-\"] = df_enhanced.apply(\n",
    "            lambda row: calculate_slope(row, cd3_neg_cols), axis=1\n",
    "        )\n",
    "    \n",
    "    print(f\"‚úÖ Feature engineering completed. Added {len(df_enhanced.columns) - len(df.columns)} new features.\")\n",
    "    \n",
    "    return df_enhanced\n",
    "\n",
    "# Apply feature engineering\n",
    "df_enhanced = create_chimerism_dynamics_features(df)\n",
    "\n",
    "# Show summary of new features\n",
    "new_features = [col for col in df_enhanced.columns if col not in df.columns]\n",
    "print(f\"\\n=== New Features Created ({len(new_features)}) ===\")\n",
    "for i, feature in enumerate(new_features[:20], 1):  # Show first 20\n",
    "    print(f\"{i:2d}. {feature}\")\n",
    "if len(new_features) > 20:\n",
    "    print(f\"... and {len(new_features) - 20} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chimerism Pattern Classification\n",
    "\n",
    "Categorize chimerism changes into clinically meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∑Ô∏è Creating chimerism pattern labels...\n",
      "‚úÖ Pattern features created successfully\n",
      "\n",
      "=== CD3+ Chimerism Pattern Distribution ===\n",
      "fluctuating :  67 ( 26.0%)\n",
      "increasing  :  65 ( 25.2%)\n",
      "unknown     :  46 ( 17.8%)\n",
      "stable      :  41 ( 15.9%)\n",
      "decreasing  :  39 ( 15.1%)\n",
      "\n",
      "=== CD3- Chimerism Pattern Distribution ===\n",
      "stable      : 128 ( 49.6%)\n",
      "increasing  :  52 ( 20.2%)\n",
      "unknown     :  46 ( 17.8%)\n",
      "decreasing  :  17 (  6.6%)\n",
      "fluctuating :  15 (  5.8%)\n"
     ]
    }
   ],
   "source": [
    "def assign_chimerism_trend_labels(df, col_a, col_b, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Assign trend labels based on chimerism changes between time periods.\n",
    "    \"\"\"\n",
    "    # Convert to categorical based on sign and magnitude\n",
    "    def categorize_change(x, threshold):\n",
    "        if pd.isna(x):\n",
    "            return np.nan\n",
    "        elif x > threshold:\n",
    "            return 1  # Increase\n",
    "        elif x < -threshold:\n",
    "            return -1  # Decrease\n",
    "        else:\n",
    "            return 0  # Stable\n",
    "    \n",
    "    # Categorize changes for both periods\n",
    "    change_a = df[col_a].apply(lambda x: categorize_change(x, threshold))\n",
    "    change_b = df[col_b].apply(lambda x: categorize_change(x, threshold))\n",
    "    \n",
    "    # Define trend patterns based on consecutive changes\n",
    "    def assign_pattern(a, b):\n",
    "        if pd.isna(a) or pd.isna(b):\n",
    "            return 'unknown'\n",
    "        elif a == -1 and b == -1:\n",
    "            return 'consistently_downward'\n",
    "        elif a == 1 and b == 1:\n",
    "            return 'consistently_upward'\n",
    "        elif a == 0 and b == 0:\n",
    "            return 'stable'\n",
    "        elif (a == -1 and b == 1) or (a == 1 and b == -1):\n",
    "            return 'fluctuating'\n",
    "        elif a == -1 and b == 0:\n",
    "            return 'downward_then_stable'\n",
    "        elif a == 0 and b == -1:\n",
    "            return 'stable_then_downward'\n",
    "        elif a == 1 and b == 0:\n",
    "            return 'upward_then_stable'\n",
    "        elif a == 0 and b == 1:\n",
    "            return 'stable_then_upward'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    # Apply pattern assignment\n",
    "    patterns = pd.Series(\n",
    "        [assign_pattern(a, b) for a, b in zip(change_a, change_b)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def create_pattern_features(df):\n",
    "    \"\"\"\n",
    "    Create pattern-based features for chimerism dynamics.\n",
    "    \"\"\"\n",
    "    print(\"üè∑Ô∏è Creating chimerism pattern labels...\")\n",
    "    \n",
    "    df_patterns = df.copy()\n",
    "    \n",
    "    # Check for required difference columns\n",
    "    required_diff_cols = ['d(30-60)_cd3+', 'd(60-100)_cd3+', 'd(30-60)_cd3-', 'd(60-100)_cd3-']\n",
    "    missing_cols = [col for col in required_diff_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"‚ö†Ô∏è Missing required columns for pattern analysis: {missing_cols}\")\n",
    "        return df_patterns\n",
    "    \n",
    "    # Create pattern labels\n",
    "    df_patterns['cd3+_trend_pattern'] = assign_chimerism_trend_labels(\n",
    "        df, 'd(30-60)_cd3+', 'd(60-100)_cd3+'\n",
    "    )\n",
    "    \n",
    "    df_patterns['cd3-_trend_pattern'] = assign_chimerism_trend_labels(\n",
    "        df, 'd(30-60)_cd3-', 'd(60-100)_cd3-'\n",
    "    )\n",
    "    \n",
    "    # Create simplified binary patterns\n",
    "    def simplify_pattern(pattern):\n",
    "        if pd.isna(pattern) or pattern == 'unknown':\n",
    "            return 'unknown'\n",
    "        elif 'upward' in pattern:\n",
    "            return 'increasing'\n",
    "        elif 'downward' in pattern:\n",
    "            return 'decreasing'\n",
    "        elif pattern == 'stable':\n",
    "            return 'stable'\n",
    "        elif pattern == 'fluctuating':\n",
    "            return 'fluctuating'\n",
    "        else:\n",
    "            return 'mixed'\n",
    "    \n",
    "    df_patterns['cd3+_simple_pattern'] = df_patterns['cd3+_trend_pattern'].apply(simplify_pattern)\n",
    "    df_patterns['cd3-_simple_pattern'] = df_patterns['cd3-_trend_pattern'].apply(simplify_pattern)\n",
    "    \n",
    "    # Create numerical encoding for ML models\n",
    "    pattern_encoder = {\n",
    "        'unknown': 0,\n",
    "        'decreasing': 1,\n",
    "        'stable': 2,\n",
    "        'increasing': 3,\n",
    "        'fluctuating': 4,\n",
    "        'mixed': 5\n",
    "    }\n",
    "    \n",
    "    df_patterns['cd3+_pattern_encoded'] = df_patterns['cd3+_simple_pattern'].map(pattern_encoder)\n",
    "    df_patterns['cd3-_pattern_encoded'] = df_patterns['cd3-_simple_pattern'].map(pattern_encoder)\n",
    "    \n",
    "    print(\"‚úÖ Pattern features created successfully\")\n",
    "    \n",
    "    return df_patterns\n",
    "\n",
    "# Apply pattern analysis\n",
    "df_with_patterns = create_pattern_features(df_enhanced)\n",
    "\n",
    "# Analyze pattern distributions\n",
    "print(f\"\\n=== CD3+ Chimerism Pattern Distribution ===\")\n",
    "cd3_pos_patterns = df_with_patterns['cd3+_simple_pattern'].value_counts()\n",
    "for pattern, count in cd3_pos_patterns.items():\n",
    "    percentage = (count / len(df_with_patterns)) * 100\n",
    "    print(f\"{pattern:12s}: {count:3d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n=== CD3- Chimerism Pattern Distribution ===\")\n",
    "cd3_neg_patterns = df_with_patterns['cd3-_simple_pattern'].value_counts()\n",
    "for pattern, count in cd3_neg_patterns.items():\n",
    "    percentage = (count / len(df_with_patterns)) * 100\n",
    "    print(f\"{pattern:12s}: {count:3d} ({percentage:5.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature and Target Extraction\n",
    "\n",
    "Extract features and target variables with intelligent column mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üë• Filtered to AML/MDS patients: 182 from 258\n",
      "\n",
      "=== Feature and Target Extraction Complete ===\n",
      "Features shape: (182, 69)\n",
      "Classification targets shape: (182, 5)\n",
      "Regression targets shape: (182, 2)\n",
      "\n",
      "Classification targets: ['y_rfs', 'y_death', 'y_relapse', 'y_cgvhd', 'y_agvhd']\n",
      "Regression targets: ['y_os_days', 'y_rfs_days']\n"
     ]
    }
   ],
   "source": [
    "def extract_features_and_targets(df, excel_mapping=None):\n",
    "    \"\"\"\n",
    "    Extract features and labels from the dataset using intelligent column detection.\n",
    "    \n",
    "    This function combines:\n",
    "    - Excel column mapping approach from aml_mds_organized\n",
    "    - Dynamic feature detection from basic_analysis_organized\n",
    "    \"\"\"\n",
    "    # Filter for AML/MDS patients (Disease == 1)\n",
    "    if 'disease' in df.columns:\n",
    "        df_filtered = df[df['disease'] == 1].copy()\n",
    "        print(f\"   üë• Filtered to AML/MDS patients: {len(df_filtered)} from {len(df)}\")\n",
    "    elif 'Disease' in df.columns:\n",
    "        df_filtered = df[df['Disease'] == 1].copy()\n",
    "        print(f\"   üë• Filtered to AML/MDS patients: {len(df_filtered)} from {len(df)}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è 'disease' column not found, using all patients\")\n",
    "        df_filtered = df.copy()\n",
    "    \n",
    "    # Identify feature columns (exclude outcome variables)\n",
    "    exclude_cols = ['dose_dli', 'dose_dli_2', 'indication_for_dli', 'post_dli_gvhd',\n",
    "                   'grade_at_onset', 'time_to_onset', 'highest_grade', 'dli']\n",
    "    \n",
    "    feature_cols = [col for col in df_filtered.columns \n",
    "                   if not col.startswith('y_') and col not in exclude_cols]\n",
    "    \n",
    "    # Identify target columns\n",
    "    classification_labels = ['y_rfs', 'y_death', 'y_relapse', 'y_cgvhd', 'y_agvhd']\n",
    "    regression_labels = ['y_os_days', 'y_rfs_days']\n",
    "    \n",
    "    # Filter to available columns\n",
    "    available_class_labels = [col for col in classification_labels if col in df_filtered.columns]\n",
    "    available_reg_labels = [col for col in regression_labels if col in df_filtered.columns]\n",
    "    \n",
    "    # Extract features and targets\n",
    "    X = df_filtered[feature_cols].copy()\n",
    "    y_classification = df_filtered[available_class_labels].copy() if available_class_labels else pd.DataFrame()\n",
    "    y_regression = df_filtered[available_reg_labels].copy() if available_reg_labels else pd.DataFrame()\n",
    "    \n",
    "    # Reset indices\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y_classification.reset_index(drop=True, inplace=True)\n",
    "    y_regression.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    print(f\"\\n=== Feature and Target Extraction Complete ===\")\n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Classification targets shape: {y_classification.shape}\")\n",
    "    print(f\"Regression targets shape: {y_regression.shape}\")\n",
    "    \n",
    "    # Show target names\n",
    "    if not y_classification.empty:\n",
    "        print(f\"\\nClassification targets: {list(y_classification.columns)}\")\n",
    "    if not y_regression.empty:\n",
    "        print(f\"Regression targets: {list(y_regression.columns)}\")\n",
    "    \n",
    "    return X, y_classification, y_regression\n",
    "\n",
    "# Extract features and targets\n",
    "X, y_classification, y_regression = extract_features_and_targets(df_with_patterns, excel_column_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Data Preprocessing\n",
    "\n",
    "Apply sophisticated preprocessing including multiple imputation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying advanced preprocessing...\n",
      "   üè∑Ô∏è Encoding 9 categorical columns\n",
      "   üìâ High missing (>50%): 0 columns\n",
      "   üìä Moderate missing (‚â§50%): 39 columns\n",
      "   üìè Scaling features using StandardScaler\n",
      "\n",
      "‚úÖ Preprocessing completed\n",
      "   Remaining missing values: 0\n",
      "\n",
      "=== Preprocessing Summary ===\n",
      "Features: (182, 69)\n",
      "Classification targets: (182, 5)\n",
      "Regression targets: (182, 2)\n"
     ]
    }
   ],
   "source": [
    "def advanced_preprocessing(X, y_classification, y_regression):\n",
    "    \"\"\"\n",
    "    Apply advanced preprocessing with multiple imputation strategies.\n",
    "    \"\"\"\n",
    "    print(\"üîß Applying advanced preprocessing...\")\n",
    "    \n",
    "    # Process features\n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    # 1. Handle datetime columns\n",
    "    datetime_cols = X_processed.select_dtypes(include=['datetime64']).columns\n",
    "    if len(datetime_cols) > 0:\n",
    "        print(f\"   üìÖ Converting {len(datetime_cols)} datetime columns to ordinal\")\n",
    "        for col in datetime_cols:\n",
    "            X_processed[col] = X_processed[col].apply(\n",
    "                lambda x: x.toordinal() if pd.notnull(x) else np.nan\n",
    "            )\n",
    "    \n",
    "    # 2. Encode categorical variables\n",
    "    categorical_cols = X_processed.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"   üè∑Ô∏è Encoding {len(categorical_cols)} categorical columns\")\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_processed[col] = le.fit_transform(X_processed[col].astype(str))\n",
    "    \n",
    "    # 3. Advanced imputation based on missing data patterns\n",
    "    numerical_cols = X_processed.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    \n",
    "    if numerical_cols:\n",
    "        missing_analysis = X_processed[numerical_cols].isnull().sum().sort_values(ascending=False)\n",
    "        high_missing = missing_analysis[missing_analysis > len(X_processed) * 0.5].index.tolist()\n",
    "        moderate_missing = missing_analysis[(missing_analysis > 0) & (missing_analysis <= len(X_processed) * 0.5)].index.tolist()\n",
    "        \n",
    "        print(f\"   üìâ High missing (>50%): {len(high_missing)} columns\")\n",
    "        print(f\"   üìä Moderate missing (‚â§50%): {len(moderate_missing)} columns\")\n",
    "        \n",
    "        # For columns with high missing values, use simple median imputation\n",
    "        if high_missing:\n",
    "            simple_imputer = SimpleImputer(strategy='median')\n",
    "            X_processed[high_missing] = simple_imputer.fit_transform(X_processed[high_missing])\n",
    "        \n",
    "        # For columns with moderate missing values, use iterative imputation\n",
    "        if moderate_missing:\n",
    "            try:\n",
    "                iterative_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "                X_processed[moderate_missing] = iterative_imputer.fit_transform(X_processed[moderate_missing])\n",
    "            except Exception as e:\n",
    "                print(f\"      ‚ö†Ô∏è Iterative imputation failed: {str(e)}\")\n",
    "                fallback_imputer = SimpleImputer(strategy='median')\n",
    "                X_processed[moderate_missing] = fallback_imputer.fit_transform(X_processed[moderate_missing])\n",
    "    \n",
    "    # 4. Scale features\n",
    "    print(\"   üìè Scaling features using StandardScaler\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_processed),\n",
    "        columns=X_processed.columns,\n",
    "        index=X_processed.index\n",
    "    )\n",
    "    \n",
    "    # Process classification labels\n",
    "    y_class_processed = y_classification.copy()\n",
    "    \n",
    "    for col in y_class_processed.columns:\n",
    "        # Convert continuous to binary if needed\n",
    "        if y_class_processed[col].dtype in ['float64', 'int64']:\n",
    "            unique_vals = y_class_processed[col].dropna().unique()\n",
    "            if len(unique_vals) > 2:\n",
    "                y_class_processed[col] = pd.cut(\n",
    "                    y_class_processed[col],\n",
    "                    bins=[-np.inf, 0, np.inf],\n",
    "                    labels=[0, 1]\n",
    "                )\n",
    "        \n",
    "        # Encode as integers\n",
    "        le = LabelEncoder()\n",
    "        y_class_processed[col] = le.fit_transform(y_class_processed[col].astype(str))\n",
    "    \n",
    "    # Handle missing values\n",
    "    y_class_processed.fillna(method='ffill', inplace=True)\n",
    "    if y_class_processed.isnull().sum().sum() > 0:\n",
    "        y_class_processed.fillna(y_class_processed.mode().iloc[0], inplace=True)\n",
    "    \n",
    "    # Remove constant columns\n",
    "    non_constant_cols = [col for col in y_class_processed.columns if y_class_processed[col].nunique() > 1]\n",
    "    y_class_processed = y_class_processed[non_constant_cols]\n",
    "    \n",
    "    # Process regression labels\n",
    "    y_reg_processed = y_regression.copy()\n",
    "    \n",
    "    for col in y_reg_processed.columns:\n",
    "        y_reg_processed[col] = pd.to_numeric(y_reg_processed[col], errors='coerce')\n",
    "        y_reg_processed[col].fillna(y_reg_processed[col].median(), inplace=True)\n",
    "    \n",
    "    print(\"\\n‚úÖ Preprocessing completed\")\n",
    "    print(f\"   Remaining missing values: {X_scaled.isnull().sum().sum()}\")\n",
    "    \n",
    "    return X_scaled, y_class_processed, y_reg_processed, scaler\n",
    "\n",
    "# Apply preprocessing\n",
    "X_preprocessed, y_classification_processed, y_regression_processed, scaler = advanced_preprocessing(\n",
    "    X, y_classification, y_regression\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Preprocessing Summary ===\")\n",
    "print(f\"Features: {X_preprocessed.shape}\")\n",
    "print(f\"Classification targets: {y_classification_processed.shape}\")\n",
    "print(f\"Regression targets: {y_regression_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exploratory Data Analysis and Visualizations\n",
    "\n",
    "Create comprehensive visualizations to understand data distributions and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_eda(X, y_classification):\n",
    "    \"\"\"\n",
    "    Create comprehensive exploratory data analysis visualizations.\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating comprehensive exploratory data analysis...\")\n",
    "    \n",
    "    # 1. TARGET VARIABLE CORRELATIONS\n",
    "    print(\"\\nüìà 1. Target Variable Correlation Analysis\")\n",
    "    \n",
    "    if not y_classification.empty and len(y_classification.columns) > 1:\n",
    "        correlation_matrix = y_classification.corr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "        sns.heatmap(\n",
    "            correlation_matrix,\n",
    "            mask=mask,\n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            fmt='.3f',\n",
    "            annot_kws={\"size\": 12},\n",
    "            linewidths=0.5,\n",
    "            cbar_kws={\"shrink\": 0.8}\n",
    "        )\n",
    "        plt.title('Target Variable Correlation Matrix', fontsize=16, pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find strongest correlations\n",
    "        corr_values = correlation_matrix.mask(mask).stack().reset_index()\n",
    "        corr_values.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "        corr_values = corr_values.sort_values('Correlation', key=abs, ascending=False)\n",
    "        \n",
    "        print(\"\\nStrongest correlations between targets:\")\n",
    "        print(corr_values.head())\n",
    "    \n",
    "    # 2. CLASS DISTRIBUTION ANALYSIS\n",
    "    print(\"\\nüéØ 2. Class Distribution Analysis\")\n",
    "    \n",
    "    if not y_classification.empty:\n",
    "        target_names = ['RFS', 'Death', 'Relapse', 'cGVHD', 'aGVHD']\n",
    "        n_targets = min(len(y_classification.columns), len(target_names))\n",
    "        \n",
    "        fig, axes = plt.subplots(1, n_targets, figsize=(4 * n_targets, 4))\n",
    "        if n_targets == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (col, name) in enumerate(zip(y_classification.columns[:n_targets], target_names[:n_targets])):\n",
    "            counts = y_classification[col].value_counts().sort_index()\n",
    "            \n",
    "            bars = axes[i].bar(\n",
    "                ['No', 'Yes'],\n",
    "                [counts.get(0, 0), counts.get(1, 0)],\n",
    "                color=['lightblue', 'lightcoral'],\n",
    "                alpha=0.7,\n",
    "                edgecolor='black'\n",
    "            )\n",
    "            \n",
    "            axes[i].set_title(f'{name}', fontsize=12)\n",
    "            axes[i].set_ylabel('Count')\n",
    "            \n",
    "            # Add value labels on bars\n",
    "            for bar, count in zip(bars, [counts.get(0, 0), counts.get(1, 0)]):\n",
    "                height = bar.get_height()\n",
    "                axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "                           f'{count}', ha='center', va='bottom', fontsize=10)\n",
    "            \n",
    "            # Add percentage\n",
    "            total = counts.sum()\n",
    "            if total > 0:\n",
    "                yes_pct = (counts.get(1, 0) / total) * 100\n",
    "                axes[i].text(0.5, max(counts) * 0.8, f'{yes_pct:.1f}%\\npositive',\n",
    "                           ha='center', va='center', fontsize=9,\n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plt.suptitle('Outcome Distributions', fontsize=16, y=1.05)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # 3. FEATURE DISTRIBUTION ANALYSIS\n",
    "    print(\"\\nüìä 3. Key Feature Distribution Analysis\")\n",
    "    \n",
    "    # Focus on key chimerism features\n",
    "    chimerism_cols = [col for col in X.columns if any(x in col.lower() for x in ['cd3', 'chimerism'])]\n",
    "    key_features = chimerism_cols[:12] if chimerism_cols else X.columns[:12]\n",
    "    \n",
    "    if key_features:\n",
    "        n_features = len(key_features)\n",
    "        n_cols = min(4, n_features)\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 3 * n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        elif n_cols == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        for i, feature in enumerate(key_features):\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            \n",
    "            # Create histogram\n",
    "            axes[row, col].hist(X[feature].dropna(), bins=20, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            axes[row, col].set_title(f'{feature}', fontsize=10)\n",
    "            axes[row, col].set_xlabel('Value')\n",
    "            axes[row, col].set_ylabel('Frequency')\n",
    "            axes[row, col].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add basic statistics\n",
    "            mean_val = X[feature].mean()\n",
    "            axes[row, col].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.2f}')\n",
    "            axes[row, col].legend(fontsize=8)\n",
    "        \n",
    "        # Hide empty subplots\n",
    "        for i in range(len(key_features), n_rows * n_cols):\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            axes[row, col].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Key Feature Distributions', fontsize=16, y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run EDA\n",
    "create_comprehensive_eda(X_preprocessed, y_classification_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Pattern-Outcome Association Analysis\n",
    "\n",
    "Analyze relationships between chimerism patterns and transplant outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_pattern_outcome_associations(df_patterns, y_classification):\n",
    "    \"\"\"\n",
    "    Analyze associations between chimerism patterns and transplant outcomes.\n",
    "    \"\"\"\n",
    "    print(\"üîç Analyzing pattern-outcome associations...\")\n",
    "    \n",
    "    # Check if pattern columns exist\n",
    "    if 'cd3+_simple_pattern' not in df_patterns.columns:\n",
    "        print(\"‚ö†Ô∏è Pattern columns not found. Skipping analysis.\")\n",
    "        return\n",
    "    \n",
    "    # Get aligned data\n",
    "    common_indices = df_patterns.index.intersection(y_classification.index)\n",
    "    df_patterns_aligned = df_patterns.loc[common_indices]\n",
    "    y_aligned = y_classification.loc[common_indices]\n",
    "    \n",
    "    target_names = ['RFS', 'Death', 'Relapse', 'cGVHD', 'aGVHD']\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, len(y_aligned.columns), figsize=(5 * len(y_aligned.columns), 10))\n",
    "    \n",
    "    for i, (col, name) in enumerate(zip(y_aligned.columns, target_names[:len(y_aligned.columns)])):\n",
    "        # CD3+ patterns\n",
    "        crosstab_cd3pos = pd.crosstab(\n",
    "            df_patterns_aligned['cd3+_simple_pattern'],\n",
    "            y_aligned[col],\n",
    "            normalize='index'\n",
    "        )\n",
    "        \n",
    "        if 1 in crosstab_cd3pos.columns:\n",
    "            relapse_props = crosstab_cd3pos[1].sort_values(ascending=True)\n",
    "            \n",
    "            axes[0, i].barh(range(len(relapse_props)), relapse_props.values,\n",
    "                           color='lightcoral', alpha=0.7)\n",
    "            axes[0, i].set_yticks(range(len(relapse_props)))\n",
    "            axes[0, i].set_yticklabels(relapse_props.index)\n",
    "            axes[0, i].set_xlabel(f'Proportion with {name}')\n",
    "            axes[0, i].set_title(f'CD3+ Pattern vs {name}')\n",
    "            axes[0, i].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for j, v in enumerate(relapse_props.values):\n",
    "                axes[0, i].text(v + 0.01, j, f'{v:.2%}', va='center', fontsize=9)\n",
    "        \n",
    "        # CD3- patterns\n",
    "        crosstab_cd3neg = pd.crosstab(\n",
    "            df_patterns_aligned['cd3-_simple_pattern'],\n",
    "            y_aligned[col],\n",
    "            normalize='index'\n",
    "        )\n",
    "        \n",
    "        if 1 in crosstab_cd3neg.columns:\n",
    "            relapse_props = crosstab_cd3neg[1].sort_values(ascending=True)\n",
    "            \n",
    "            axes[1, i].barh(range(len(relapse_props)), relapse_props.values,\n",
    "                           color='lightblue', alpha=0.7)\n",
    "            axes[1, i].set_yticks(range(len(relapse_props)))\n",
    "            axes[1, i].set_yticklabels(relapse_props.index)\n",
    "            axes[1, i].set_xlabel(f'Proportion with {name}')\n",
    "            axes[1, i].set_title(f'CD3- Pattern vs {name}')\n",
    "            axes[1, i].grid(axis='x', alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for j, v in enumerate(relapse_props.values):\n",
    "                axes[1, i].text(v + 0.01, j, f'{v:.2%}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Chimerism Pattern vs Outcome Associations', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Analyze pattern associations\n",
    "if 'cd3+_simple_pattern' in df_with_patterns.columns:\n",
    "    analyze_pattern_outcome_associations(df_with_patterns, y_classification_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Selection and Optimization\n",
    "\n",
    "Implement advanced feature selection with multiple methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_feature_selection(X, y, task=\"classification\", method=\"select_k_best\"):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for feature selection methods.\n",
    "    \n",
    "    This function tests different feature set sizes and returns the optimal selection.\n",
    "    \"\"\"\n",
    "    # Choose appropriate scoring function and model\n",
    "    if task == \"classification\":\n",
    "        score_func = f_classif\n",
    "        model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "        scoring = 'accuracy'\n",
    "    else:\n",
    "        score_func = f_regression\n",
    "        model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "        scoring = 'neg_mean_squared_error'\n",
    "    \n",
    "    if method == \"select_k_best\":\n",
    "        # Test Fibonacci sequence k values (optimal for feature selection)\n",
    "        k_values = [1, 2, 3, 5, 8, 13, 21, 34]\n",
    "        k_values = [k for k in k_values if k <= X.shape[1]]\n",
    "        \n",
    "        best_k = k_values[0]\n",
    "        best_score = -np.inf\n",
    "        best_selector = None\n",
    "        \n",
    "        for k in k_values:\n",
    "            selector = SelectKBest(score_func=score_func, k=k)\n",
    "            X_selected = selector.fit_transform(X, y)\n",
    "            \n",
    "            # Cross-validation score\n",
    "            scores = cross_val_score(model, X_selected, y, cv=5, scoring=scoring)\n",
    "            score = np.mean(scores)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_k, best_score = k, score\n",
    "                best_selector = selector\n",
    "        \n",
    "        # Get selected feature names\n",
    "        selected_features = X.columns[best_selector.get_support()].tolist()\n",
    "        print(f\"   Optimal k={best_k}, score={best_score:.3f}\")\n",
    "        print(f\"   Selected features: {selected_features}\")\n",
    "        \n",
    "        return best_selector.transform(X), selected_features, best_k\n",
    "    \n",
    "    elif method == \"rfe\":\n",
    "        # Test different numbers of features\n",
    "        n_features = [1, 2, 3, 5, 8, 13, 21]\n",
    "        n_features = [n for n in n_features if n <= X.shape[1]]\n",
    "        \n",
    "        best_n = n_features[0]\n",
    "        best_score = -np.inf\n",
    "        best_selector = None\n",
    "        \n",
    "        for n in n_features:\n",
    "            selector = RFE(model, n_features_to_select=n)\n",
    "            X_selected = selector.fit_transform(X, y)\n",
    "            \n",
    "            scores = cross_val_score(model, X_selected, y, cv=5, scoring=scoring)\n",
    "            score = np.mean(scores)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_n, best_score = n, score\n",
    "                best_selector = selector\n",
    "        \n",
    "        selected_features = X.columns[best_selector.get_support()].tolist()\n",
    "        return best_selector.transform(X), selected_features, best_n\n",
    "    \n",
    "    elif method == \"feature_importance\":\n",
    "        # Train model and get feature importance\n",
    "        model.fit(X, y)\n",
    "        importance_scores = model.feature_importances_\n",
    "        \n",
    "        # Test different thresholds\n",
    "        thresholds = np.percentile(importance_scores, [50, 60, 70, 80, 90])\n",
    "        \n",
    "        best_threshold = thresholds[0]\n",
    "        best_score = -np.inf\n",
    "        best_features = None\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            selected_features = X.columns[importance_scores > threshold].tolist()\n",
    "            \n",
    "            if len(selected_features) == 0:\n",
    "                continue\n",
    "            \n",
    "            X_selected = X[selected_features]\n",
    "            scores = cross_val_score(model, X_selected, y, cv=5, scoring=scoring)\n",
    "            score = np.mean(scores)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_threshold, best_score = threshold, score\n",
    "                best_features = selected_features\n",
    "        \n",
    "        return X[best_features].values, best_features, len(best_features)\n",
    "\n",
    "# Create feature set definitions\n",
    "def create_feature_sets_for_analysis():\n",
    "    \"\"\"\n",
    "    Define different feature sets for comparative analysis.\n",
    "    \"\"\"\n",
    "    feature_sets = {\n",
    "        \"dynamics_only\": {\n",
    "            \"name\": \"Chimerism Dynamics Only\",\n",
    "            \"features\": [\"d(30-60)_cd3+\", \"d(60-100)_cd3+\", \"d(30-60)_cd3-\", \"d(60-100)_cd3-\"],\n",
    "            \"description\": \"Only time-point differences\"\n",
    "        },\n",
    "        \"timepoints_only\": {\n",
    "            \"name\": \"Time Points Only\",\n",
    "            \"features\": [\"d30_cd3+\", \"d30_cd3-\", \"d60_cd3+\", \"d60_cd3-\", \"d100_cd3+\", \"d100_cd3-\"],\n",
    "            \"description\": \"Raw chimerism values at each time point\"\n",
    "        },\n",
    "        \"statistics_only\": {\n",
    "            \"name\": \"Statistical Features Only\",\n",
    "            \"features\": [\"mean_cd3+\", \"mean_cd3-\", \"std_cd3+\", \"std_cd3-\", \"cv_cd3+\", \"cv_cd3-\"],\n",
    "            \"description\": \"Statistical summaries across time points\"\n",
    "        },\n",
    "        \"patterns_only\": {\n",
    "            \"name\": \"Pattern Features Only\",\n",
    "            \"features\": [\"cd3+_pattern_encoded\", \"cd3-_pattern_encoded\"],\n",
    "            \"description\": \"Encoded trend patterns\"\n",
    "        },\n",
    "        \"minimal_predictive\": {\n",
    "            \"name\": \"Minimal Predictive Set\",\n",
    "            \"features\": [\"d(60-100)_cd3+\", \"d100_cd3-\", \"std_cd3+\"],\n",
    "            \"description\": \"Top 3 most predictive features based on analysis\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return feature_sets\n",
    "\n",
    "print(\"‚úÖ Feature selection functions defined\")\n",
    "print(\"üìä Available feature selection methods: select_k_best, rfe, feature_importance\")\n",
    "print(\"üéØ Available feature sets:\", list(create_feature_sets_for_analysis().keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Machine Learning Model Training Pipeline\n",
    "\n",
    "Implement comprehensive ML pipeline with multiple algorithms and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_models(X, y_classification, feature_selection_method=\"select_k_best\", cv=5):\n",
    "    \"\"\"\n",
    "    Train classification models with cross-validation and feature selection for each target.\n",
    "    \n",
    "    This function combines:\n",
    "    - GridSearchCV hyperparameter tuning from aml_mds_organized\n",
    "    - Optimal k selection from basic_analysis_organized\n",
    "    \"\"\"\n",
    "    # Define hyperparameter grids\n",
    "    param_grids = {\n",
    "        \"Random Forest\": {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        \"SVM\": {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        },\n",
    "        \"Naive Bayes\": {}\n",
    "    }\n",
    "    \n",
    "    # Initialize classifiers\n",
    "    classifiers = {\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"SVM\": SVC(random_state=42, probability=True),\n",
    "        \"Naive Bayes\": GaussianNB()\n",
    "    }\n",
    "    \n",
    "    target_names = [\"RFS\", \"Death\", \"Relapse\", \"cGVHD\", \"aGVHD\"]\n",
    "    classification_results = {}\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    for idx, label in enumerate(y_classification.columns):\n",
    "        target_name = target_names[idx] if idx < len(target_names) else f\"Target_{idx}\"\n",
    "        print(f\"\\nüîπ Training models for {target_name} üîπ\")\n",
    "        \n",
    "        # Get target variable and ensure no missing values\n",
    "        y_label = y_classification[label].dropna()\n",
    "        X_label = X.loc[y_label.index]\n",
    "        \n",
    "        # Check if we have multiple classes\n",
    "        if len(y_label.unique()) <= 1:\n",
    "            print(f\"‚ö†Ô∏è Skipping {target_name}: Only one class present\")\n",
    "            continue\n",
    "        \n",
    "        # Apply feature selection\n",
    "        print(f\"Applying {feature_selection_method} feature selection...\")\n",
    "        X_selected, selected_features, n_features = optimize_feature_selection(\n",
    "            X_label, y_label, task=\"classification\", method=feature_selection_method\n",
    "        )\n",
    "        \n",
    "        classification_results[target_name] = {\n",
    "            'selected_features': selected_features,\n",
    "            'n_features': n_features,\n",
    "            'models': {}\n",
    "        }\n",
    "\n",
    "        for model_name, model in classifiers.items():\n",
    "            print(f\"  Training {model_name}...\")\n",
    "            \n",
    "            # Hyperparameter tuning\n",
    "            if param_grids[model_name]:  # If there are parameters to tune\n",
    "                grid_search = GridSearchCV(\n",
    "                    model, param_grids[model_name], cv=3, scoring='accuracy', n_jobs=-1\n",
    "                )\n",
    "                grid_search.fit(X_selected, y_label)\n",
    "                best_model = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "            else:\n",
    "                # For models without hyperparameters (like Naive Bayes)\n",
    "                best_model = model\n",
    "                best_params = {}\n",
    "                best_model.fit(X_selected, y_label)\n",
    "            \n",
    "            # Cross-validation evaluation\n",
    "            accuracy_scores = cross_val_score(\n",
    "                best_model, X_selected, y_label, cv=kf, scoring='accuracy'\n",
    "            )\n",
    "            f1_scores = cross_val_score(\n",
    "                best_model, X_selected, y_label, cv=kf, scoring='f1_macro'\n",
    "            )\n",
    "            \n",
    "            # Generate predictions for detailed report\n",
    "            y_pred = cross_val_predict(best_model, X_selected, y_label, cv=kf)\n",
    "            \n",
    "            # Store results\n",
    "            classification_results[target_name]['models'][model_name] = {\n",
    "                \"best_model_params\": best_params,\n",
    "                \"accuracy\": np.mean(accuracy_scores),\n",
    "                \"accuracy_std\": np.std(accuracy_scores),\n",
    "                \"f1_score\": np.mean(f1_scores),\n",
    "                \"f1_std\": np.std(f1_scores),\n",
    "                \"report\": classification_report(y_label, y_pred, output_dict=True),\n",
    "                \"model\": best_model\n",
    "            }\n",
    "            \n",
    "            print(f\"    Accuracy: {np.mean(accuracy_scores):.3f} (¬±{np.std(accuracy_scores):.3f})\")\n",
    "            print(f\"    F1-Score: {np.mean(f1_scores):.3f} (¬±{np.std(f1_scores):.3f})\")\n",
    "\n",
    "    return classification_results\n",
    "\n",
    "# Train models with different feature selection methods\n",
    "print(\"=== Starting Model Training ===\")\n",
    "feature_selection_methods = [\"select_k_best\", \"rfe\", \"feature_importance\"]\n",
    "classification_results_all = {}\n",
    "\n",
    "for method in feature_selection_methods:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üîπ FEATURE SELECTION: {method.upper()} üîπ\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    classification_results = train_classification_models(\n",
    "        X_preprocessed, y_classification_processed,\n",
    "        feature_selection_method=method, cv=5\n",
    "    )\n",
    "    \n",
    "    classification_results_all[method] = classification_results\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéâ ALL MODEL TRAINING COMPLETED üéâ\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Results Visualization and Performance Comparison\n",
    "\n",
    "Create comprehensive visualizations to compare model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_results(results):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of classification results.\n",
    "    \"\"\"\n",
    "    methods = list(results.keys())\n",
    "    classifiers = [\"Random Forest\", \"SVM\", \"Naive Bayes\"]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 8))\n",
    "    \n",
    "    num_methods = len(methods)\n",
    "    \n",
    "    # Get all unique targets across all methods\n",
    "    all_targets = set()\n",
    "    for method in methods:\n",
    "        all_targets.update(results[method].keys())\n",
    "    all_targets = sorted(list(all_targets))\n",
    "    \n",
    "    x = np.arange(len(all_targets))\n",
    "    width = 0.25 / num_methods\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(methods) * len(classifiers)))\n",
    "    color_idx = 0\n",
    "    \n",
    "    for method_idx, method in enumerate(methods):\n",
    "        for model_idx, model in enumerate(classifiers):\n",
    "            accuracies = []\n",
    "            accuracy_stds = []\n",
    "            f1_scores = []\n",
    "            f1_stds = []\n",
    "            \n",
    "            for target in all_targets:\n",
    "                if (target in results[method] and \n",
    "                    'models' in results[method][target] and\n",
    "                    model in results[method][target]['models']):\n",
    "                    model_results = results[method][target]['models'][model]\n",
    "                    accuracies.append(model_results[\"accuracy\"])\n",
    "                    accuracy_stds.append(model_results[\"accuracy_std\"])\n",
    "                    f1_scores.append(model_results[\"f1_score\"])\n",
    "                    f1_stds.append(model_results[\"f1_std\"])\n",
    "                else:\n",
    "                    accuracies.append(0)\n",
    "                    accuracy_stds.append(0)\n",
    "                    f1_scores.append(0)\n",
    "                    f1_stds.append(0)\n",
    "            \n",
    "            position = x + (method_idx * len(classifiers) + model_idx) * width\n",
    "            \n",
    "            # Plot accuracy\n",
    "            ax1.bar(position, accuracies, width=width,\n",
    "                   label=f\"{method.replace('_', ' ').title()} - {model}\",\n",
    "                   color=colors[color_idx], alpha=0.8)\n",
    "            ax1.errorbar(position, accuracies, yerr=accuracy_stds,\n",
    "                        fmt='none', color='black', capsize=2, linewidth=1)\n",
    "            \n",
    "            # Plot F1 scores\n",
    "            ax2.bar(position, f1_scores, width=width,\n",
    "                   label=f\"{method.replace('_', ' ').title()} - {model}\",\n",
    "                   color=colors[color_idx], alpha=0.8)\n",
    "            ax2.errorbar(position, f1_scores, yerr=f1_stds,\n",
    "                        fmt='none', color='black', capsize=2, linewidth=1)\n",
    "            \n",
    "            color_idx += 1\n",
    "    \n",
    "    # Customize accuracy subplot\n",
    "    ax1.set_xticks(x + (num_methods * len(classifiers) * width) / 2)\n",
    "    ax1.set_xticklabels(all_targets, rotation=0)\n",
    "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.set_title('Classification Accuracy Across Feature Selection Methods', fontsize=14, pad=20)\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    \n",
    "    # Customize F1 score subplot\n",
    "    ax2.set_xticks(x + (num_methods * len(classifiers) * width) / 2)\n",
    "    ax2.set_xticklabels(all_targets, rotation=0)\n",
    "    ax2.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_title('F1 Scores Across Feature Selection Methods', fontsize=14, pad=20)\n",
    "    ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='center right', bbox_to_anchor=(1.15, 0.5), fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Model Performance Comparison', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.85)\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "print(\"\\n=== VISUALIZING RESULTS ===\")\n",
    "plot_classification_results(classification_results_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance_analysis(classification_results_all, X_preprocessed):\n",
    "    \"\"\"\n",
    "    Create comprehensive feature importance analysis across all methods and targets.\n",
    "    \"\"\"\n",
    "    print(\"üîç Analyzing feature importance across all models...\")\n",
    "    \n",
    "    # Aggregate feature importance scores\n",
    "    feature_importance_scores = {}\n",
    "    \n",
    "    for method, method_results in classification_results_all.items():\n",
    "        for target, target_results in method_results.items():\n",
    "            if 'selected_features' in target_results:\n",
    "                # Weight features by model performance\n",
    "                for feature in target_results['selected_features']:\n",
    "                    if feature not in feature_importance_scores:\n",
    "                        feature_importance_scores[feature] = []\n",
    "                    \n",
    "                    # Get the best model's accuracy for this target\n",
    "                    best_accuracy = 0\n",
    "                    if 'models' in target_results:\n",
    "                        for model_name, model_results in target_results['models'].items():\n",
    "                            if 'accuracy' in model_results:\n",
    "                                best_accuracy = max(best_accuracy, model_results['accuracy'])\n",
    "                    \n",
    "                    feature_importance_scores[feature].append(best_accuracy)\n",
    "    \n",
    "    # Calculate average importance score for each feature\n",
    "    feature_importance_avg = {\n",
    "        feature: np.mean(scores) for feature, scores in feature_importance_scores.items()\n",
    "    }\n",
    "    \n",
    "    # Sort features by importance\n",
    "    sorted_features = sorted(feature_importance_avg.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # 1. Top 20 Most Important Features\n",
    "    top_features = sorted_features[:20]\n",
    "    features, scores = zip(*top_features)\n",
    "    \n",
    "    y_pos = np.arange(len(features))\n",
    "    ax1.barh(y_pos, scores, color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(features)\n",
    "    ax1.set_xlabel('Average Model Accuracy')\n",
    "    ax1.set_title('Top 20 Most Important Features', fontsize=14, pad=20)\n",
    "    ax1.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(scores):\n",
    "        ax1.text(v + 0.005, i, f'{v:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    # 2. Feature Selection Frequency\n",
    "    feature_selection_count = {feature: len(scores) for feature, scores in feature_importance_scores.items()}\n",
    "    sorted_by_frequency = sorted(feature_selection_count.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    \n",
    "    features_freq, counts = zip(*sorted_by_frequency)\n",
    "    \n",
    "    y_pos = np.arange(len(features_freq))\n",
    "    ax2.barh(y_pos, counts, color='lightcoral', edgecolor='darkred', alpha=0.7)\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(features_freq)\n",
    "    ax2.set_xlabel('Selection Frequency')\n",
    "    ax2.set_title('Most Frequently Selected Features', fontsize=14, pad=20)\n",
    "    ax2.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(counts):\n",
    "        ax2.text(v + 0.1, i, f'{v}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Feature Importance Analysis', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(\"\\n=== Feature Importance Summary ===\")\n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    for i, (feature, score) in enumerate(sorted_features[:10], 1):\n",
    "        print(f\"{i:2d}. {feature:<30} | Avg Accuracy: {score:.3f}\")\n",
    "    \n",
    "    print(\"\\n\\nChimerism Features in Top 20:\")\n",
    "    chimerism_features = [f for f, _ in top_features if 'cd3' in f.lower() or 'chimerism' in f.lower()]\n",
    "    print(f\"  - Count: {len(chimerism_features)}/{len(top_features)}\")\n",
    "    print(f\"  - Features: {', '.join(chimerism_features[:5])}...\")\n",
    "    \n",
    "    return feature_importance_avg\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_importance_scores = plot_feature_importance_analysis(classification_results_all, X_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_kmeans_clustering_with_umap(X, n_clusters=4, n_neighbors=15, min_dist=0.1):\n",
    "    \"\"\"\n",
    "    Perform K-Means clustering with UMAP 3D visualization.\n",
    "    \n",
    "    This function:\n",
    "    1. Applies K-Means clustering to the feature space\n",
    "    2. Reduces dimensionality to 3D using UMAP\n",
    "    3. Creates interactive 3D visualization with Plotly\n",
    "    4. Analyzes cluster statistics and quality\n",
    "    \"\"\"\n",
    "    print(f\"üî¨ Performing K-Means clustering with {n_clusters} clusters...\")\n",
    "    \n",
    "    # Apply K-Means clustering\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    print(\"   ‚úÖ K-Means clustering completed\")\n",
    "    \n",
    "    # Calculate cluster statistics\n",
    "    print(\"\\nüìä Cluster Statistics:\")\n",
    "    unique_labels, counts = np.unique(cluster_labels, return_counts=True)\n",
    "    for label, count in zip(unique_labels, counts):\n",
    "        percentage = (count / len(cluster_labels)) * 100\n",
    "        print(f\"   Cluster {label}: {count} samples ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Apply UMAP for 3D visualization\n",
    "    print(f\"\\nüé® Applying UMAP for 3D visualization...\")\n",
    "    print(f\"   Parameters: n_neighbors={n_neighbors}, min_dist={min_dist}\")\n",
    "    \n",
    "    umap_3d = umap.UMAP(\n",
    "        n_components=3,\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        random_state=42,\n",
    "        n_epochs=200\n",
    "    )\n",
    "    \n",
    "    umap_features = umap_3d.fit_transform(X)\n",
    "    print(\"   ‚úÖ UMAP transformation completed\")\n",
    "    \n",
    "    # Create interactive 3D scatter plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Define colors for clusters\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FED766', '#6C5CE7', '#A8E6CF', '#FD79A8', '#FDCB6E']\n",
    "    \n",
    "    # Add scatter plot for each cluster\n",
    "    for i in range(n_clusters):\n",
    "        cluster_mask = cluster_labels == i\n",
    "        \n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=umap_features[cluster_mask, 0],\n",
    "            y=umap_features[cluster_mask, 1],\n",
    "            z=umap_features[cluster_mask, 2],\n",
    "            mode='markers',\n",
    "            name=f'Cluster {i}',\n",
    "            marker=dict(\n",
    "                size=5,\n",
    "                color=colors[i % len(colors)],\n",
    "                opacity=0.8,\n",
    "                line=dict(width=1, color='DarkSlateGrey')\n",
    "            ),\n",
    "            text=[f'Sample {idx}<br>Cluster {i}' for idx in np.where(cluster_mask)[0]],\n",
    "            hovertemplate='%{text}<br>UMAP1: %{x:.2f}<br>UMAP2: %{y:.2f}<br>UMAP3: %{z:.2f}<extra></extra>'\n",
    "        ))\n",
    "    \n",
    "    # Add cluster centers (projected to UMAP space)\n",
    "    cluster_centers_umap = umap_3d.transform(kmeans.cluster_centers_)\n",
    "    \n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=cluster_centers_umap[:, 0],\n",
    "        y=cluster_centers_umap[:, 1],\n",
    "        z=cluster_centers_umap[:, 2],\n",
    "        mode='markers',\n",
    "        name='Cluster Centers',\n",
    "        marker=dict(\n",
    "            size=15,\n",
    "            color='black',\n",
    "            symbol='x',\n",
    "            line=dict(width=3, color='white')\n",
    "        ),\n",
    "        text=[f'Center {i}' for i in range(n_clusters)],\n",
    "        hovertemplate='%{text}<br>UMAP1: %{x:.2f}<br>UMAP2: %{y:.2f}<br>UMAP3: %{z:.2f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'K-Means Clustering Results ({n_clusters} clusters) - 3D UMAP Projection',\n",
    "            font=dict(size=20)\n",
    "        ),\n",
    "        scene=dict(\n",
    "            xaxis_title='UMAP 1',\n",
    "            yaxis_title='UMAP 2',\n",
    "            zaxis_title='UMAP 3',\n",
    "            camera=dict(\n",
    "                eye=dict(x=1.5, y=1.5, z=1.5)\n",
    "            )\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=40),\n",
    "        width=1000,\n",
    "        height=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"v\",\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"left\",\n",
    "            x=1.02\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Show the plot\n",
    "    fig.show()\n",
    "    \n",
    "    # Calculate silhouette score\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(f\"\\nüìà Clustering Quality Metrics:\")\n",
    "    print(f\"   Average Silhouette Score: {silhouette_avg:.3f}\")\n",
    "    print(f\"   Inertia: {kmeans.inertia_:.2f}\")\n",
    "    \n",
    "    # Return results\n",
    "    results = {\n",
    "        'cluster_labels': cluster_labels,\n",
    "        'cluster_centers': kmeans.cluster_centers_,\n",
    "        'umap_features': umap_features,\n",
    "        'cluster_centers_umap': cluster_centers_umap,\n",
    "        'silhouette_score': silhouette_avg,\n",
    "        'inertia': kmeans.inertia_,\n",
    "        'model': kmeans\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Perform K-Means clustering with UMAP\n",
    "print(\"\\n=== K-MEANS CLUSTERING WITH UMAP ===\")\n",
    "kmeans_results = perform_kmeans_clustering_with_umap(X_preprocessed, n_clusters=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Advanced Clustering with K-Means and UMAP\n",
    "\n",
    "Perform advanced clustering analysis with 3D visualization using UMAP dimensionality reduction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
