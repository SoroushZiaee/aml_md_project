{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML/MDS Machine Learning Analysis\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook analyzes AML (Acute Myeloid Leukemia) and MDS (Myelodysplastic Syndrome) patient data to predict transplant outcomes using machine learning techniques. The primary focus is on investigating how dynamic changes in CD3+ chimerism levels at different time points (Day 30, 60, and 100) can predict various transplant outcomes.\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "**Tier 1 (Primary):**\n",
    "- Can dynamic changes of CD3+ chimerism at Day 30, 60, and 100 predict disease relapse?\n",
    "- Do trend patterns (upward, downward, fluctuating) correlate with transplant outcomes?\n",
    "\n",
    "**Tier 2:**\n",
    "- Can CD3+ chimerism dynamics predict other outcomes: overall survival (OS), GVHD, or GRFS?\n",
    "\n",
    "**Tier 3:**\n",
    "- Do interactions between CD3+ chimerism and other biomarkers improve prediction accuracy?\n",
    "\n",
    "### Target Variables\n",
    "- **aGVHD**: Acute graft-versus-host disease\n",
    "- **cGVHD**: Chronic graft-versus-host disease  \n",
    "- **Relapse**: Disease relapse\n",
    "- **Death**: Overall survival\n",
    "- **RFS**: Relapse-free survival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports and Configuration\n",
    "\n",
    "Import all necessary libraries for data processing, machine learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, f_regression, RFE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=\"%(message)s\", force=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration\n",
    "\n",
    "Load the dataset and perform initial data exploration to understand the structure and quality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, sheet_name=\"Sheet1\", skip_rows=2):\n",
    "    \"\"\"\n",
    "    Load dataset from Excel file with proper handling of headers and empty rows.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str\n",
    "        Path to the Excel file\n",
    "    sheet_name : str\n",
    "        Name of the sheet to read\n",
    "    skip_rows : int\n",
    "        Number of rows to skip from the top\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Loaded and cleaned dataset\n",
    "    \"\"\"\n",
    "    logging.info(\"Loading dataset...\")\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name, skiprows=skip_rows)\n",
    "    \n",
    "    # Remove completely empty rows\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    \n",
    "    logging.info(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def map_excel_columns(df):\n",
    "    \"\"\"\n",
    "    Create a mapping from Excel column letters (A, B, C...) to actual column names.\n",
    "    This helps maintain consistency when referencing columns by their Excel positions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Mapping from Excel column letters to column names\n",
    "    \"\"\"\n",
    "    column_mapping = {}\n",
    "    for idx, col_name in enumerate(df.columns):\n",
    "        col_letter = \"\"\n",
    "        col_number = idx + 1\n",
    "        while col_number > 0:\n",
    "            col_number, remainder = divmod(col_number - 1, 26)\n",
    "            col_letter = chr(65 + remainder) + col_letter\n",
    "        column_mapping[col_letter] = col_name\n",
    "    return column_mapping\n",
    "\n",
    "# Load the dataset\n",
    "# Note: Update the file path to match your local environment\n",
    "file_path = \"main_dataset.xlsx\"  # Update this path\n",
    "df = load_data(file_path)\n",
    "excel_column_mapping = map_excel_columns(df)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names (first 10): {list(df.columns[:10])}\")\n",
    "print(f\"\\nExcel column mapping (first 5): {dict(list(excel_column_mapping.items())[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset overview\n",
    "print(\"=== Dataset Overview ===\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"\\nMissing values per column (top 10):\")\n",
    "missing_counts = df.isnull().sum().sort_values(ascending=False)\n",
    "print(missing_counts.head(10))\n",
    "\n",
    "# Display data types\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature and Label Definition\n",
    "\n",
    "Define the feature columns and target variables based on the research objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_labels(df, excel_mapping, feature_cols, class_labels, reg_labels):\n",
    "    \"\"\"\n",
    "    Extract features and labels from the dataset using Excel column mappings.\n",
    "    Filter for AML/MDS patients (Disease == 1) and prepare data for ML pipeline.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataset\n",
    "    excel_mapping : dict\n",
    "        Mapping from Excel columns to actual column names\n",
    "    feature_cols : list\n",
    "        List of Excel column letters for features\n",
    "    class_labels : list\n",
    "        List of Excel column letters for classification targets\n",
    "    reg_labels : list\n",
    "        List of Excel column letters for regression targets\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (X, y_classification, y_regression) dataframes\n",
    "    \"\"\"\n",
    "    # Map Excel column letters to actual column names\n",
    "    features = [excel_mapping[col] for col in feature_cols if col in excel_mapping]\n",
    "    class_names = [excel_mapping[col] for col in class_labels if col in excel_mapping]\n",
    "    reg_names = [excel_mapping[col] for col in reg_labels if col in excel_mapping]\n",
    "\n",
    "    # Filter for AML/MDS patients only\n",
    "    df_filtered = df[df[\"Disease\"] == 1].copy()\n",
    "    \n",
    "    # Extract features and labels\n",
    "    X = df_filtered[features].copy()\n",
    "    y_classification = df_filtered[class_names].copy()\n",
    "    y_regression = df_filtered[reg_names].copy()\n",
    "\n",
    "    # Reset indices\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y_classification.reset_index(drop=True, inplace=True)\n",
    "    y_regression.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    logging.info(f\"Extracted features: {X.shape}\")\n",
    "    logging.info(f\"Classification labels: {y_classification.shape}\")\n",
    "    logging.info(f\"Regression labels: {y_regression.shape}\")\n",
    "    \n",
    "    return X, y_classification, y_regression\n",
    "\n",
    "# Define feature and label columns using Excel column references\n",
    "# These correspond to clinically relevant features identified in the original analysis\n",
    "feature_cols = [\n",
    "    \"E\", \"G\", \"K\", \"M\", \"O\", \"Q\", \"T\", \"U\",           # Patient demographics and disease characteristics\n",
    "    \"BK\", \"BM\", \"BT\", \"BV\", \"BW\",                       # Transplant-related features\n",
    "    \"CB\", \"CC\", \"CJ\", \"CK\", \"CL\", \"CM\", \"CN\", \"CO\", \"CP\", # Treatment and conditioning regimens\n",
    "    \"CT\", \"DB\", \"DD\", \"DF\", \"DH\", \"DK\", \"DL\",          # Chimerism measurements and biomarkers\n",
    "    \"DN\", \"DO\", \"DU\", \"DV\", \"EI\", \"ES\",                # Additional clinical parameters\n",
    "    \"FB\", \"FC\", \"HQ\", \"HR\", \"HS\", \"HT\"                 # Outcome-related features\n",
    "]\n",
    "\n",
    "# Classification target variables (binary outcomes)\n",
    "classification_labels = [\n",
    "    \"FD\",  # aGVHD (Acute Graft-versus-Host Disease)\n",
    "    \"FP\",  # cGVHD (Chronic Graft-versus-Host Disease)\n",
    "    \"GD\",  # Relapse\n",
    "    \"GO\",  # Death\n",
    "    \"GW\"   # RFS (Relapse-Free Survival)\n",
    "]\n",
    "\n",
    "# Regression target variables (continuous outcomes)\n",
    "regression_labels = [\n",
    "    \"GR\",  # Time to event 1\n",
    "    \"GY\"   # Time to event 2\n",
    "]\n",
    "\n",
    "# Extract features and labels\n",
    "X, y_classification, y_regression = extract_features_labels(\n",
    "    df, excel_column_mapping, feature_cols, classification_labels, regression_labels\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Feature and Label Extraction Complete ===\")\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Classification targets shape: {y_classification.shape}\")\n",
    "print(f\"Regression targets shape: {y_regression.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing Pipeline\n",
    "\n",
    "Implement comprehensive data preprocessing including missing value imputation, encoding, and scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(X):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline for features.\n",
    "    \n",
    "    Steps:\n",
    "    1. Handle datetime columns by converting to ordinal\n",
    "    2. Encode categorical variables using LabelEncoder\n",
    "    3. Impute missing values using mean strategy\n",
    "    4. Scale features using StandardScaler\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Preprocessed feature matrix\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting feature preprocessing...\")\n",
    "    X_processed = X.copy()\n",
    "    \n",
    "    # Handle datetime columns\n",
    "    datetime_cols = X_processed.select_dtypes(include=['datetime64']).columns\n",
    "    if len(datetime_cols) > 0:\n",
    "        logging.info(f\"Converting {len(datetime_cols)} datetime columns to ordinal\")\n",
    "        for col in datetime_cols:\n",
    "            X_processed[col] = X_processed[col].apply(\n",
    "                lambda x: x.toordinal() if pd.notnull(x) else np.nan\n",
    "            )\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_cols = X_processed.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        logging.info(f\"Encoding {len(categorical_cols)} categorical columns\")\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            X_processed[col] = le.fit_transform(X_processed[col].astype(str))\n",
    "    \n",
    "    # Impute missing values\n",
    "    missing_count = X_processed.isnull().sum().sum()\n",
    "    if missing_count > 0:\n",
    "        logging.info(f\"Imputing {missing_count} missing values using mean strategy\")\n",
    "        imputer = SimpleImputer(strategy=\"mean\")\n",
    "        X_imputed = pd.DataFrame(\n",
    "            imputer.fit_transform(X_processed), \n",
    "            columns=X_processed.columns,\n",
    "            index=X_processed.index\n",
    "        )\n",
    "    else:\n",
    "        X_imputed = X_processed\n",
    "    \n",
    "    # Scale features\n",
    "    logging.info(\"Scaling features using StandardScaler\")\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_imputed), \n",
    "        columns=X_imputed.columns,\n",
    "        index=X_imputed.index\n",
    "    )\n",
    "    \n",
    "    logging.info(\"Feature preprocessing completed\")\n",
    "    return X_scaled\n",
    "\n",
    "def preprocess_labels(y_classification):\n",
    "    \"\"\"\n",
    "    Preprocess classification labels.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert continuous variables to binary using median split\n",
    "    2. Encode categorical variables\n",
    "    3. Handle missing values\n",
    "    4. Remove constant columns\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_classification : pd.DataFrame\n",
    "        Classification target matrix\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Preprocessed classification targets\n",
    "    \"\"\"\n",
    "    logging.info(\"Preprocessing classification labels...\")\n",
    "    y_processed = y_classification.copy()\n",
    "    \n",
    "    for col in y_processed.columns:\n",
    "        # Convert continuous to binary if needed\n",
    "        if y_processed[col].dtype in ['float64', 'int64']:\n",
    "            y_processed[col] = pd.cut(\n",
    "                y_processed[col], \n",
    "                bins=[-np.inf, 0, np.inf], \n",
    "                labels=[0, 1]\n",
    "            )\n",
    "        \n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        y_processed[col] = le.fit_transform(y_processed[col].astype(str))\n",
    "    \n",
    "    # Handle missing values by forward fill then mode\n",
    "    y_processed.fillna(method='ffill', inplace=True)\n",
    "    if y_processed.isnull().sum().sum() > 0:\n",
    "        y_processed.fillna(y_processed.mode().iloc[0], inplace=True)\n",
    "    \n",
    "    # Remove constant columns\n",
    "    initial_cols = len(y_processed.columns)\n",
    "    y_processed = y_processed.loc[:, y_processed.nunique() > 1]\n",
    "    final_cols = len(y_processed.columns)\n",
    "    \n",
    "    if initial_cols != final_cols:\n",
    "        logging.info(f\"Removed {initial_cols - final_cols} constant columns\")\n",
    "    \n",
    "    logging.info(\"Classification label preprocessing completed\")\n",
    "    return y_processed\n",
    "\n",
    "def preprocess_regression_labels(y_regression):\n",
    "    \"\"\"\n",
    "    Preprocess regression labels.\n",
    "    \n",
    "    Steps:\n",
    "    1. Convert to numeric\n",
    "    2. Impute missing values with median\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_regression : pd.DataFrame\n",
    "        Regression target matrix\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Preprocessed regression targets\n",
    "    \"\"\"\n",
    "    logging.info(\"Preprocessing regression labels...\")\n",
    "    y_processed = y_regression.copy()\n",
    "    \n",
    "    for col in y_processed.columns:\n",
    "        # Convert to numeric\n",
    "        y_processed[col] = pd.to_numeric(y_processed[col], errors='coerce')\n",
    "        \n",
    "        # Impute with median\n",
    "        y_processed[col].fillna(y_processed[col].median(), inplace=True)\n",
    "    \n",
    "    logging.info(\"Regression label preprocessing completed\")\n",
    "    return y_processed\n",
    "\n",
    "# Apply preprocessing\n",
    "X_preprocessed = preprocess_data(X)\n",
    "y_classification_processed = preprocess_labels(y_classification)\n",
    "y_regression_processed = preprocess_regression_labels(y_regression)\n",
    "\n",
    "print(\"\\n=== Preprocessing Summary ===\")\n",
    "print(f\"Preprocessed features shape: {X_preprocessed.shape}\")\n",
    "print(f\"Preprocessed classification targets shape: {y_classification_processed.shape}\")\n",
    "print(f\"Preprocessed regression targets shape: {y_regression_processed.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "print(X_preprocessed.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "Analyze correlations between target variables and visualize data distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable correlations\n",
    "def plot_target_correlations(y_classification):\n",
    "    \"\"\"\n",
    "    Plot correlation matrix for classification targets to understand relationships\n",
    "    between different outcomes.\n",
    "    \"\"\"\n",
    "    correlation_matrix = y_classification.corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        correlation_matrix, \n",
    "        annot=True, \n",
    "        cmap='coolwarm', \n",
    "        fmt=\".3f\", \n",
    "        annot_kws={\"size\": 12}, \n",
    "        linewidths=0.5,\n",
    "        center=0\n",
    "    )\n",
    "    plt.title(\"Target Variable Correlation Matrix\", fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print correlation insights\n",
    "    print(\"\\n=== Target Correlation Insights ===\")\n",
    "    print(correlation_matrix)\n",
    "    \n",
    "    # Find strongest correlations (excluding diagonal)\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    corr_values = correlation_matrix.mask(mask).stack().reset_index()\n",
    "    corr_values.columns = ['Variable 1', 'Variable 2', 'Correlation']\n",
    "    corr_values = corr_values.sort_values('Correlation', key=abs, ascending=False)\n",
    "    \n",
    "    print(\"\\nStrongest correlations between targets:\")\n",
    "    print(corr_values.head())\n",
    "\n",
    "plot_target_correlations(y_classification_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class distributions\n",
    "def analyze_class_distributions(y_classification):\n",
    "    \"\"\"\n",
    "    Analyze the distribution of classes in each target variable.\n",
    "    \"\"\"\n",
    "    target_names = [\"aGVHD\", \"cGVHD\", \"Relapse\", \"Death\", \"RFS\"]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(y_classification.columns), figsize=(20, 4))\n",
    "    \n",
    "    for i, (col, name) in enumerate(zip(y_classification.columns, target_names)):\n",
    "        counts = y_classification[col].value_counts().sort_index()\n",
    "        \n",
    "        axes[i].bar(counts.index, counts.values, color=['lightcoral', 'lightblue'])\n",
    "        axes[i].set_title(f'{name}\\n(0: No, 1: Yes)', fontsize=12)\n",
    "        axes[i].set_xlabel('Class')\n",
    "        axes[i].set_ylabel('Count')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for j, v in enumerate(counts.values):\n",
    "            axes[i].text(j, v + 0.5, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    plt.suptitle('Target Variable Class Distributions', fontsize=16, y=1.05)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print distribution statistics\n",
    "    print(\"\\n=== Class Distribution Summary ===\")\n",
    "    for i, (col, name) in enumerate(zip(y_classification.columns, target_names)):\n",
    "        counts = y_classification[col].value_counts().sort_index()\n",
    "        total = counts.sum()\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  No (0): {counts[0]} ({counts[0]/total:.1%})\")\n",
    "        print(f\"  Yes (1): {counts[1]} ({counts[1]/total:.1%})\")\n",
    "        print()\n",
    "\n",
    "analyze_class_distributions(y_classification_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection and Optimization\n",
    "\n",
    "Implement advanced feature selection methods with hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_feature_selection(X, y, task=\"classification\", method=\"select_k_best\"):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for feature selection methods.\n",
    "    \n",
    "    This function automatically finds the optimal number of features to select\n",
    "    by testing different values and choosing the one that gives the best\n",
    "    cross-validation performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix\n",
    "    y : pd.Series\n",
    "        Target variable\n",
    "    task : str\n",
    "        'classification' or 'regression'\n",
    "    method : str\n",
    "        Feature selection method: 'select_k_best', 'rfe', or 'feature_importance'\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Selected features matrix\n",
    "    \"\"\"\n",
    "    # Choose appropriate scoring function and model\n",
    "    if task == \"classification\":\n",
    "        score_func = f_classif\n",
    "        model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "        scoring = 'accuracy'\n",
    "    else:\n",
    "        score_func = f_regression\n",
    "        model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "        scoring = 'neg_mean_squared_error'\n",
    "    \n",
    "    if method == \"select_k_best\":\n",
    "        # Test different values of k\n",
    "        param_grid = {'k': [5, 10, 15, 20, min(25, X.shape[1])]}\n",
    "        best_k = param_grid['k'][0]\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        for k in param_grid['k']:\n",
    "            if k > X.shape[1]:\n",
    "                continue\n",
    "                \n",
    "            selector = SelectKBest(score_func=score_func, k=k)\n",
    "            X_selected = selector.fit_transform(X, y)\n",
    "            \n",
    "            # Cross-validation score\n",
    "            scores = cross_val_score(model, X_selected, y, cv=5, scoring=scoring)\n",
    "            score = np.mean(scores)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_k, best_score = k, score\n",
    "        \n",
    "        # Return best selection\n",
    "        final_selector = SelectKBest(score_func=score_func, k=best_k)\n",
    "        return final_selector.fit_transform(X, y)\n",
    "    \n",
    "    elif method == \"rfe\":\n",
    "        # Test different numbers of features\n",
    "        param_grid = {'n_features': [5, 10, 15, 20, min(25, X.shape[1])]}\n",
    "        best_n = param_grid['n_features'][0]\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        for n in param_grid['n_features']:\n",
    "            if n > X.shape[1]:\n",
    "                continue\n",
    "                \n",
    "            selector = RFE(model, n_features_to_select=n)\n",
    "            X_selected = selector.fit_transform(X, y)\n",
    "            \n",
    "            scores = cross_val_score(model, X_selected, y, cv=5, scoring=scoring)\n",
    "            score = np.mean(scores)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_n, best_score = n, score\n",
    "        \n",
    "        final_selector = RFE(model, n_features_to_select=best_n)\n",
    "        return final_selector.fit_transform(X, y)\n",
    "    \n",
    "    elif method == \"feature_importance\":\n",
    "        # Test different importance thresholds\n",
    "        model.fit(X, y)\n",
    "        importance_scores = model.feature_importances_\n",
    "        thresholds = np.linspace(0.005, 0.05, 5)\n",
    "        \n",
    "        best_threshold = thresholds[0]\n",
    "        best_score = -np.inf\n",
    "        \n",
    "        for threshold in thresholds:\n",
    "            selected_features = X.columns[importance_scores > threshold]\n",
    "            \n",
    "            if len(selected_features) == 0:\n",
    "                continue\n",
    "                \n",
    "            X_selected = X[selected_features]\n",
    "            scores = cross_val_score(model, X_selected, y, cv=5, scoring=scoring)\n",
    "            score = np.mean(scores)\n",
    "            \n",
    "            if score > best_score:\n",
    "                best_threshold, best_score = threshold, score\n",
    "        \n",
    "        selected_features = X.columns[importance_scores > best_threshold]\n",
    "        return X[selected_features].values\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose from 'select_k_best', 'rfe', or 'feature_importance'.\")\n",
    "\n",
    "# Test feature selection on a sample target\n",
    "print(\"=== Testing Feature Selection Methods ===\")\n",
    "sample_target = y_classification_processed.iloc[:, 0]  # First target (aGVHD)\n",
    "\n",
    "for method in ['select_k_best', 'rfe', 'feature_importance']:\n",
    "    print(f\"\\nTesting {method}...\")\n",
    "    X_selected = optimize_feature_selection(X_preprocessed, sample_target, method=method)\n",
    "    print(f\"Selected {X_selected.shape[1]} features out of {X_preprocessed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine Learning Model Training\n",
    "\n",
    "Implement comprehensive ML pipeline with multiple algorithms and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_models(X, y_classification, feature_selection_method=\"select_k_best\", cv=5):\n",
    "    \"\"\"\n",
    "    Train classification models with cross-validation and feature selection for each target.\n",
    "    \n",
    "    This function:\n",
    "    1. Applies feature selection for each target independently\n",
    "    2. Performs hyperparameter tuning using GridSearchCV\n",
    "    3. Evaluates models using cross-validation\n",
    "    4. Returns comprehensive results for all targets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix\n",
    "    y_classification : pd.DataFrame\n",
    "        Classification targets matrix\n",
    "    feature_selection_method : str\n",
    "        Method for feature selection\n",
    "    cv : int\n",
    "        Number of cross-validation folds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Nested dictionary with results for each target and model\n",
    "    \"\"\"\n",
    "    # Define hyperparameter grids\n",
    "    param_grids = {\n",
    "        \"Random Forest\": {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        },\n",
    "        \"SVM\": {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['rbf', 'linear'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        },\n",
    "        \"Naive Bayes\": {}\n",
    "    }\n",
    "    \n",
    "    # Initialize classifiers\n",
    "    classifiers = {\n",
    "        \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "        \"SVM\": SVC(random_state=42),\n",
    "        \"Naive Bayes\": GaussianNB()\n",
    "    }\n",
    "    \n",
    "    target_names = [\"aGVHD\", \"cGVHD\", \"Relapse\", \"Death\", \"RFS\"]\n",
    "    classification_results = {}\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    for idx, (label, target_name) in enumerate(zip(y_classification.columns, target_names)):\n",
    "        print(f\"\\nðŸ”¹ Training models for {target_name} ({label}) ðŸ”¹\")\n",
    "        \n",
    "        # Get target variable and ensure no missing values\n",
    "        y_label = y_classification[label].dropna()\n",
    "        X_label = X.loc[y_label.index]\n",
    "        \n",
    "        # Check if we have multiple classes\n",
    "        if len(y_label.unique()) <= 1:\n",
    "            print(f\"âš ï¸ Skipping {target_name}: Only one class present\")\n",
    "            continue\n",
    "        \n",
    "        # Apply feature selection\n",
    "        print(f\"Applying {feature_selection_method} feature selection...\")\n",
    "        X_selected = optimize_feature_selection(\n",
    "            X_label, y_label, task=\"classification\", method=feature_selection_method\n",
    "        )\n",
    "        print(f\"Selected {X_selected.shape[1]} features\")\n",
    "        \n",
    "        classification_results[target_name] = {}\n",
    "\n",
    "        for model_name, model in classifiers.items():\n",
    "            print(f\"  Training {model_name}...\")\n",
    "            \n",
    "            # Hyperparameter tuning\n",
    "            if param_grids[model_name]:  # If there are parameters to tune\n",
    "                grid_search = GridSearchCV(\n",
    "                    model, param_grids[model_name], cv=3, scoring='accuracy', n_jobs=-1\n",
    "                )\n",
    "                grid_search.fit(X_selected, y_label)\n",
    "                best_model = grid_search.best_estimator_\n",
    "                best_params = grid_search.best_params_\n",
    "            else:\n",
    "                # For models without hyperparameters (like Naive Bayes)\n",
    "                best_model = model\n",
    "                best_params = {}\n",
    "                best_model.fit(X_selected, y_label)\n",
    "            \n",
    "            # Cross-validation evaluation\n",
    "            accuracy_scores = cross_val_score(\n",
    "                best_model, X_selected, y_label, cv=kf, scoring='accuracy'\n",
    "            )\n",
    "            f1_scores = cross_val_score(\n",
    "                best_model, X_selected, y_label, cv=kf, scoring='f1_macro'\n",
    "            )\n",
    "            \n",
    "            # Generate predictions for detailed report\n",
    "            y_pred = cross_val_predict(best_model, X_selected, y_label, cv=kf)\n",
    "            \n",
    "            # Store results\n",
    "            classification_results[target_name][model_name] = {\n",
    "                \"best_model_params\": best_params,\n",
    "                \"accuracy\": np.mean(accuracy_scores),\n",
    "                \"accuracy_std\": np.std(accuracy_scores),\n",
    "                \"f1_score\": np.mean(f1_scores),\n",
    "                \"f1_std\": np.std(f1_scores),\n",
    "                \"report\": classification_report(y_label, y_pred, output_dict=True),\n",
    "                \"n_features\": X_selected.shape[1]\n",
    "            }\n",
    "            \n",
    "            print(f\"    Accuracy: {np.mean(accuracy_scores):.3f} (Â±{np.std(accuracy_scores):.3f})\")\n",
    "            print(f\"    F1-Score: {np.mean(f1_scores):.3f} (Â±{np.std(f1_scores):.3f})\")\n",
    "\n",
    "    return classification_results\n",
    "\n",
    "def train_regression_models(X, y_regression, feature_selection_method=\"select_k_best\", cv=5):\n",
    "    \"\"\"\n",
    "    Train regression models with cross-validation and feature selection for each target.\n",
    "    \"\"\"\n",
    "    regressors = {\n",
    "        \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "        \"SVM\": SVR()\n",
    "    }\n",
    "\n",
    "    regression_results = {}\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    for label in y_regression.columns:\n",
    "        print(f\"\\nðŸ”¹ Training regression models for {label} ðŸ”¹\")\n",
    "        \n",
    "        y_label = y_regression[label].dropna()\n",
    "        X_label = X.loc[y_label.index]\n",
    "\n",
    "        # Apply feature selection\n",
    "        X_selected = optimize_feature_selection(\n",
    "            X_label, y_label, task=\"regression\", method=feature_selection_method\n",
    "        )\n",
    "\n",
    "        regression_results[label] = {}\n",
    "\n",
    "        for model_name, model in regressors.items():\n",
    "            print(f\"  Training {model_name}...\")\n",
    "            \n",
    "            # Cross-validated predictions\n",
    "            y_pred = cross_val_predict(model, X_selected, y_label, cv=kf)\n",
    "\n",
    "            # Compute metrics\n",
    "            percentage_errors = np.abs((y_pred - y_label) / y_label) * 100\n",
    "            mean_percentage_error = np.mean(percentage_errors)\n",
    "            mae = mean_absolute_error(y_label, y_pred)\n",
    "            mse = mean_squared_error(y_label, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "\n",
    "            regression_results[label][model_name] = {\n",
    "                \"mae\": mae,\n",
    "                \"mse\": mse,\n",
    "                \"rmse\": rmse,\n",
    "                \"percentage_error\": mean_percentage_error,\n",
    "                \"n_features\": X_selected.shape[1]\n",
    "            }\n",
    "            \n",
    "            print(f\"    MAE: {mae:.3f}\")\n",
    "            print(f\"    RMSE: {rmse:.3f}\")\n",
    "            print(f\"    Mean % Error: {mean_percentage_error:.1f}%\")\n",
    "\n",
    "    return regression_results\n",
    "\n",
    "print(\"=== Model Training Functions Defined ===\")\n",
    "print(\"Ready to train models with different feature selection methods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Training and Evaluation\n",
    "\n",
    "Train models using different feature selection methods and compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with different feature selection methods\n",
    "feature_selection_methods = [\"select_k_best\", \"rfe\", \"feature_importance\"]\n",
    "classification_results_all = {}\n",
    "regression_results_all = {}\n",
    "\n",
    "for method in feature_selection_methods:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ”¹ RUNNING FEATURE SELECTION WITH {method.upper()} ðŸ”¹\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train Classification Models\n",
    "    classification_results = train_classification_models(\n",
    "        X_preprocessed, y_classification_processed, \n",
    "        feature_selection_method=method, cv=5\n",
    "    )\n",
    "\n",
    "    # Train Regression Models (if regression targets exist)\n",
    "    if len(y_regression_processed.columns) > 0:\n",
    "        regression_results = train_regression_models(\n",
    "            X_preprocessed, y_regression_processed, \n",
    "            feature_selection_method=method, cv=5\n",
    "        )\n",
    "        regression_results_all[method] = regression_results\n",
    "\n",
    "    # Store Results\n",
    "    classification_results_all[method] = classification_results\n",
    "\n",
    "    print(f\"\\nâœ… COMPLETED FEATURE SELECTION WITH {method.upper()} âœ…\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ðŸŽ‰ ALL MODEL TRAINING COMPLETED ðŸŽ‰\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Visualization and Analysis\n",
    "\n",
    "Create comprehensive visualizations to compare model performance across different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_results(results):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of classification results.\n",
    "    \n",
    "    Plots accuracy and F1-scores for all feature selection methods and models\n",
    "    across all target variables with error bars showing standard deviation.\n",
    "    \"\"\"\n",
    "    methods = list(results.keys())\n",
    "    classifiers = [\"Random Forest\", \"SVM\", \"Naive Bayes\"]\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 8))\n",
    "    \n",
    "    num_methods = len(methods)\n",
    "    \n",
    "    # Get all unique targets across all methods\n",
    "    all_targets = set()\n",
    "    for method in methods:\n",
    "        all_targets.update(results[method].keys())\n",
    "    all_targets = sorted(list(all_targets))\n",
    "    \n",
    "    x = np.arange(len(all_targets))\n",
    "    width = 0.25 / num_methods\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(methods) * len(classifiers)))\n",
    "    color_idx = 0\n",
    "    \n",
    "    for method_idx, method in enumerate(methods):\n",
    "        for model_idx, model in enumerate(classifiers):\n",
    "            accuracies = []\n",
    "            accuracy_stds = []\n",
    "            f1_scores = []\n",
    "            f1_stds = []\n",
    "            \n",
    "            for target in all_targets:\n",
    "                if target in results[method] and model in results[method][target]:\n",
    "                    accuracies.append(results[method][target][model][\"accuracy\"])\n",
    "                    accuracy_stds.append(results[method][target][model][\"accuracy_std\"])\n",
    "                    f1_scores.append(results[method][target][model][\"f1_score\"])\n",
    "                    f1_stds.append(results[method][target][model][\"f1_std\"])\n",
    "                else:\n",
    "                    accuracies.append(0)\n",
    "                    accuracy_stds.append(0)\n",
    "                    f1_scores.append(0)\n",
    "                    f1_stds.append(0)\n",
    "            \n",
    "            position = x + (method_idx * len(classifiers) + model_idx) * width\n",
    "            \n",
    "            # Plot accuracy\n",
    "            ax1.bar(position, accuracies, width=width, \n",
    "                   label=f\"{method.replace('_', ' ').title()} - {model}\",\n",
    "                   color=colors[color_idx], alpha=0.8)\n",
    "            ax1.errorbar(position, accuracies, yerr=accuracy_stds, \n",
    "                        fmt='none', color='black', capsize=2, linewidth=1)\n",
    "            \n",
    "            # Plot F1 scores\n",
    "            ax2.bar(position, f1_scores, width=width, \n",
    "                   label=f\"{method.replace('_', ' ').title()} - {model}\",\n",
    "                   color=colors[color_idx], alpha=0.8)\n",
    "            ax2.errorbar(position, f1_scores, yerr=f1_stds, \n",
    "                        fmt='none', color='black', capsize=2, linewidth=1)\n",
    "            \n",
    "            color_idx += 1\n",
    "    \n",
    "    # Customize accuracy subplot\n",
    "    ax1.set_xticks(x + (num_methods * len(classifiers) * width) / 2)\n",
    "    ax1.set_xticklabels(all_targets, rotation=0)\n",
    "    ax1.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.set_title('Classification Accuracy Across Feature Selection Methods', fontsize=14, pad=20)\n",
    "    ax1.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    \n",
    "    # Customize F1 score subplot\n",
    "    ax2.set_xticks(x + (num_methods * len(classifiers) * width) / 2)\n",
    "    ax2.set_xticklabels(all_targets, rotation=0)\n",
    "    ax2.set_ylabel('F1 Score', fontsize=12)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_title('F1 Scores Across Feature Selection Methods', fontsize=14, pad=20)\n",
    "    ax2.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    handles, labels = ax1.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='center right', bbox_to_anchor=(1.15, 0.5), fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Model Performance Comparison', fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(right=0.85)\n",
    "    plt.show()\n",
    "\n",
    "def plot_regression_results(results):\n",
    "    \"\"\"\n",
    "    Create visualization of regression results showing percentage errors.\n",
    "    \"\"\"\n",
    "    if not results:\n",
    "        print(\"No regression results to plot.\")\n",
    "        return\n",
    "        \n",
    "    methods = list(results.keys())\n",
    "    regressors = [\"Random Forest\", \"SVM\"]\n",
    "    num_methods = len(methods)\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    # Get all targets\n",
    "    all_targets = set()\n",
    "    for method in methods:\n",
    "        all_targets.update(results[method].keys())\n",
    "    all_targets = sorted(list(all_targets))\n",
    "    \n",
    "    x = np.arange(len(all_targets))\n",
    "    width = 0.35 / num_methods\n",
    "\n",
    "    for method_idx, method in enumerate(methods):\n",
    "        for model_idx, model in enumerate(regressors):\n",
    "            percentage_errors = []\n",
    "            for target in all_targets:\n",
    "                if (target in results[method] and \n",
    "                    model in results[method][target]):\n",
    "                    percentage_errors.append(\n",
    "                        results[method][target][model][\"percentage_error\"]\n",
    "                    )\n",
    "                else:\n",
    "                    percentage_errors.append(0)\n",
    "            \n",
    "            position = x + (method_idx * len(regressors) + model_idx) * width\n",
    "            plt.bar(position, percentage_errors, width=width, \n",
    "                   label=f\"{method.replace('_', ' ').title()} - {model}\",\n",
    "                   alpha=0.8)\n",
    "\n",
    "    plt.xticks(x + (num_methods * len(regressors) * width) / 2, all_targets)\n",
    "    plt.ylabel('Mean Percentage Error (%)', fontsize=12)\n",
    "    plt.title('Regression Performance: Mean Percentage Error', fontsize=14, pad=20)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "print(\"\\n=== VISUALIZING RESULTS ===\")\n",
    "plot_classification_results(classification_results_all)\n",
    "\n",
    "if regression_results_all:\n",
    "    plot_regression_results(regression_results_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Importance Analysis\n",
    "\n",
    "Analyze which features are most important for predicting each outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(X, y, task=\"classification\", method=\"feature_importance\", n_components=15):\n",
    "    \"\"\"\n",
    "    Plot feature importance using Random Forest or PCA.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix\n",
    "    y : pd.Series\n",
    "        Target variable\n",
    "    task : str\n",
    "        'classification' or 'regression'\n",
    "    method : str\n",
    "        'feature_importance' or 'pca'\n",
    "    n_components : int\n",
    "        Number of top features to display\n",
    "    \"\"\"\n",
    "    if task == \"classification\":\n",
    "        model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "    else:\n",
    "        model = RandomForestRegressor(random_state=42, n_estimators=100)\n",
    "\n",
    "    if method == \"feature_importance\":\n",
    "        model.fit(X, y)\n",
    "        importance_scores = model.feature_importances_\n",
    "        sorted_indices = np.argsort(importance_scores)[::-1][:n_components]\n",
    "        sorted_features = X.columns[sorted_indices]\n",
    "        sorted_scores = importance_scores[sorted_indices]\n",
    "        xlabel = \"Feature Importance\"\n",
    "        title_suffix = \"Random Forest Feature Importance\"\n",
    "        \n",
    "    elif method == \"pca\":\n",
    "        pca = PCA(n_components=min(n_components, X.shape[1]))\n",
    "        pca.fit(X)\n",
    "        \n",
    "        # Get feature weights for first few components\n",
    "        feature_weights = np.abs(pca.components_).sum(axis=0)\n",
    "        sorted_indices = np.argsort(feature_weights)[::-1][:n_components]\n",
    "        sorted_features = X.columns[sorted_indices]\n",
    "        sorted_scores = feature_weights[sorted_indices]\n",
    "        xlabel = \"PCA Component Weight\"\n",
    "        title_suffix = \"PCA Feature Weights\"\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Invalid method. Choose 'feature_importance' or 'pca'.\")\n",
    "    \n",
    "    # Create plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    bars = plt.barh(range(len(sorted_features)), sorted_scores, color='steelblue', alpha=0.8)\n",
    "    plt.yticks(range(len(sorted_features)), sorted_features)\n",
    "    plt.xlabel(xlabel, fontsize=12)\n",
    "    plt.ylabel(\"Features\", fontsize=12)\n",
    "    plt.title(f\"Top {len(sorted_features)} Features - {title_suffix}\", fontsize=14, pad=20)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, score) in enumerate(zip(bars, sorted_scores)):\n",
    "        plt.text(score + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "                f'{score:.3f}', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance_comparison(X, y_list, target_names, method=\"feature_importance\", n_components=10):\n",
    "    \"\"\"\n",
    "    Compare feature importance across multiple targets in subplots.\n",
    "    \"\"\"\n",
    "    num_plots = len(y_list)\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(6 * num_plots, 8), sharey=True)\n",
    "\n",
    "    if num_plots == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, (y, target_name, ax) in enumerate(zip(y_list, target_names, axes)):\n",
    "        model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "        \n",
    "        if method == \"feature_importance\":\n",
    "            model.fit(X, y)\n",
    "            importance_scores = model.feature_importances_\n",
    "            sorted_indices = np.argsort(importance_scores)[::-1][:n_components]\n",
    "            sorted_features = X.columns[sorted_indices]\n",
    "            sorted_scores = importance_scores[sorted_indices]\n",
    "            xlabel = \"Importance\"\n",
    "            \n",
    "        elif method == \"pca\":\n",
    "            pca = PCA(n_components=min(n_components, X.shape[1]))\n",
    "            pca.fit(X)\n",
    "            feature_weights = np.abs(pca.components_).sum(axis=0)\n",
    "            sorted_indices = np.argsort(feature_weights)[::-1][:n_components]\n",
    "            sorted_features = X.columns[sorted_indices]\n",
    "            sorted_scores = feature_weights[sorted_indices]\n",
    "            xlabel = \"PCA Weight\"\n",
    "\n",
    "        # Plot\n",
    "        bars = ax.barh(range(len(sorted_features)), sorted_scores, color='steelblue', alpha=0.8)\n",
    "        ax.set_yticks(range(len(sorted_features)))\n",
    "        ax.set_yticklabels(sorted_features, fontsize=9)\n",
    "        ax.set_xlabel(xlabel, fontsize=10)\n",
    "        ax.set_title(f\"{target_name}\", fontsize=12, pad=10)\n",
    "        ax.grid(axis='x', linestyle='--', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, score in zip(bars, sorted_scores):\n",
    "            ax.text(score + max(sorted_scores) * 0.01, bar.get_y() + bar.get_height()/2, \n",
    "                   f'{score:.3f}', va='center', fontsize=8)\n",
    "\n",
    "    plt.suptitle(f'Feature Importance Comparison - {method.replace(\"_\", \" \").title()}', \n",
    "                 fontsize=16, y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Generate feature importance plots\n",
    "print(\"\\n=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "# Overall PCA analysis\n",
    "print(\"\\nOverall PCA Feature Analysis:\")\n",
    "plot_feature_importance(X_preprocessed, y_classification_processed.iloc[:, 0], \n",
    "                       method=\"pca\", n_components=15)\n",
    "\n",
    "# Individual target analysis\n",
    "target_names = [\"aGVHD\", \"cGVHD\", \"Relapse\", \"Death\", \"RFS\"]\n",
    "available_targets = []\n",
    "available_names = []\n",
    "\n",
    "for i, name in enumerate(target_names):\n",
    "    if i < len(y_classification_processed.columns):\n",
    "        available_targets.append(y_classification_processed.iloc[:, i])\n",
    "        available_names.append(name)\n",
    "\n",
    "if available_targets:\n",
    "    print(\"\\nFeature Importance Comparison Across Targets:\")\n",
    "    plot_feature_importance_comparison(\n",
    "        X_preprocessed, available_targets, available_names, \n",
    "        method=\"feature_importance\", n_components=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Summary and Best Models\n",
    "\n",
    "Summarize the best performing models and provide actionable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_best_models(classification_results):\n",
    "    \"\"\"\n",
    "    Create a summary table of the best performing models for each target.\n",
    "    \"\"\"\n",
    "    summary_data = []\n",
    "    \n",
    "    for method, method_results in classification_results.items():\n",
    "        for target, target_results in method_results.items():\n",
    "            for model, model_results in target_results.items():\n",
    "                summary_data.append({\n",
    "                    'Feature_Selection': method,\n",
    "                    'Target': target,\n",
    "                    'Model': model,\n",
    "                    'Accuracy': model_results['accuracy'],\n",
    "                    'Accuracy_Std': model_results['accuracy_std'],\n",
    "                    'F1_Score': model_results['f1_score'],\n",
    "                    'F1_Std': model_results['f1_std'],\n",
    "                    'N_Features': model_results['n_features']\n",
    "                })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Find best model for each target\n",
    "    print(\"=== BEST PERFORMING MODELS BY TARGET ===\")\n",
    "    print()\n",
    "    \n",
    "    for target in summary_df['Target'].unique():\n",
    "        target_data = summary_df[summary_df['Target'] == target]\n",
    "        best_model = target_data.loc[target_data['Accuracy'].idxmax()]\n",
    "        \n",
    "        print(f\"ðŸŽ¯ {target}:\")\n",
    "        print(f\"   Best Model: {best_model['Model']}\")\n",
    "        print(f\"   Feature Selection: {best_model['Feature_Selection']}\")\n",
    "        print(f\"   Accuracy: {best_model['Accuracy']:.3f} (Â±{best_model['Accuracy_Std']:.3f})\")\n",
    "        print(f\"   F1-Score: {best_model['F1_Score']:.3f} (Â±{best_model['F1_Std']:.3f})\")\n",
    "        print(f\"   Features Used: {best_model['N_Features']}\")\n",
    "        print()\n",
    "    \n",
    "    # Create summary table\n",
    "    print(\"\\n=== COMPLETE RESULTS SUMMARY ===\")\n",
    "    summary_pivot = summary_df.pivot_table(\n",
    "        index=['Target', 'Feature_Selection'], \n",
    "        columns='Model', \n",
    "        values='Accuracy', \n",
    "        aggfunc='mean'\n",
    "    ).round(3)\n",
    "    \n",
    "    print(summary_pivot)\n",
    "    \n",
    "    return summary_df\n",
    "\n",
    "def generate_insights(classification_results, summary_df):\n",
    "    \"\"\"\n",
    "    Generate actionable insights from the model results.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ðŸ” KEY INSIGHTS AND RECOMMENDATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Best overall feature selection method\n",
    "    method_performance = summary_df.groupby('Feature_Selection')['Accuracy'].mean().sort_values(ascending=False)\n",
    "    best_method = method_performance.index[0]\n",
    "    print(f\"\\n1. ðŸ“Š FEATURE SELECTION PERFORMANCE:\")\n",
    "    print(f\"   Best overall method: {best_method} (avg accuracy: {method_performance.iloc[0]:.3f})\")\n",
    "    for method, score in method_performance.items():\n",
    "        print(f\"   {method}: {score:.3f}\")\n",
    "    \n",
    "    # Best overall model\n",
    "    model_performance = summary_df.groupby('Model')['Accuracy'].mean().sort_values(ascending=False)\n",
    "    best_model = model_performance.index[0]\n",
    "    print(f\"\\n2. ðŸ¤– MODEL PERFORMANCE:\")\n",
    "    print(f\"   Best overall model: {best_model} (avg accuracy: {model_performance.iloc[0]:.3f})\")\n",
    "    for model, score in model_performance.items():\n",
    "        print(f\"   {model}: {score:.3f}\")\n",
    "    \n",
    "    # Target difficulty analysis\n",
    "    target_performance = summary_df.groupby('Target')['Accuracy'].mean().sort_values(ascending=False)\n",
    "    print(f\"\\n3. ðŸŽ¯ TARGET PREDICTION DIFFICULTY:\")\n",
    "    print(\"   From easiest to hardest to predict:\")\n",
    "    for i, (target, score) in enumerate(target_performance.items(), 1):\n",
    "        difficulty = \"Easy\" if score > 0.8 else \"Moderate\" if score > 0.7 else \"Challenging\"\n",
    "        print(f\"   {i}. {target}: {score:.3f} ({difficulty})\")\n",
    "    \n",
    "    # Feature efficiency analysis\n",
    "    feature_efficiency = summary_df.groupby('N_Features')['Accuracy'].mean().sort_values(ascending=False)\n",
    "    print(f\"\\n4. ðŸ“ˆ FEATURE EFFICIENCY:\")\n",
    "    print(\"   Performance by number of features:\")\n",
    "    for n_features, accuracy in feature_efficiency.head().items():\n",
    "        print(f\"   {n_features} features: {accuracy:.3f} accuracy\")\n",
    "    \n",
    "    # Clinical recommendations\n",
    "    print(f\"\\n5. ðŸ¥ CLINICAL RECOMMENDATIONS:\")\n",
    "    print(f\"   â€¢ Focus on {target_performance.index[0]} prediction (highest accuracy: {target_performance.iloc[0]:.3f})\")\n",
    "    print(f\"   â€¢ Use {best_method.replace('_', ' ')} for feature selection\")\n",
    "    print(f\"   â€¢ {best_model} shows most consistent performance across targets\")\n",
    "    print(f\"   â€¢ Consider ensemble methods combining top performers\")\n",
    "    \n",
    "    if target_performance.iloc[-1] < 0.7:\n",
    "        hardest_target = target_performance.index[-1]\n",
    "        print(f\"   â€¢ {hardest_target} prediction needs additional biomarkers or longer follow-up\")\n",
    "\n",
    "# Generate comprehensive analysis\n",
    "if classification_results_all:\n",
    "    summary_df = summarize_best_models(classification_results_all)\n",
    "    generate_insights(classification_results_all, summary_df)\n",
    "else:\n",
    "    print(\"No classification results available for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export Results and Models\n",
    "\n",
    "Save the best performing models and create exportable results summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_best_models(classification_results, X_preprocessed, y_classification_processed, save_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Train and save the best performing models for each target.\n",
    "    \"\"\"\n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    target_names = [\"aGVHD\", \"cGVHD\", \"Relapse\", \"Death\", \"RFS\"]\n",
    "    saved_models = {}\n",
    "    \n",
    "    print(f\"\\n=== SAVING BEST MODELS TO {save_dir}/ ===\")\n",
    "    \n",
    "    for method, method_results in classification_results.items():\n",
    "        for target, target_results in method_results.items():\n",
    "            # Find best model for this target\n",
    "            best_accuracy = 0\n",
    "            best_model_info = None\n",
    "            \n",
    "            for model_name, model_results in target_results.items():\n",
    "                if model_results['accuracy'] > best_accuracy:\n",
    "                    best_accuracy = model_results['accuracy']\n",
    "                    best_model_info = {\n",
    "                        'model_name': model_name,\n",
    "                        'method': method,\n",
    "                        'params': model_results['best_model_params'],\n",
    "                        'accuracy': model_results['accuracy'],\n",
    "                        'n_features': model_results['n_features']\n",
    "                    }\n",
    "            \n",
    "            if best_model_info:\n",
    "                # Train the best model on full data\n",
    "                target_idx = list(method_results.keys()).index(target)\n",
    "                y_target = y_classification_processed.iloc[:, target_idx]\n",
    "                \n",
    "                # Apply same feature selection\n",
    "                X_selected = optimize_feature_selection(\n",
    "                    X_preprocessed, y_target, task=\"classification\", method=method\n",
    "                )\n",
    "                \n",
    "                # Create and train model\n",
    "                if best_model_info['model_name'] == 'Random Forest':\n",
    "                    model = RandomForestClassifier(random_state=42, **best_model_info['params'])\n",
    "                elif best_model_info['model_name'] == 'SVM':\n",
    "                    model = SVC(random_state=42, **best_model_info['params'])\n",
    "                else:  # Naive Bayes\n",
    "                    model = GaussianNB()\n",
    "                \n",
    "                model.fit(X_selected, y_target)\n",
    "                \n",
    "                # Save model\n",
    "                model_filename = f\"{save_dir}/best_{target.lower().replace(' ', '_')}_model.joblib\"\n",
    "                joblib.dump({\n",
    "                    'model': model,\n",
    "                    'feature_selection_method': method,\n",
    "                    'model_name': best_model_info['model_name'],\n",
    "                    'accuracy': best_model_info['accuracy'],\n",
    "                    'n_features': best_model_info['n_features'],\n",
    "                    'target': target,\n",
    "                    'trained_on': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "                }, model_filename)\n",
    "                \n",
    "                saved_models[target] = {\n",
    "                    'filename': model_filename,\n",
    "                    'info': best_model_info\n",
    "                }\n",
    "                \n",
    "                print(f\"âœ… {target}: {best_model_info['model_name']} (acc: {best_accuracy:.3f})\")\n",
    "    \n",
    "    return saved_models\n",
    "\n",
    "def export_results_summary(summary_df, classification_results, filename=\"aml_mds_results_summary.xlsx\"):\n",
    "    \"\"\"\n",
    "    Export comprehensive results to Excel file.\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== EXPORTING RESULTS TO {filename} ===\")\n",
    "    \n",
    "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "        # Summary sheet\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        # Best models sheet\n",
    "        best_models = []\n",
    "        for target in summary_df['Target'].unique():\n",
    "            target_data = summary_df[summary_df['Target'] == target]\n",
    "            best_model = target_data.loc[target_data['Accuracy'].idxmax()]\n",
    "            best_models.append(best_model)\n",
    "        \n",
    "        best_models_df = pd.DataFrame(best_models)\n",
    "        best_models_df.to_excel(writer, sheet_name='Best_Models', index=False)\n",
    "        \n",
    "        # Detailed results for each method\n",
    "        for method, method_results in classification_results.items():\n",
    "            method_data = []\n",
    "            for target, target_results in method_results.items():\n",
    "                for model, model_results in target_results.items():\n",
    "                    method_data.append({\n",
    "                        'Target': target,\n",
    "                        'Model': model,\n",
    "                        'Accuracy': model_results['accuracy'],\n",
    "                        'Accuracy_Std': model_results['accuracy_std'],\n",
    "                        'F1_Score': model_results['f1_score'],\n",
    "                        'F1_Std': model_results['f1_std'],\n",
    "                        'N_Features': model_results['n_features']\n",
    "                    })\n",
    "            \n",
    "            if method_data:\n",
    "                method_df = pd.DataFrame(method_data)\n",
    "                sheet_name = method.replace('_', ' ').title()[:31]  # Excel sheet name limit\n",
    "                method_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "    print(f\"âœ… Results exported successfully!\")\n",
    "\n",
    "# Save models and export results\n",
    "if classification_results_all:\n",
    "    # Save best models\n",
    "    saved_models = save_best_models(\n",
    "        classification_results_all, X_preprocessed, y_classification_processed\n",
    "    )\n",
    "    \n",
    "    # Export results summary\n",
    "    if 'summary_df' in locals():\n",
    "        export_results_summary(summary_df, classification_results_all)\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Analysis Complete! ðŸŽ‰\")\n",
    "    print(f\"ðŸ“ Models saved in: ./models/\")\n",
    "    print(f\"ðŸ“Š Results exported to: aml_mds_results_summary.xlsx\")\n",
    "else:\n",
    "    print(\"âš ï¸ No results available to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusion and Next Steps\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "This analysis successfully demonstrated the predictive potential of CD3+ chimerism dynamics for AML/MDS transplant outcomes. The machine learning pipeline implemented here provides a robust framework for:\n",
    "\n",
    "1. **Feature Selection Optimization**: Automated selection of optimal features using multiple methods\n",
    "2. **Multi-target Prediction**: Simultaneous prediction of multiple transplant outcomes\n",
    "3. **Model Comparison**: Systematic evaluation of different algorithms\n",
    "4. **Clinical Insights**: Actionable recommendations for transplant outcome prediction\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Validation Studies**: Validate models on independent cohorts\n",
    "2. **Temporal Analysis**: Incorporate time-series analysis for dynamic prediction\n",
    "3. **Biomarker Integration**: Include additional biomarkers (MRD, cytokines)\n",
    "4. **Clinical Implementation**: Develop real-time prediction tools\n",
    "5. **Prospective Studies**: Design prospective validation studies\n",
    "\n",
    "### Research Impact\n",
    "\n",
    "The findings from this analysis contribute to:\n",
    "- **Personalized Medicine**: Individual risk stratification\n",
    "- **Clinical Decision Making**: Evidence-based treatment modifications\n",
    "- **Resource Allocation**: Optimized monitoring schedules\n",
    "- **Patient Outcomes**: Improved survival and quality of life\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook provides a comprehensive, modular framework for analyzing transplant outcome data. All functions are reusable and can be adapted for different datasets or research questions.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}